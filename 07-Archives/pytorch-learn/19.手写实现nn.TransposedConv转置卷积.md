#! https://zhuanlan.zhihu.com/p/556344691
# 19.手写实现nn.TransposedConv转置卷积

之前我们在使用向量内积实现二维卷积的时候是手写分块，我们可以使用unfole操作实现分块
```python
unfold = nn.Unfold(kernel_size=(2, 3))
input = torch.randn(2, 5, 3, 4)
output = unfold(input)
# each patch contains 30 values (2x3=6 vectors, each of 5 channels)
# 4 blocks (2x3 kernels) in total in the 3x4 input
output.size()

# Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)
inp = torch.randn(1, 3, 10, 12)
w = torch.randn(2, 3, 4, 5)
inp_unf = torch.nn.functional.unfold(inp, (4, 5))
out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)
out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))
# or equivalently (and avoiding a copy),
# out = out_unf.view(1, 2, 7, 8)
(torch.nn.functional.conv2d(inp, w) - out).abs().max()
```

下面用转置卷积实现上采样，当然我们也可以通过填充的方式，再用普通的卷积实现
```python
#step4：通过对kernel进行展开并实现二维卷积，并推导出转置卷积,不考虑batch\channel\padding,sride = 1
def get_kernel_matrix(kernel, input_size):
    '''基于kernel和输入特征图的大小来得到填充拉直kernel堆叠后的矩阵'''
    kernel_h, kernel_w = kernel.shape
    input_h, input_w = input_size
    num_out_feat_map = (input_h - kernel_h + 1) * (input_w - kernel_w + 1)
    result = torch.zeros((num_out_feat_map, input_h * input_w)) #初始化结果矩阵，输出特征图元素个数 * 输入特征图元素个数
    for i in range(0, input_h-kernel_h+1, 1):
        for j in range(0, input_w - kernel_w + 1, 1):#填充成跟输入特征图一样大小
            #pad从里向外填充，顺序是左列，右列，上行，下行
            padded_kernel = F.pad(kernel, (i, input_h-kernel_h-i, j, input_w-kernel_w-j))
            result[i*(input_w - kernel_w + 1) + j] = padded_kernel.flatten()

    return result
#测试1：验证二维卷积
kernel = torch.randn(3, 3)
input = torch.randn(4, 4) #输出4行16列 2x2=4, 4x4=16
kernel_matrix = get_kernel_matrix(kernel, input.shape)
mm_conov2d_output = kernel_matrix @ input.reshape(-1, 1)  #矩阵相乘算出卷积
mm_conov2d_output
pytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0))

'''总之，两者都是用来重塑tensor的shape的。
view只适合对满足连续性条件（contiguous）的tensor进行操作，
而reshape同时还可以对不满足连续性条件的tensor进行操作，具有更好的鲁棒性。
view能干的reshape都能干，如果view不能干就可以用reshape来处理。
'''
#print(mm_conov2d_output).reshape(2,2)
#print(pytorch_conv2d_output.squeeze(0).squeeze(0))


#测试2：验证二维转置卷积,实现上采样
'''
把上面得到的kernel_matrix转置变成16x4  4x1，两个矩阵相乘就是16*1也就是4x4
本质就是后向传播y=wx dy/dx=w^T
'''
mm_transposed_conv2d_output = kernel_matrix.transpose(-1,-2) @ mm_conov2d_output
mm_transposed_conv2d_output= mm_transposed_conv2d_output.reshape(4 , 4)

print(mm_transposed_conv2d_output)

#pytorch torch.nn.ConvTranspose2d
pytorch_transposed_conv2d_output = F.conv_transpose2d(pytorch_conv2d_output,kernel.unsqueeze(0).unsqueeze(0) )
print(pytorch_transposed_conv2d_output)
```

