{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# step1 convert image to embedding vector sequence\n",
    "#写法1 DNN视角\n",
    "def image2emb_navie(image, patch_size, weight):\n",
    "    #image shape :bs*channels*h*w \n",
    "    patch = F.unfold(image, kernel_size = patch_size, stride = patch_size)\n",
    "    #print(patch.shape) #(bs,patch_depth,(image_h/patchsize)*(image_w/patch_size))\n",
    "    patch = patch.transpose(-1, -2)\n",
    "    patch_embdding = patch @ weight\n",
    "    return patch_embdding\n",
    "\n",
    "\n",
    "#test code for image2emb\n",
    "patch_size = 4\n",
    "bs, ic, image_h, image_w = 1,3,8,8\n",
    "model_dim = 8\n",
    "patch_depth = patch_size * patch_size * ic\n",
    "image = torch.randn(bs, ic, image_h, image_w)\n",
    "weight = torch.randn(patch_depth, model_dim)\n",
    "patch_embedding_navie = image2emb_navie(image, patch_size,weight)\n",
    "patch_embedding_navie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#写法2卷积视角，卷积步长等于patch长度\n",
    "def image2emb_conv(image, kernel, stride):\n",
    "    conv_output = F.conv2d(image, kernel, stride = stride)  #bs*oc*oh*ow\n",
    "    bs, oc, oh, ow = conv_output.shape\n",
    "    patch_embedding = conv_output.reshape((bs, oc, oh * ow)).transpose(-1, -2)\n",
    "    return patch_embedding\n",
    "\n",
    "#test code for image2emb\n",
    "patch_size = 4\n",
    "bs, ic, image_h, image_w = 1,3,8,8\n",
    "model_dim = 8\n",
    "patch_depth = patch_size * patch_size * ic\n",
    "image = torch.randn(bs, ic, image_h, image_w)\n",
    "weight = torch.randn(patch_depth, model_dim) #model_dim最后是输出通道数目，patch_depth是kernel.numel * 输入通道数目\n",
    "\n",
    "kernel = weight.transpose(0, 1).reshape(-1, ic, patch_size, patch_size)# oc*ic*kh*kw\n",
    "patch_embedding_conv = image2emb_conv(image, kernel,patch_size)\n",
    "patch_embedding_conv\n",
    "patch_embedding_navie = image2emb_navie(image, patch_size,weight)\n",
    "flag = torch.allclose(patch_embedding_conv,patch_embedding_navie)\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tile和repeat都可以复制，可以看[博客](https://www.cnblogs.com/nxf-rabbit75/p/10055932.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2 序列开头增加一个cls\n",
    "#注意一下: VisionTransformer中这里可学习的变量cls_token_embedding是一个长度为model_dim的向量，\n",
    "# 与batchsize无关，concat的时候对batch维度复制一下即可。特此更正一下。\n",
    "max_num_token = 16\n",
    "cls_token_embedding = torch.randn(1, 1, model_dim, requires_grad=True)\n",
    "\n",
    "cls_token_embedding.repeat(bs,1,1).shape\n",
    "token_embedding = torch.cat([cls_token_embedding.repeat(bs,1,1),patch_embedding_conv],dim = 1)\n",
    "\n",
    "# step3 增加位置编码，采用随机的可学习的embedding,各种位置编码性能差不多\n",
    "position_embedding_table = torch.randn(max_num_token, model_dim, requires_grad=True)\n",
    "seq_len = token_embedding.shape[1] #由于图中大小确定，mask也相对确定，不像nlp那种句子长度不确定\n",
    "position_embdding = torch.tile(position_embedding_table[:seq_len], [token_embedding.shape[0],1,1])#batch复制\n",
    "# position_embdding.shape\n",
    "token_embedding += position_embdding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor(3.1925, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#step4. 将embedding送入encoder\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model = model_dim, nhead = 8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\n",
    "encoder_output = transformer_encoder(token_embedding)\n",
    "\n",
    "#step5 做分类\n",
    "num_classes = 10\n",
    "label = torch.randint(10,(bs,))\n",
    "# print(label)\n",
    "cls_token_output = encoder_output[:,0,:]\n",
    "linear_layer = nn.Linear(model_dim, num_classes)\n",
    "logits = linear_layer(cls_token_embedding)\n",
    "logits = logits.squeeze(1) #要把中间的维度消除掉\n",
    "print(logits.shape)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, label)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
