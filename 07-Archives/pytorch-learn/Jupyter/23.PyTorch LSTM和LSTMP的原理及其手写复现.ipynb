{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现LSTM和LSTMP源码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 5])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "#定义常量\n",
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "proj_size = 3#比hidden_size小\n",
    "input = torch.randn(bs, T, i_size) #输入序列\n",
    "c0 = torch.randn(bs, h_size) #初始值不参与训练，api里面维度(D∗num_layers,N,Hout)\n",
    "h0 = torch.randn(bs, h_size)\n",
    "\n",
    "#调用官方api\n",
    "lstm_layer = nn.LSTM(i_size, h_size, batch_first = True)\n",
    "output, (h_final, c_final) = lstm_layer(input, (h0.unsqueeze(0), c0.unsqueeze(0)))\n",
    "# print(output)\n",
    "# print(h_final)\n",
    "'''\n",
    "weight_ih_l0 torch.Size([20, 4]) 4个W拼起来 4个5*4\n",
    "weight_hh_l0 torch.Size([20, 5]) 4个W拼起来 4个5*5\n",
    "bias_ih_l0 torch.Size([20])\n",
    "bias_hh_l0 torch.Size([20])\n",
    "'''\n",
    "for k, v in lstm_layer.named_parameters():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1173, -0.1761,  0.0655, -0.0596, -0.1059],\n",
      "         [ 0.3345, -0.1519,  0.0752, -0.2560,  0.0303],\n",
      "         [ 0.2519, -0.1415,  0.1104, -0.2484, -0.0150]],\n",
      "\n",
      "        [[-0.1127,  0.5996, -0.3856, -0.1163,  0.0772],\n",
      "         [ 0.1967,  0.3045, -0.1864, -0.2168,  0.1104],\n",
      "         [ 0.0886,  0.2985, -0.0083, -0.0245,  0.0345]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.1173, -0.1761,  0.0655, -0.0596, -0.1059],\n",
      "         [ 0.3345, -0.1519,  0.0752, -0.2560,  0.0303],\n",
      "         [ 0.2519, -0.1415,  0.1104, -0.2484, -0.0150]],\n",
      "\n",
      "        [[-0.1127,  0.5996, -0.3856, -0.1163,  0.0772],\n",
      "         [ 0.1967,  0.3045, -0.1864, -0.2168,  0.1104],\n",
      "         [ 0.0886,  0.2985, -0.0083, -0.0245,  0.0345]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#自己写一个LSTM模型\n",
    "def lstm_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    h0, c0 = initial_states #初始状态\n",
    "    bs, T, i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1) #bs,4*hidden_size,i_size\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)#bs,4*size, h_size\n",
    "\n",
    "    output_size = h_size\n",
    "    output = torch.zeros(bs, T, output_size) #输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t, :] #当前时刻的输入向量,[bs, i_size],矩阵相乘后面加一个维度\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))  #bs, 4*h_size,1\n",
    "        w_times_x = w_times_x.squeeze(-1) #bs, 4*h_size\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  #bs, 4*h_size,1\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1) #bs, 4*h_size\n",
    "\n",
    "        #分别计算输入们(i)\\遗忘门(f)\\cell门(g)\\输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size] + \\\n",
    "            w_times_h_prev[:, :h_size] +b_ih[:h_size] + b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + \\\n",
    "            w_times_h_prev[:, h_size:2*h_size] +b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:,2*h_size:3*h_size]\\\n",
    "             +b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3*h_size:] + \\\n",
    "            w_times_h_prev[:, 3*h_size:] +b_ih[3*h_size:] + b_hh[3*h_size:])\n",
    "        \n",
    "        #然后算记忆元ct,迭代实现\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "        output[:, t, :] = prev_h\n",
    "    return output, (prev_h, prev_c)\n",
    "\n",
    "output_custom, (h_final_custom, c_final_custom) = lstm_forward(input, (h0,c0), \\\n",
    "    lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0)\n",
    "\n",
    "print(output)\n",
    "print(output_custom)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3]) torch.Size([1, 2, 3]) torch.Size([1, 2, 5])\n",
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 3])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n",
      "weight_hr_l0 torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "#加入proj_size\n",
    "#定义常量\n",
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "proj_size = 3#比hidden_size小\n",
    "input = torch.randn(bs, T, i_size) #输入序列\n",
    "c0 = torch.randn(bs, h_size) #初始值不参与训练，api里面维度(D∗num_layers,N,Hout)\n",
    "h0 = torch.randn(bs, proj_size) #修改输出\n",
    "\n",
    "#调用官方api\n",
    "lstm_layer = nn.LSTM(i_size, h_size, batch_first = True, proj_size=proj_size)\n",
    "output, (h_final, c_final) = lstm_layer(input, (h0.unsqueeze(0), c0.unsqueeze(0)))\n",
    "# print(output)\n",
    "# print(h_final)\n",
    "print(output.shape, h_final.shape, c_final.shape)#记忆元的输出大小不变，最终hidden_states变小\n",
    "'''\n",
    "weight_ih_l0 torch.Size([20, 4])\n",
    "weight_hh_l0 torch.Size([20, 3]) 这里维度减少了\n",
    "bias_ih_l0 torch.Size([20])\n",
    "bias_hh_l0 torch.Size([20])\n",
    "weight_hr_l0 torch.Size([3, 5]) 实现对hidden_states的压缩\n",
    "'''\n",
    "for k, v in lstm_layer.named_parameters():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1222, -0.2479,  0.0743],\n",
      "         [ 0.0369, -0.1440, -0.1019],\n",
      "         [ 0.1141, -0.0750, -0.0232]],\n",
      "\n",
      "        [[ 0.0354,  0.0718, -0.0084],\n",
      "         [ 0.1311,  0.0020, -0.0178],\n",
      "         [ 0.1000,  0.0042, -0.1350]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.1222, -0.2479,  0.0743],\n",
      "         [ 0.0369, -0.1440, -0.1019],\n",
      "         [ 0.1141, -0.0750, -0.0232]],\n",
      "\n",
      "        [[ 0.0354,  0.0718, -0.0084],\n",
      "         [ 0.1311,  0.0020, -0.0178],\n",
      "         [ 0.1000,  0.0042, -0.1350]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#自己写一个LSTM模型\n",
    "def lstm_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh, w_hr = None):\n",
    "    h0, c0 = initial_states #初始状态\n",
    "    bs, T, i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1) #bs,4*hidden_size,i_size\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)#bs,4*size, h_size\n",
    "\n",
    "    if w_hr is not None:\n",
    "        p_size = w_hr.shape[0]\n",
    "        output_size = p_size\n",
    "        batch_w_hr = w_hr.unsqueeze(0).tile(bs, 1, 1) #[bs, p_size, h_size]\n",
    "    else:\n",
    "        output_size = h_size\n",
    "\n",
    "    output = torch.zeros(bs, T, output_size) #输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t, :] #当前时刻的输入向量,[bs, i_size],矩阵相乘后面加一个维度\n",
    "        \n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))  #bs, 4*h_size,1,\n",
    "        w_times_x = w_times_x.squeeze(-1) #bs, 4*h_size\n",
    "\n",
    "#4*hidden_size , p_size   @   p_size * 1,有prev_h的地方计算量会减少\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  #bs, 4*h_size,1，加入proj这里计算量小了\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1) #bs, 4*h_size\n",
    "\n",
    "        #分别计算输入们(i)\\遗忘门(f)\\cell门(g)\\输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size] + \\\n",
    "            w_times_h_prev[:, :h_size] +b_ih[:h_size] + b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + \\\n",
    "            w_times_h_prev[:, h_size:2*h_size] +b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:,2*h_size:3*h_size]\\\n",
    "             +b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3*h_size:] + \\\n",
    "            w_times_h_prev[:, 3*h_size:] +b_ih[3*h_size:] + b_hh[3*h_size:])\n",
    "        \n",
    "        #然后算记忆元ct,迭代实现\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c) #[bs, h_size]\n",
    "        if w_hr is not None:  #做proj\n",
    "            prev_h = torch.bmm(batch_w_hr, prev_h.unsqueeze(-1)) #[bs, p_size, 1]\n",
    "            prev_h = prev_h.squeeze(-1)  #[bs, p_size]\n",
    "\n",
    "        output[:, t, :] = prev_h\n",
    "    return output, (prev_h, prev_c)\n",
    "\n",
    "output_custom, (h_final_custom, c_final_custom) = lstm_forward(input, (h0,c0), \\\n",
    "    lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0\\\n",
    "        ,lstm_layer.weight_hr_l0)\n",
    "\n",
    "print(output)\n",
    "print(output_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step5 逐步实现GRU网络\n",
    "\n",
    "#对比GRU和LSTM大小\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "lstm_layer = nn.LSTM(3, 5)\n",
    "gru_layer = nn.GRU(3,5)\n",
    "#在此强调parameters()是一个函数！\n",
    "sum(p.numel() for p in lstm_layer.parameters()) #200\n",
    "sum(p.numel() for p in gru_layer.parameters()) #150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6181,  0.6273, -1.5152, -0.1698,  1.3558],\n",
      "         [-0.0634,  0.5651, -0.8531, -0.2970,  0.5613],\n",
      "         [-0.1826,  0.4736, -0.4083, -0.5083,  0.3924]],\n",
      "\n",
      "        [[-0.6443,  0.0289, -0.1807, -0.3848, -0.3714],\n",
      "         [-0.3887,  0.0804, -0.3085, -0.4390, -0.1755],\n",
      "         [-0.0996,  0.0738, -0.4211, -0.4593, -0.0508]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "weight_ih_l0 torch.Size([15, 4])\n",
      "weight_hh_l0 torch.Size([15, 5])\n",
      "bias_ih_l0 torch.Size([15])\n",
      "bias_hh_l0 torch.Size([15])\n",
      "tensor([[[ 0.6181,  0.6273, -1.5152, -0.1698,  1.3558],\n",
      "         [-0.0634,  0.5651, -0.8531, -0.2970,  0.5613],\n",
      "         [-0.1826,  0.4736, -0.4083, -0.5083,  0.3924]],\n",
      "\n",
      "        [[-0.6443,  0.0289, -0.1807, -0.3848, -0.3714],\n",
      "         [-0.3887,  0.0804, -0.3085, -0.4390, -0.1755],\n",
      "         [-0.0996,  0.0738, -0.4211, -0.4593, -0.0508]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#开始实现\n",
    "def gru_forward(input, initial_states,w_ih,w_hh,b_ih, b_hh):\n",
    "    prev_h = initial_states\n",
    "    bs, T, i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 3\n",
    "    #对权重扩维，变成batch_size倍\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)\n",
    "\n",
    "    output = torch.zeros(bs, T, h_size)  #GRU网络输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t,:]   #t时刻输入 [bs, i_size]\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)) #[bs,3*hs,1]\n",
    "        w_times_x = w_times_x.squeeze(-1)\n",
    "\n",
    "        w_times_h_prev= torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)) #[bs,3*hs,1]\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)\n",
    "\n",
    "        r_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size])#重置门\n",
    "        z_t = torch.sigmoid(w_times_x[:,h_size:2*h_size] + w_times_h_prev[:,h_size:2*h_size] \\\n",
    "            + b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])#更新门\n",
    "        \n",
    "        n_t = torch.tanh(w_times_x[:,2*h_size:3*h_size] + b_ih[2*h_size:3*h_size] +\\\n",
    "            r_t * (w_times_h_prev[:,2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])) #候选状态\n",
    "\n",
    "        prev_h = (1-z_t) * n_t + z_t * prev_h  #更新隐藏状态\n",
    "        output[:,t,:] = prev_h\n",
    "    \n",
    "    return output,prev_h\n",
    "\n",
    "\n",
    "#测试函数正确性\n",
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "input = torch.randn(bs, T, i_size) #输入序列\n",
    "h0 = torch.randn(bs, h_size) #初始值不参与训练，api里面维度(D∗num_layers,N,Hout)\n",
    "\n",
    "gru_layer = nn.GRU(i_size, h_size, batch_first = True)\n",
    "output, h_final = gru_layer(input,h0.unsqueeze(0))\n",
    "print(output)\n",
    "'''\n",
    "weight_ih_l0 torch.Size([15, 4])\n",
    "weight_hh_l0 torch.Size([15, 5])\n",
    "bias_ih_l0 torch.Size([15])\n",
    "bias_hh_l0 torch.Size([15])\n",
    "'''\n",
    "for k, v in gru_layer.named_parameters():\n",
    "    print(k, v.shape)\n",
    "\n",
    "output_custom, h_final_custom = gru_forward(input, h0, gru_layer.weight_ih_l0, \\\n",
    "    gru_layer.weight_hh_l0, gru_layer.bias_ih_l0, gru_layer.bias_hh_l0)\n",
    "\n",
    "print(output_custom)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
