{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5313,  0.2610, -0.7020, -1.0472],\n",
      "          [ 1.6464, -0.7453,  0.6006, -0.4810],\n",
      "          [ 0.2911, -0.3596,  0.0160, -0.5955],\n",
      "          [-0.7642, -0.5402, -0.7338, -0.5263]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0543, -0.2173, -0.0795],\n",
      "          [-0.0767,  0.3148, -0.2659],\n",
      "          [-0.2994, -0.0280, -0.0093]]]], requires_grad=True)\n",
      "tensor([[[[-0.6276,  0.7368],\n",
      "          [ 0.3146,  0.2455]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "tensor([[[[-0.6276,  0.7368],\n",
      "          [ 0.3146,  0.2455]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 1\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "kernel_size = 3\n",
    "bias = False\n",
    "input_size = [batch_size, in_channels, 4, 4]\n",
    "\n",
    "conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias = bias)\n",
    "input_feature_map = torch.randn(input_size)\n",
    "output_feature_map = conv_layer(input_feature_map)\n",
    "\n",
    "print(input_feature_map)\n",
    "print(conv_layer.weight)  #1*3*3*3 = out_channels*in_channels*height*width\n",
    "print(output_feature_map)\n",
    "\n",
    "#使用nn.functional中的卷积，作为函数调用\n",
    "output_feature_map1 = F.conv2d(input_feature_map, conv_layer.weight)\n",
    "print(output_feature_map1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4542, -4.8156,  3.3035,  1.6480,  1.7177],\n",
      "        [-1.9997,  0.5993,  2.1027,  1.5318, -0.3574],\n",
      "        [ 1.0011,  9.5139,  0.3971, -0.2130, -0.8853],\n",
      "        [ 2.7897, -2.0240,  0.3914,  2.1176,  3.5692],\n",
      "        [ 0.4185,  0.7012,  1.4854,  0.1911, -1.4043]])\n",
      "tensor([[-1.4542, -4.8156,  3.3035,  1.6480,  1.7177],\n",
      "        [-1.9997,  0.5993,  2.1027,  1.5318, -0.3574],\n",
      "        [ 1.0011,  9.5139,  0.3971, -0.2130, -0.8853],\n",
      "        [ 2.7897, -2.0240,  0.3914,  2.1176,  3.5692],\n",
      "        [ 0.4185,  0.7012,  1.4854,  0.1911, -1.4043]])\n"
     ]
    }
   ],
   "source": [
    "# 17手写滑动相乘\n",
    "import math\n",
    "input = torch.randn(5, 5)\n",
    "kernel = torch.randn(3, 3)\n",
    "bias = torch.randn(1) #偏置，默认通道为1\n",
    "\n",
    "#step1 用原始的矩阵运算实现二维卷积,先不考虑batch_size和channel\n",
    "def matrix_multiplication_for_conv2d(input, kernel,bias = 0,  stride = 1, padding = 0):\n",
    "\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding))\n",
    "\n",
    "    input_h, input_w = input.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "\n",
    "    output_h = (math.floor((input_h - kernel_h ) /stride ) + 1)\n",
    "    output_w = (math.floor((input_w - kernel_w ) /stride ) + 1)\n",
    "    output = torch.zeros(output_h, output_w) #初始化输出矩阵\n",
    "    for i in range(0, input_h - kernel_h + 1, stride):  #对高度维进行遍历\n",
    "        for j in range(0, input_w - kernel_w + 1, stride): #对宽度维进行遍历\n",
    "            region  = input[i : i + kernel_h , j : j + kernel_w]\n",
    "            output[int(i / stride), int(j/stride)] = torch.sum(region * kernel) + bias#逐元素相乘\n",
    "    return output\n",
    "#矩阵运算实现卷积结果\n",
    "mat_mul_conv_output = matrix_multiplication_for_conv2d(input, kernel, padding = 1,bias = bias)\n",
    "print(mat_mul_conv_output)\n",
    "\n",
    "#调用API结果\n",
    "pytorcg_api_conv_output = F.conv2d(input.reshape(1,1, input.shape[0], input.shape[1]),\\\n",
    "    kernel.reshape(1,1,kernel.shape[0],kernel.shape[1]),\\\n",
    "        padding=1,bias = bias)\n",
    "print(pytorcg_api_conv_output.squeeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as  F\n",
    "# 演示torch.flatten\n",
    "a = torch.randn(2, 3)\n",
    "a\n",
    "\n",
    "torch.flatten(a) #展平成为1维\n",
    "\n",
    "#numel\n",
    "a = torch.randn(2, 3)\n",
    "a.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3577,  3.1594,  5.0397,  3.2003, -0.7363],\n",
      "        [ 3.1394,  1.6667,  3.8867,  1.7496, -0.1544],\n",
      "        [ 4.8046, -1.4344, -0.8525,  3.1343,  1.1117],\n",
      "        [ 1.6232,  0.6599, -3.4425,  3.2378,  0.5643],\n",
      "        [ 3.8436,  1.8094, -1.5216, -0.3772,  0.2371]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 18手写滑动相乘\n",
    "import math\n",
    "input = torch.randn(5, 5)\n",
    "kernel = torch.randn(3, 3)\n",
    "bias = torch.randn(1) #偏置，默认通道为1\n",
    "\n",
    "#step2 内积实现\n",
    "def matrix_multiplication_for_conv2d_flatten(input, kernel,bias = 0,  stride = 1, padding = 0):\n",
    "\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding))\n",
    "\n",
    "    input_h, input_w = input.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "\n",
    "    output_h = (math.floor((input_h - kernel_h ) /stride ) + 1)\n",
    "    output_w = (math.floor((input_w - kernel_w ) /stride ) + 1)\n",
    "    output = torch.zeros(output_h, output_w) #初始化输出矩阵\n",
    "\n",
    "    region_matrix = torch.zeros(output.numel(), kernel.numel())#存储所有拉平后的特征区域\n",
    "    kernel_matrix = kernel.reshape(kernel.numel(), 1)\n",
    "    for i in range(0, input_h - kernel_h + 1, stride):  #对高度维进行遍历\n",
    "        for j in range(0, input_w - kernel_w + 1, stride): #对宽度维进行遍历\n",
    "            region  = input[i : i + kernel_h , j : j + kernel_w]\n",
    "            region_vector = torch.flatten(region)\n",
    "            region_matrix[i * output_w+j] = region_vector\n",
    "\n",
    "            output_matrix = region_matrix @ kernel_matrix\n",
    "            output = output_matrix.reshape(output_h, output_w) + bias\n",
    "            #output[int(i / stride), int(j/stride)] = torch.sum(region * kernel) + bias#逐元素相乘\n",
    "    return output\n",
    "#矩阵运算实现卷积结果\n",
    "mat_mul_conv_output_flatten = matrix_multiplication_for_conv2d_flatten(input, kernel, padding = 1,bias = bias)\n",
    "print(mat_mul_conv_output_flatten)\n",
    "\n",
    "#调用API结果\n",
    "pytorch_api_conv_output = F.conv2d(input.reshape(1,1, input.shape[0], input.shape[1]),\\\n",
    "    kernel.reshape(1,1,kernel.shape[0],kernel.shape[1]),\\\n",
    "        padding=1,bias = bias).squeeze(0).squeeze(0)\n",
    "flag = torch.allclose(mat_mul_conv_output_flatten, pytorch_api_conv_output)\n",
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! https://zhuanlan.zhihu.com/p/555265996\n",
    "# 引入batch_size和channel维度\n",
    "import math\n",
    "input = torch.randn(2, 2, 5, 5)\n",
    "kernel = torch.randn(3, 2, 3, 3)\n",
    "bias = torch.randn(3) #偏置，默认通道为1\n",
    "\n",
    "#step1 用原始的矩阵运算实现二维卷积,先不考虑batch_size和channel\n",
    "def matrix_multiplication_for_conv2d_full(input, kernel,bias = 0,  stride = 1, padding = 0):\n",
    "\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding,0,0,0,0))\n",
    "\n",
    "    bs, in_channel, input_h, input_w = input.shape\n",
    "    out_channel, in_channel, kernel_h, kernel_w = kernel.shape\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channel)\n",
    "\n",
    "    output_h = (math.floor((input_h - kernel_h ) /stride ) + 1)\n",
    "    output_w = (math.floor((input_w - kernel_w ) /stride ) + 1)\n",
    "    output = torch.zeros(bs, out_channel, output_h, output_w) #初始化输出矩阵\n",
    "\n",
    "    for ind in range(bs):    \n",
    "        for oc in range(out_channel):\n",
    "            for ic in range(in_channel):\n",
    "                for i in range(0, input_h - kernel_h + 1, stride):  #对高度维进行遍历\n",
    "                    for j in range(0, input_w - kernel_w + 1, stride): #对宽度维进行遍历\n",
    "                        region  = input[ind, ic, i : i + kernel_h , j : j + kernel_w]\n",
    "                        output[ind, oc, int(i / stride), int(j/stride)] += torch.sum(region * kernel[oc,ic]) #逐元素相乘\n",
    "\n",
    "            output[ind, oc] +=bias[oc]\n",
    "    return output\n",
    "#矩阵运算实现卷积结果\n",
    "mat_mul_conv_output = matrix_multiplication_for_conv2d_full(input, kernel, padding = 1,bias = bias,stride = 2)\n",
    "#print(mat_mul_conv_output)\n",
    "\n",
    "#调用API结果\n",
    "pytorcg_api_conv_output = F.conv2d(input,kernel,padding=1,bias = bias,stride = 2)\n",
    "#print(pytorcg_api_conv_output)\n",
    "flag = torch.isclose(mat_mul_conv_output, pytorcg_api_conv_output)\n",
    "#flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0064, -7.0201, -1.7938,  0.1585],\n",
      "        [ 6.6041, -3.2594, -6.4683,  1.4094],\n",
      "        [ 1.8870, 15.0458,  0.0467, -3.5150],\n",
      "        [-3.8525, -5.3362,  5.0689, -0.6575]])\n",
      "tensor([[[[-4.0064, -7.0201, -1.7938,  0.1585],\n",
      "          [ 6.6041, -3.2594, -6.4683,  1.4094],\n",
      "          [ 1.8870, 15.0458,  0.0467, -3.5150],\n",
      "          [-3.8525, -5.3362,  5.0689, -0.6575]]]])\n"
     ]
    }
   ],
   "source": [
    "#step4：通过对kernel进行展开并实现二维卷积，并推导出转置卷积,不考虑batch\\channel\\padding,sride = 1\n",
    "def get_kernel_matrix(kernel, input_size):\n",
    "    '''基于kernel和输入特征图的大小来得到填充拉直kernel堆叠后的矩阵'''\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    input_h, input_w = input_size\n",
    "    num_out_feat_map = (input_h - kernel_h + 1) * (input_w - kernel_w + 1)\n",
    "    result = torch.zeros((num_out_feat_map, input_h * input_w)) #初始化结果矩阵，输出特征图元素个数 * 输入特征图元素个数\n",
    "    for i in range(0, input_h-kernel_h+1, 1):\n",
    "        for j in range(0, input_w - kernel_w + 1, 1):#填充成跟输入特征图一样大小\n",
    "            #pad从里向外填充，顺序是左列，右列，上行，下行\n",
    "            padded_kernel = F.pad(kernel, (i, input_h-kernel_h-i, j, input_w-kernel_w-j))\n",
    "            result[i*(input_w - kernel_w + 1) + j] = padded_kernel.flatten()\n",
    "\n",
    "    return result\n",
    "#测试1：验证二维卷积\n",
    "kernel = torch.randn(3, 3)\n",
    "input = torch.randn(4, 4) #输出4行16列 2x2=4, 4x4=16\n",
    "kernel_matrix = get_kernel_matrix(kernel, input.shape)\n",
    "mm_conov2d_output = kernel_matrix @ input.reshape(-1, 1)  #矩阵相乘算出卷积\n",
    "mm_conov2d_output\n",
    "pytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "'''总之，两者都是用来重塑tensor的shape的。\n",
    "view只适合对满足连续性条件（contiguous）的tensor进行操作，\n",
    "而reshape同时还可以对不满足连续性条件的tensor进行操作，具有更好的鲁棒性。\n",
    "view能干的reshape都能干，如果view不能干就可以用reshape来处理。\n",
    "'''\n",
    "#print(mm_conov2d_output).reshape(2,2)\n",
    "#print(pytorch_conv2d_output.squeeze(0).squeeze(0))\n",
    "\n",
    "\n",
    "#测试2：验证二维转置卷积,实现上采样\n",
    "'''\n",
    "把上面得到的kernel_matrix转置变成16x4  4x1，两个矩阵相乘就是16*1也就是4x4\n",
    "本质就是后向传播y=wx dy/dx=w^T\n",
    "'''\n",
    "mm_transposed_conv2d_output = kernel_matrix.transpose(-1,-2) @ mm_conov2d_output\n",
    "mm_transposed_conv2d_output= mm_transposed_conv2d_output.reshape(4 , 4)\n",
    "\n",
    "print(mm_transposed_conv2d_output)\n",
    "\n",
    "#pytorch torch.nn.ConvTranspose2d\n",
    "pytorch_transposed_conv2d_output = F.conv_transpose2d(pytorch_conv2d_output,kernel.unsqueeze(0).unsqueeze(0) )\n",
    "print(pytorch_transposed_conv2d_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1445, -0.0805, -0.8505,  0.7367, -0.2037,  1.3674,  0.9186],\n",
       "        [-0.3654, -1.0221, -0.2708, -0.2582,  1.6529, -0.4387,  0.7612],\n",
       "        [ 0.9434, -0.6605, -0.4336,  1.6358, -1.1560,  0.6988,  1.2185],\n",
       "        [-0.4670, -0.3864, -1.8162, -1.4449, -0.7560, -0.9054,  0.7602],\n",
       "        [ 0.2413, -0.1736,  0.9102, -0.7934,  0.6402, -0.0801, -0.4665],\n",
       "        [ 0.4164,  0.0931, -0.2639, -0.3033,  1.7619, -0.4075,  0.9078],\n",
       "        [ 0.2184, -1.6596, -0.8298, -2.0634, -0.5273,  0.1174,  1.3770]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 空洞卷积\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "a = torch.randn(7,7)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1445, -0.0805, -0.8505],\n",
       "        [-0.3654, -1.0221, -0.2708],\n",
       "        [ 0.9434, -0.6605, -0.4336]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:3, 0:3] #如果和3x3卷积核\n",
    "a[0:3, 0:3] #dilation = 1\n",
    "a[0:5:2, 0:5:2] #dilation = 2 在不增加计算量的情况下，增加感受野的面积\n",
    "a[0:7:3, 0:7:3] #dilation = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group > 1，把一个大卷积看出多个小卷积，通道融合不需要完全充分，只在group内融合\n",
    "#后面加一个1*1 pointwise convolution即可，就是前面说的convmixer的方法\n",
    "in_channel, out_channel = 2, 4\n",
    "group = 2\n",
    "sub_in_channel, sub_out_channel = 1, 2 #2组一共4个卷积核，相比上面8个卷积核少了一半，没有考虑通道融合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "def matrix_multiplication_for_conv2d_final(input, kernel, bias = None, stride = 1, padding = 0, dilation = 1, groups = 1):\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding,padding,padding,padding,0,0,0,0))\n",
    "    bs, in_channel, input_h, input_w = input.shape    \n",
    "    out_channel,_, kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    assert out_channel % groups == 0 and in_channel % groups == 0,\"groups必须要同时被通道数整除！\"\n",
    "    input = input.reshape((bs, groups,in_channel//groups,input_h, input_w))\n",
    "\n",
    "    kernel = kernel.reshape((groups, out_channel//groups, in_channel//groups, kernel_h, kernel_w))\n",
    "    \n",
    "    kernel_h = (kernel_h-1) *(dilation-1) + kernel_h#后面是卷积作用的点，前面是空洞的数目，加起来就是作用范围\n",
    "    kernel_w = (kernel_w-1) *(dilation-1) + kernel_w\n",
    "\n",
    "    output_h = math.floor((input_h-kernel_h)/stride)+1\n",
    "    output_w = math.floor((input_w-kernel_w)/stride)+1\n",
    "    output_shape = (bs, groups, out_channel//groups, output_h, output_w)\n",
    "    output = torch.zeros(output_shape)\n",
    "\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channel)\n",
    "\n",
    "    for ind in range(bs): #对batch_size进行遍历\n",
    "        for g in range(groups): #对群组进行遍历\n",
    "            for oc in range(out_channel//groups): #对分组后的通道进行遍历\n",
    "                for ic in range(in_channel//groups): #对分组后的输入通道进行遍历\n",
    "                    for i in range(0, input_h-kernel_h+1, stride):\n",
    "                        for j in range(0, input_w-kernel_w+1, stride):\n",
    "                            region = input[ind, g, ic, i:i+kernel_h:dilation,j:j+kernel_w:dilation] #特征区域\n",
    "                            output[ind, g, oc, int(i/stride), int(j/stride)] += torch.sum(region * kernel[g, oc, ic])\n",
    "                output[ind, g, oc] += bias[g*(out_channel//groups)+oc ]#考虑偏置\n",
    "    \n",
    "    output = output.reshape((bs,out_channel, output_h,output_w))#还原成4维\n",
    "    return output\n",
    "\n",
    "#开始验证测试\n",
    "kernel_size = 3\n",
    "bs, in_channel, input_h, input_w = 2, 2, 5, 5\n",
    "out_channel = 4\n",
    "groups ,dilation, stride, padding= 2 ,2 ,2, 1\n",
    "input = torch.randn(bs,in_channel,input_h,input_w)\n",
    "kernel = torch.randn(out_channel,in_channel//groups,kernel_size, kernel_size )\n",
    "bias = torch.randn(out_channel)\n",
    "\n",
    "pytorch_conv2d_api_output = \\\n",
    "    F.conv2d(input, kernel, bias=bias,padding=padding,stride=stride,dilation=dilation,groups=groups)\n",
    "\n",
    "mm_conv2d_final_output = matrix_multiplication_for_conv2d_final\\\n",
    "    (input, kernel, bias = bias, stride = stride, padding = padding, dilation = dilation, groups = groups)\n",
    "\n",
    "flag = torch.allclose(pytorch_conv2d_api_output, mm_conv2d_final_output)\n",
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
