{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True\n",
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
    "# !pip install numpy matplotlib spacy torchtext seaborn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math,copy,time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\") \n",
    "# seaborn只在最后可视化self-attention的时候用到，\n",
    "# 可以先不管或者注释掉这两行\n",
    "#%matplotlibinline # only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Embedding类\n",
    "顾名思义，Embeddings类主要负责对输入的source sequence和target\n",
    "\n",
    "sequence的词嵌入表示的映射，具体为每个词从one-hot表示，映射为d_model维度的一个向量。这样的话，如果有10个词，d_model为512的时候，则我们得到的是一个10*512的矩阵。每一行是512列，代表一个词的dense表示。\n",
    "\n",
    "实际上我们在构造一个句子输入的时候，每个字对应voacb的索引，并不需要把它转化成onehot的形式，应该是Embedding会帮我们自动完成这个过程。\n",
    "\n",
    "“In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation”,\n",
    "这句话，在没有理解他们的一篇ACL的使用词根做机器翻译的论文的前提下，其实不容易理解。例如，在做欧洲语系和英语翻译的时候，很多词是共享词根的，所以，他们这个源语言和目标语言共享一个词嵌入矩阵有一定的道理。如果是中文和英文之间，则完全没有共享词嵌入矩阵的必要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "  def __init__(self,d_model,vocab):\n",
    "    #d_model=512, vocab=当前语言的词表大小\n",
    "    super(Embeddings,self).__init__()\n",
    "    self.lut=nn.Embedding(vocab,d_model) \n",
    "    # one-hot转词嵌入，这里有一个待训练的矩阵E，大小是vocab*d_model\n",
    "    self.d_model=d_model # 512\n",
    "  def forward(self,x): \n",
    "     # x ~ (batch.size, sequence.length, one-hot), \n",
    "     #one-hot大小=vocab，当前语言的词表大小\n",
    "     return self.lut(x)*math.sqrt(self.d_model) \n",
    "     # 得到的10*512词嵌入矩阵，主动乘以sqrt(512)=22.6，\n",
    "     #这里我做了一些对比，感觉这个乘以sqrt(512)没啥用… 求反驳。\n",
    "     #这里的输出的tensor大小类似于(batch.size, sequence.length, 512)\n",
    "\n",
    "     '''\n",
    "     具体的源语言和目标语言的共享词嵌入矩阵的方法如下，其中的weight就是vocab*d_model的词嵌入矩阵：\n",
    "    if False:\n",
    "        model.src_embed[0].lut.weight=model.tgt_embeddings[0].lut.weight\n",
    "        model.generator.lut.weight=model.tgt_embed[0].lut.weight\n",
    "     '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.PositionalEmbedding(PE)位置编码\n",
    "注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数。\n",
    "需要将drop应用于编码器和解码器堆栈中词嵌入和位置编码的和，使用drop = 0.1\n",
    "\n",
    "位置编码等价变形如下图所示：![公式变形](https://i.imgur.com/l77G1tz.jpg)\n",
    "\n",
    "PyTorch中定义模型时，有时候会遇到self.register_buffer('name', Tensor)的操作，该方法的作用是定义一组参数，该组参数的特别之处在于：模型训练时不会更新（即调用 optimizer.step() 后该组参数不会变化，只可人为地改变它们的值），但是保存模型时，该组参数又作为模型参数不可或缺的一部分被保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): \n",
    "  \"Implement the PE function.\" \n",
    "  def __init__(self, d_model, dropout, max_len=5000): \n",
    "    #d_model=512,dropout=0.1,\n",
    "    #max_len=5000代表事先准备好长度为5000的序列的位置编码，其实没必要，\n",
    "    #一般100或者200足够了。\n",
    "    super(PositionalEncoding, self).__init__() \n",
    "    self.dropout = nn.Dropout(p=dropout) \n",
    "\n",
    "    # Compute the positional encodings once in log space. \n",
    "    pe = torch.zeros(max_len, d_model) \n",
    "    #(5000,512)矩阵，保持每个位置的位置编码，一共5000个位置，\n",
    "    #每个位置用一个512维度向量来表示其位置编码\n",
    "    position = torch.arange(0, max_len).unsqueeze(1) #扩充一维借助广播机制，不然没法相乘\n",
    "    # (5000) -> (5000,1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * \n",
    "      -(math.log(10000.0) / d_model)) \n",
    "      # (0,2,…, 510)一共准备256个值，供sin, cos调用\n",
    "    pe[:, 0::2] = torch.sin(position * div_term) # 偶数下标的位置\n",
    "    pe[:, 1::2] = torch.cos(position * div_term) # 奇数下标的位置\n",
    "    pe = pe.unsqueeze(0) \n",
    "    # (5000, 512) -> (1, 5000, 512) 为batch.size留出位置\n",
    "    self.register_buffer('pe', pe) \n",
    "  def forward(self, x): \n",
    "    x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) \n",
    "    # 接受1.Embeddings的词嵌入结果x，\n",
    "    #然后把自己的位置编码pe，封装成torch的Variable(不需要梯度)，加上去。\n",
    "    #例如，假设x是(30,10,512)的一个tensor，\n",
    "    #30是batch.size, 10是该batch的序列长度, 512是每个词的词嵌入向量；\n",
    "    #则该行代码的第二项是(1, min(10, 5000), 512)=(1,10,512)，\n",
    "    #在具体相加的时候，会扩展(1,10,512)为(30,10,512)，\n",
    "    #保证一个batch中的30个序列，都使用（叠加）一样的位置编码。\n",
    "    return self.dropout(x) # 增加一次dropout操作\n",
    "# 注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4c7d530053b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m div_term = torch.exp(torch.arange(0, 20, 2) * \n\u001b[0;32m      3\u001b[0m       -(math.log(10000.0) / 20)) \n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposition\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiv_term\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "'''\n",
    "有了1.Embeddings和2.PositionalEncoding；在具体使用的时候，是通过torch.nn.Sequential来把他们两个串起来的：\n",
    "\n",
    "'''\n",
    "\n",
    "nn.Sequential(Embeddings(d_model,src_vocab),\n",
    "  PositionalEncoding(d_model,dropout)) \n",
    "# 例如，d_model=512, src_vocab=源语言的词表大小, \n",
    "#dropout=0.1即 dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.多头注意力\n",
    "MultiHead $(Q, K, V)=$ Concat $\\left(\\right.$ head $_{1}, \\ldots$, head $\\left._{\\mathrm{h}}\\right) W^{O}$ where head $\\operatorname{d}_{\\mathrm{i}}=\\operatorname{Attention}\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)$\n",
    "\n",
    "$W_{i}^{Q} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{k}}, W_{i}^{K} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{k}}, W_{i}^{V} \\in$ $\\mathbb{R}^{d_{\\text {model }} \\times d_{v}}$ and $W^{O} \\in \\mathbb{R}^{h d_{v} \\times d_{\\text {model }}}$.\n",
    "\n",
    "这里$h=8$ ，取$d_{k}=$ $d_{v}=d_{\\text {model }} / h=64$\n",
    "\n",
    "dk和dv维度一致,dv可以不一致，最后经过全连接层再回到原来的维度即可，我们这里设置成一样的\n",
    "\n",
    "这个过程就是说每个头计算出结果是64维度，8个头拼接起来就是512，经过一个全连接，保持维度不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None): \n",
    "# query, key, value的形状类似于(30, 8, 10, 64), (30, 8, 11, 64), \n",
    "#(30, 8, 11, 64)，例如30是batch.size，即当前batch中有多少一个序列；\n",
    "# 8=head.num，注意力头的个数；\n",
    "# 10=目标序列中词的个数，64是每个词对应的向量表示；\n",
    "# 11=源语言序列传过来的memory中，当前序列的词的个数，\n",
    "# 64是每个词对应的向量表示。\n",
    "# 类似于，这里假定query来自target language sequence；\n",
    "# key和value都来自source language sequence.\n",
    "  \"Compute 'Scaled Dot Product Attention'\" \n",
    "  d_k = query.size(-1) # 64=d_k\n",
    "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # 先是(30,8,10,64)和(30, 8, 64, 11)相乘，\n",
    "    #（注意是最后两个维度相乘）得到(30,8,10,11)，\n",
    "    #代表10个目标语言序列中每个词和11个源语言序列的分别的“亲密度”。\n",
    "    #然后除以sqrt(d_k)=8，防止过大的亲密度。\n",
    "    #这里的scores的shape是(30, 8, 10, 11)\n",
    "  if mask is not None: \n",
    "    scores = scores.masked_fill(mask == 0, -1e9) \n",
    "    #使用mask，对已经计算好的scores，按照mask矩阵，填-1e9，padding是0的位置是1e-9\n",
    "    #然后在下一步计算softmax的时候，被设置成-1e9的数对应的值~0,被忽视\n",
    "  p_attn = F.softmax(scores, dim = -1) \n",
    "    #对scores的最后一个维度执行softmax，得到的还是一个tensor, \n",
    "    #(30, 8, 10, 11)\n",
    "  if dropout is not None: \n",
    "    p_attn = dropout(p_attn) #执行一次dropout\n",
    "  return torch.matmul(p_attn, value), p_attn\n",
    "#返回的第一项，是(30,8,10, 11)乘以（最后两个维度相乘）\n",
    "#value=(30,8,11,64)，得到的tensor是(30,8,10,64)，\n",
    "#和query的最初的形状一样。另外，返回p_attn，形状为(30,8,10,11). \n",
    "#注意，这里返回p_attn主要是用来可视化显示多头注意力机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中contiguous的[使用](https://zhuanlan.zhihu.com/p/64551412)\n",
    "\n",
    "**一定要看上面的博客，让你从底层理解，这就体现出系统知识的重要性了**\n",
    "\n",
    "PyTorch 提供了 **`is_contiguous`、`contiguous`**(形容词动用)两个方法 ，分别用于判定Tensor是否是 **contiguous** 的，以及保证Tensor是 **contiguous**的。\n",
    "\n",
    "Tensor多维数组底层实现是使用一块连续内存的1维数组（行优先顺序存储，下文描述），Tensor在元信息里保存了多维数组的形状，在访问元素时，通过多维度索引转化成1维数组相对于数组起始位置的偏移量即可找到对应的数据。某些Tensor操作（如transpose、permute、narrow、expand）与原Tensor是共享内存中的数据，不会改变底层数组的存储，但原来在语义上相邻、内存里也相邻的元素在执行这样的操作后，在语义上相邻，但在内存不相邻，即不连续了（is not contiguous）。\n",
    "\n",
    "如果想要变得连续使用contiguous方法，如果Tensor不是连续的，则会重新开辟一块内存空间保证数据是在内存中是连续的，如果Tensor是连续的，则contiguous无操作。\n",
    "\n",
    "\n",
    "torch.view等方法操作需要连续的Tensor。\n",
    "transpose、permute 操作虽然没有修改底层一维数组，但是新建了一份Tensor元信息，并在新的元信息中的 重新指定 stride。torch.view 方法约定了不修改数组本身，只是使用新的形状查看数据。如果我们在 transpose、permute 操作后执行 view，Pytorch 会抛出错误：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module): \n",
    "  def __init__(self, h, d_model, dropout=0.1): \n",
    "    # h=8, d_model=512\n",
    "    \"Take in model size and number of heads.\" \n",
    "    super(MultiHeadedAttention, self).__init__() \n",
    "    assert d_model % h == 0 # We assume d_v always equals d_k 512%8=0\n",
    "    self.d_k = d_model // h # d_k=512//8=64\n",
    "    self.h = h #8\n",
    "    self.linears = clones(nn.Linear(d_model, d_model), 4) \n",
    "    #定义四个Linear networks, 每个的大小是(512, 512)的，\n",
    "    #每个Linear network里面有两类可训练参数，Weights，\n",
    "    #其大小为512*512，以及biases，其大小为512=d_model。\n",
    "\n",
    "    self.attn = None \n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "  def forward(self, query, key, value, mask=None): \n",
    "   # 注意，输入query的形状类似于(30, 10, 512)，\n",
    "   # key.size() ~ (30, 11, 512), \n",
    "   #以及value.size() ~ (30, 11, 512)\n",
    "    \n",
    "    if mask is not None: # Same mask applied to all h heads. \n",
    "      mask = mask.unsqueeze(1) # mask下回细细分解。\n",
    "    nbatches = query.size(0) #e.g., nbatches=30\n",
    "    # 1) Do all the linear projections in batch from \n",
    "    #d_model => h x d_k \n",
    "    query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in \\\n",
    "      zip(self.linears, (query, key, value))] \n",
    "      # 这里是前三个Linear Networks的具体应用，\n",
    "      #例如query=(30,10, 512) -> Linear network -> (30, 10, 512) \n",
    "      #-> view -> (30,10, 8, 64) -> transpose(1,2) -> (30, 8, 10, 64)\n",
    "      #，其他的key和value也是类似地，\n",
    "      #从(30, 11, 512) -> (30, 8, 11, 64)。\n",
    "    # 2) Apply attention on all the projected vectors in batch. \n",
    "    x, self.attn = attention(query, key, value, mask=mask, \n",
    "      dropout=self.dropout) \n",
    "      #调用上面定义好的attention函数，输出的x形状为(30, 8, 10, 64)；\n",
    "      #attn的形状为(30, 8, 10=target.seq.len, 11=src.seq.len)\n",
    "    # 3) \"Concat\" using a view and apply a final linear. \n",
    "    x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) \n",
    "      # x ~ (30, 8, 10, 64) -> transpose(1,2) -> \n",
    "      #(30, 10, 8, 64) -> contiguous() and view -> \n",
    "      #(30, 10, 8*64) = (30, 10, 512)\n",
    "    return self.linears[-1](x) \n",
    "#执行第四个Linear network，把(30, 10, 512)经过一次linear network，\n",
    "#得到(30, 10, 512)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source sequence是否需要mask，为什么？其实是需要的，因为一个batch里面，可能序列的长短不一，有些是10个词打满，有些可能只有8个词，则这个时候，剩下的2个词的位置会被填充为”blank”这样的词，从而mask的时候，需要知道哪些地方是”blank”，这些位置不参与softmax以及序列的整体向量化表示等进一步的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LayerNorm \n",
    "用来对每个sublayer的输出进行处理，也对6层EncoderLayer之后的输出进行处理\n",
    "\n",
    "层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。\n",
    "\n",
    "不过你要特定Transformer模型的话，你会发现CV中的ViT也是用了LN的，这就违背了大家以往的“CV用BN，NLP用LN“的常识了，而且你会发现，真要将ViT中的LN换成BN，结果还真的会下降，所以Transformer（而不是NLP或CV）跟LN似乎真的更配。这又有什么解释呢？\n",
    "\n",
    "我猜测，是因为Transformer主要用的是Scaled-Dot Self Attention，里边的q,k是做内积的，并且\n",
    "$$\n",
    "\\langle q, k\\rangle=\\|q\\|\\|k\\| \\cos (q, k) \\leq\\|q\\|\\|k\\|\n",
    "$$\n",
    "\n",
    "LN本质上是L2 Normalzation的一个简单变体，q,k一般是LN之后再接一个Dense变换，这样一来 ‖ q ‖ , ‖ k ‖ 就会一定程度上得到控制，从而使Attention的值在合理范围内，不至于梯度消失/爆炸。如果换成BN，对‖ q ‖ , ‖ k ‖的控制就没那么有效了。\n",
    "主要是指对q,k的模长控制没那么直接有效，LN的维度 是hidden 正好是q k的维度，所以能约束他们的norm，BN的维度是batch 对应的是q k的每一个元素，不能对qk整体约束\n",
    "\n",
    "也有可能：causal attention，如果做batch norm，后面的数据会隐晦的影响前面的数据，导致loss突然大幅下降，其实就是信息泄露了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        # features=d_model=512, eps=epsilon 用于分母的非0化平滑\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        # a_2 是一个可训练参数向量，(512)\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        # b_2 也是一个可训练参数向量, (512)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状为(batch.size, sequence.len, 512)\n",
    "        mean = x.mean(-1, keepdim=True) \n",
    "        # 对x的最后一个维度，取平均值，得到tensor (batch.size, seq.len)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 对x的最后一个维度，取标准方差，得(batch.size, seq.len)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "        # 本质上类似于（x-mean)/std，不过这里加入了两个可训练向量\n",
    "        # a_2 and b_2，以及分母上增加一个极小值epsilon，用来防止std为0\n",
    "        # 的时候的除法溢出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这个类有两个可训练参数，a_2和b_2，分别是(512)维度的向量。\n",
    "\n",
    "这个类的操作相对简单，有兴趣的同学，可以尝试一下下面的代码，了解一下具体咋算的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a = np.asarray([[1.,2.],[3., 7.]])\n",
    "print (a) # 2*2\n",
    "#[[1. 2.]\n",
    "# [3. 7.]]\n",
    "\n",
    "b = torch.from_numpy(a)\n",
    "print (b) # torch tensor, 2*2\n",
    "#tensor([[1., 2.],\n",
    "#        [3., 7.]], dtype=torch.float64)\n",
    "\n",
    "mean = b.mean(-1, keepdims=True)\n",
    "print(mean) # (2*1)\n",
    "#tensor([[1.5000],\n",
    "#        [5.0000]], dtype=torch.float64)\n",
    "\n",
    "std = b.std(-1, keepdims=True)\n",
    "print(std) # (2*1)\n",
    "#tensor([[0.7071],\n",
    "#        [2.8284]], dtype=torch.float64)\n",
    "\n",
    "print((b-mean)/std) # (2*2)\n",
    "#tensor([[-0.7071,  0.7071],\n",
    "#        [-0.7071,  0.7071]], dtype=torch.float64)\n",
    "'''\n",
    "(1-1.5)/0.7071\n",
    "-0.7071135624381276\n",
    "\n",
    "(2-1.5)/0.7071\n",
    "0.7071135624381276\n",
    "\n",
    "(3-5)/2.8284\n",
    "-0.7071135624381276\n",
    "\n",
    "(7-5)/2.8284\n",
    "0.7071135624381276\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  SubLayerConnection\n",
    "主要实现两个功能，残差Add以及Norm（使用上面的LayerNorm类）：\n",
    "这个类，没有自己的可训练参数。(self.norm里面，即LayerNorm类，有1024个可训练参数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        # size=d_model=512; dropout=0.1\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size) # (512)，用来定义a_2和b_2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the \"\n",
    "        \"same size.\"\n",
    "        # x is alike (batch.size, sequence.len, 512)\n",
    "        # sublayer是一个具体的MultiHeadAttention\n",
    "        #或者PositionwiseFeedForward对象\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        # x (30, 10, 512) -> norm (LayerNorm) -> (30, 10, 512)\n",
    "        # -> sublayer (MultiHeadAttention or PositionwiseFeedForward)\n",
    "        # -> (30, 10, 512) -> dropout -> (30, 10, 512)\n",
    "        \n",
    "        # 然后输入的x（没有走sublayer) + 上面的结果，\n",
    "        #即实现了残差相加的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.  positionwiseFeedForward\n",
    "\n",
    "可训练参数包括两个权重矩阵，(512, 2048)以及(2048, 512)，以及两个偏移bias向量，(2048)和(512)。则总共的可训练参数的个数是：2*512*2048 + 2048 + 512 = 2,099,712。一个全连接层，轻松达到2百万个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        # d_model = 512\n",
    "        # d_ff = 2048 = 512*4\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        # 构建第一个全连接层，(512, 2048)，其中有两种可训练参数：\n",
    "        # weights矩阵，(512, 2048)，以及\n",
    "        # biases偏移向量, (2048)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # 构建第二个全连接层, (2048, 512)，两种可训练参数：\n",
    "        # weights矩阵，(2048, 512)，以及\n",
    "        # biases偏移向量, (512)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape = (batch.size, sequence.len, 512)\n",
    "        # 例如, (30, 10, 512)\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        # x (30, 10, 512) -> self.w_1 -> (30, 10, 2048)\n",
    "        # -> relu -> (30, 10, 2048) \n",
    "        # -> dropout -> (30, 10, 2048)\n",
    "        # -> self.w_2 -> (30, 10, 512)是输出的shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这个clones方法，实现一个网络的深copy，也就是说一个新的对象，和原来的对象，完全分离，不分享任何存储空间：（从而保证可训练参数，都有自己的取值，梯度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. EncoderLayer\n",
    "可以看到EncoderLayer没有自己独立的可训练参数，所有的参数，都来自其他地方：\n",
    "\n",
    "其一，一个MultiHeadAttention对象，里面有4个Linear network；参数个数是：4*(512*512+512)=1,050,624\n",
    "\n",
    "其二，一个PositionwiseFeedForward对象，里面有2个Linear network；参数个数是：2,099,712。\n",
    "\n",
    "其三，两个SublayerConnection对象，每个对象使用一个LayerNorm对象，LayerNorm里面有两个可训练参数向量a_2和b_2。则，可训练参数个数是：\n",
    "\n",
    "2 * (512 + 512) = 2048。\n",
    "\n",
    "综合上面三个部分：一共有: 1,050,624 + 2,099,712 + 2048 = 3,152,384个参数。（一个EncoderLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and \"\n",
    "    \"feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        # size=d_model=512\n",
    "        # self_attn = MultiHeadAttention对象, first sublayer\n",
    "        # feed_forward = PositionwiseFeedForward对象，second sublayer\n",
    "        # dropout = 0.1 (e.g.)\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        # 使用深度克隆方法，完整地复制出来两个SublayerConnection\n",
    "        self.size = size # 512\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        # x shape = (30, 10, 512)\n",
    "        # mask 是(batch.size, 10,10)的矩阵，类似于当前一个词w，有哪些词是w可见的\n",
    "        # 源语言的序列的话，所有其他词都可见，除了\"<blank>\"这样的填充；\n",
    "        # 目标语言的序列的话，所有w的左边的词，都可见。\n",
    "        x = self.sublayer[0](x, \n",
    "          lambda x: self.self_attn(x, x, x, mask))\n",
    "        # x (30, 10, 512) -> self_attn (MultiHeadAttention) \n",
    "        # shape is same (30, 10, 512) -> SublayerConnection \n",
    "        # -> (30, 10, 512)\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "        # x 和feed_forward对象一起，给第二个SublayerConnection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Encoder类实现\n",
    "\n",
    "数数这里的可训练参数的个数：\n",
    "\n",
    "6层EncoderLayers: 每一层是 3,152,384个参数；\n",
    "\n",
    "LayerNorm里面有两个512维度的向量，有1024个参数；\n",
    "\n",
    "则一共有：3,152,384*6 + 1,024 = 18,915,328个参数(Encoder类)。\n",
    "\n",
    "再假设，源语言词表大小是30,000词，则embeddings部分有30,000 * 512 = 15,360,000个参数（词嵌入矩阵）。\n",
    "\n",
    "这样的话，源语言的Embeddings+Encoder，一共有34,275,328个可训练参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        # layer = one EncoderLayer object, N=6\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N) \n",
    "        # 深copy，N=6，\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        # 定义一个LayerNorm，layer.size=d_model=512\n",
    "        # 其中有两个可训练参数a_2和b_2\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        # x is alike (30, 10, 512)\n",
    "        # (batch.size, sequence.len, d_model)\n",
    "        # mask是类似于(batch.size, 10, 10)的矩阵\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            # 进行六次EncoderLayer操作\n",
    "        return self.norm(x)\n",
    "        # 最后做一次LayerNorm，最后的输出也是(30, 10, 512) shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.DecoderLayer\n",
    "这里估计一下DecoderLayer的可训练参数的个数：\n",
    "\n",
    "其一，两个MultiHeadAttention对象，每个对象有1,050,624个参数；\n",
    "\n",
    "其二，feedforward对象，2,099,712个参数；\n",
    "\n",
    "其三，三个SublayerConnection, 3*(512+512) = 3,072个参数。\n",
    "\n",
    "综合起来有，2*1,050,624 + 2,099,712 + 3,072 = 4,204,032个参数 (DecoderLayer)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, \"\n",
    "    \"and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, \n",
    "      feed_forward, dropout):\n",
    "      # size = d_model=512,\n",
    "      # self_attn = one MultiHeadAttention object，目标语言序列的\n",
    "      # src_attn = second MultiHeadAttention object, 目标语言序列\n",
    "      # 和源语言序列之间的\n",
    "      # feed_forward 一个全连接层\n",
    "      # dropout = 0.1\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size # 512\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "        # 需要三个SublayerConnection, 分别在\n",
    "        # self.self_attn, self.src_attn, 和self.feed_forward\n",
    "        # 的后边\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory # (batch.size, sequence.len, 512) \n",
    "        # 来自源语言序列的Encoder之后的输出，作为memory\n",
    "        # 供目标语言的序列检索匹配：（类似于alignment in SMT)\n",
    "        x = self.sublayer[0](x, \n",
    "          lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 通过一个匿名函数，来实现目标序列的自注意力编码\n",
    "        # 结果扔给sublayer[0]:SublayerConnection\n",
    "        x = self.sublayer[1](x, \n",
    "          lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        # 通过第二个匿名函数，来实现目标序列和源序列的注意力计算\n",
    "        # 结果扔给sublayer[1]:SublayerConnection\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "        # 走一个全连接层，然后\n",
    "        # 结果扔给sublayer[2]:SublayerConnection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.Decoder\n",
    "Decoder类的可训练参数的个数是：\n",
    "\n",
    "其一，六层DecoderLayer: 6*4,204,032；\n",
    "\n",
    "其二，一个LayerNorm, 1,024个参数；\n",
    "\n",
    "则一共有：6*4,204,032 + 1024 = 25,225,216个参数。\n",
    "\n",
    "如果假设目标语言也是30,000的词表大小，则又增加30,000 * 512=15,360,000个参数，从而有：40,585,216个可训练参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        # layer = DecoderLayer object\n",
    "        # N = 6\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        # 深度copy六次DecoderLayer\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        # 初始化一个LayerNorm\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "            # 执行六次DecoderLayer\n",
    "        return self.norm(x)\n",
    "        # 执行一次LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.Generator\n",
    "里面有一个全连接层，参数个数是，一个权重矩阵 512*trg_vocab_size，一个偏移向量 (trg_vocab_size)。\n",
    "\n",
    "如果假设trg_vocab_size=30,000, 则可训练参数的个数是：\n",
    "\n",
    "512*30000 + 30000 = 15,390,000个 (Generator)。\n",
    "\n",
    "到目前位置，我们基本阐述完毕Encoder, Decoder，Generator，以及source sequence and target sequence的Embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        # d_model=512\n",
    "        # vocab = 目标语言词表大小\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        # 定义一个全连接层，可训练参数个数是(512 * trg_vocab_size) + \n",
    "        # trg_vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "        # x 类似于 (batch.size, sequence.length, 512)\n",
    "        # -> proj 全连接层 (30, 10, trg_vocab_size) = logits\n",
    "        # 对最后一个维度执行log_soft_max\n",
    "        # 得到(30, 10, trg_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.EncoderDecoder\n",
    "\n",
    "类里面，有五大属性，分别的可训练参数的个数是：\n",
    "\n",
    "（假设源语言和目标语言的词表都是30,000)\n",
    "\n",
    "源语言的Embeddings+Encoder，一共有**34,275,328**个可训练参数 (Embedding+Encoder);\n",
    "\n",
    "假设目标语言也是30,000的词表大小，则又增加30,000 \\* 512=15,360,000个参数，从而有：**40,585,216**个可训练参数(Embedding+Decoder);\n",
    "\n",
    "如果假设trg\\_vocab\\_size=30,000, 则可训练参数的个数是：\n",
    "\n",
    "512\\*30000 + 30000 = **15,390,000**个(Generator).\n",
    "\n",
    "**合计为：90,250,544个参数，9千万。稍稍扩大一下词表大小，轻松一亿个参数。所以，170亿参数，也没啥。**\n",
    "\n",
    "当然，实际词嵌入矩阵，一般的维度小于512，可以是100维，200维等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. \n",
    "    Base for this and many other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, \n",
    "      src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        # Encoder对象\n",
    "        self.decoder = decoder\n",
    "        # Decoder对象\n",
    "        self.src_embed = src_embed\n",
    "        # 源语言序列的编码，包括词嵌入和位置编码\n",
    "        self.tgt_embed = tgt_embed\n",
    "        # 目标语言序列的编码，包括词嵌入和位置编码\n",
    "        self.generator = generator\n",
    "        # 生成器\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "        # 先对源语言序列进行编码，\n",
    "        # 结果作为memory传递给目标语言的编码器\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # src = (batch.size, seq.length)\n",
    "        # src_mask 负责对src加掩码\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "        # 对源语言序列进行编码，得到的结果为\n",
    "        # (batch.size, seq.length, 512)的tensor\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), \n",
    "          memory, src_mask, tgt_mask)\n",
    "        # 对目标语言序列进行编码，得到的结果为\n",
    "        # (batch.size, seq.length, 512)的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.mask的构造函数：\n",
    "这里的k=1，1号对角线，不太容易理解：举例子如下：![decoder因果矩阵](https://i.imgur.com/LXozM1b.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    # e.g., size=10\n",
    "    attn_shape = (1, size, size) # (1, 10, 10)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    # triu: 负责生成一个三角矩阵，k-th对角线以下都是设置为0 \n",
    "    # 上三角中元素为1.\n",
    "    \n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "    # 反转上面的triu得到的上三角矩阵，修改为下三角矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.构造EncoderDecoder对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    # src_vocab = 源语言词表大小\n",
    "    # tgt_vocab = 目标语言词表大小\n",
    "    \n",
    "    c = copy.deepcopy # 对象的深度copy/clone\n",
    "    attn = MultiHeadedAttention(h, d_model) # 8, 512\n",
    "    # 构造一个MultiHeadAttention对象\n",
    "    \n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    # 512, 2048, 0.1\n",
    "    # 构造一个feed forward对象\n",
    "\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    # 位置编码\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model # EncoderDecoder 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "然后调用make_model，得到一个对象：\n",
    "'''\n",
    "\n",
    "tmp_model = make_model(10, 10, 2)\n",
    "# src_vocab_size = 10\n",
    "# tgt_vocab_size = 10\n",
    "# N = 2, number for EncoderLayer and DecoderLayer\n",
    "\n",
    "for name, param in tmp_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data.shape)\n",
    "    else:\n",
    "        print ('no gradient necessary', name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "最后，看全量模型：\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(30000, 30000, 6) \n",
    "# src_vocab_size=30000, tgt_vocab_size=30000, N=6\n",
    "#None\n",
    "for name, param in tmp_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data.shape)\n",
    "    else:\n",
    "        print ('no gradient necessary', name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-619b3ae59aa1>:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Untrained Model Prediction: tensor([[0, 2, 2, 2, 0, 2, 0, 2, 0, 2]])\n",
      "Example Untrained Model Prediction: tensor([[ 0, 10, 10, 10, 10, 10, 10, 10, 10, 10]])\n",
      "Example Untrained Model Prediction: tensor([[0, 9, 9, 9, 9, 4, 8, 5, 4, 8]])\n",
      "Example Untrained Model Prediction: tensor([[0, 5, 5, 4, 2, 5, 4, 5, 5, 5]])\n",
      "Example Untrained Model Prediction: tensor([[0, 4, 7, 5, 0, 7, 7, 5, 0, 7]])\n",
      "Example Untrained Model Prediction: tensor([[ 0, 10,  7,  2,  2,  2,  2,  2,  2,  2]])\n",
      "Example Untrained Model Prediction: tensor([[0, 7, 5, 7, 5, 7, 5, 7, 5, 7]])\n",
      "Example Untrained Model Prediction: tensor([[0, 4, 4, 4, 4, 4, 4, 4, 4, 4]])\n",
      "Example Untrained Model Prediction: tensor([[ 0,  7, 10,  6, 10,  6, 10,  6, 10,  6]])\n",
      "Example Untrained Model Prediction: tensor([[0, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "我们用一个未训练的transformer实例测试，输出是随机的\n",
    "'''\n",
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "RUN_EXAMPLES = True\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "show_example(run_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.构建batch和mask\n",
    "到目前为止，Transformer的Embeddings, Encoder, Decoder, Generator等部分的细节代码都阐述完毕了。按照pytorch的标准写法，一般先是独立定义一个个的class，class里面包括__init__和forward两个重要的函数。__init__负责“武装自己”，forward负责“处理别人”。\n",
    "\n",
    "接下来讲述损失函数的定义，训练和实际解码测试的流程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        # src: 源语言序列，(batch.size, src.seq.len)\n",
    "        # 二维tensor，第一维度是batch.size；第二个维度是源语言句子的长度\n",
    "        # 例如：[ [2,1,3,4], [2,3,1,4] ]这样的二行四列的，\n",
    "        # 1-4代表每个单词word的id\n",
    "        \n",
    "        # trg: 目标语言序列，默认为空，其shape和src类似\n",
    "        # (batch.size, trg.seq.len)，\n",
    "        #二维tensor，第一维度是batch.size；第二个维度是目标语言句子的长度\n",
    "        # 例如trg=[ [2,1,3,4], [2,3,1,4] ] for a \"copy network\"\n",
    "        # (输出序列和输入序列完全相同）\n",
    "        \n",
    "        # pad: 源语言和目标语言统一使用的 位置填充符号，'<blank>'\n",
    "        # 所对应的id，这里默认为0\n",
    "        # 例如，如果一个source sequence，长度不到4，则在右边补0\n",
    "        # [1,2] -> [1,2,0,0]\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)  #pad的位置是0\n",
    "        # src = (batch.size, seq.len) -> != pad -> \n",
    "        # (batch.size, seq.len) -> usnqueeze ->\n",
    "        # (batch.size, 1, seq.len) 相当于在倒数第二个维度扩展\n",
    "        # e.g., src=[ [2,1,3,4], [2,3,1,4] ]对应的是\n",
    "        # src_mask=[ [[1,1,1,1], [1,1,1,1]] ]\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1] # 重要\n",
    "            # trg 相当于目标序列的前N-1个单词的序列\n",
    "            #（去掉了最后一个词）\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # trg_y 相当于目标序列的后N-1个单词的序列\n",
    "            # (去掉了第一个词）\n",
    "            # 目的是(src + trg) 来预测出来(trg_y)，\n",
    "            # 这个在illustrated transformer中详细图示过。\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "                ## ntokens = 6 (trg_y中非'<blank>'的token的个数)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod   #静态方法无需实例化\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        # 这里的tgt类似于：\n",
    "        #[ [2,1,3], [2,3,1] ] （最初的输入目标序列，分别去掉了最后一个词\n",
    "        # pad=0, '<blank>'的id编号\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        # 得到的tgt_mask类似于\n",
    "        # tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)\n",
    "        # shape=(2,1,3)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        # 先看subsequent_mask, 其输入的是tgt.size(-1)=3\n",
    "        # 这个函数的输出为= tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]]], dtype=torch.uint8)\n",
    "        # type_as 把这个tensor转成tgt_mask.data的type(也是torch.uint8)\n",
    "        \n",
    "        # 这样的话，&的两边的tensor分别是(2,1,3), (1,3,3);\n",
    "        #tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)\n",
    "        #and\n",
    "        # tensor([[[1, 0, 0], [1, 1, 0], [1, 1, 1]]], dtype=torch.uint8)\n",
    "        \n",
    "        # (2,3,3)就是得到的tensor\n",
    "        # tgt_mask.data = tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]],\n",
    "\n",
    "        #[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]]], dtype=torch.uint8)\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要仔细看的是上面的 & 这个算子\n",
    "![mask算子](https://i.imgur.com/nvwUwIA.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "# &算子左边，当tgt_mask,shape=(3,1,4)是：\n",
    "# [1 1 1 1], \n",
    "# [1 1 1 0], # attention here\n",
    "# [1 1 1 1]\n",
    "# 类似于第二个序列的最后一个词是'<blank>' 。 \n",
    "\n",
    "# &算子右边，的Variable()内部，shape=(1,4,4)，\n",
    "# 是： \n",
    "# 1 0 0 0 \n",
    "# 1 1 0 0 \n",
    "# 1 1 1 0 \n",
    "# 1 1 1 1 \n",
    "# 的时候。\n",
    "\n",
    "# 这两者的&的结果是 (3, 4, 4)： \n",
    "# 1 0 0 0 \n",
    "# 1 1 0 0 \n",
    "# 1 1 1 0 \n",
    "# 1 1 1 1 \n",
    "\n",
    "# 1 0 0 0 \n",
    "# 1 1 0 0 \n",
    "# 1 1 1 0 \n",
    "# 1 1 1 0 # attention here， 因为第二个序列只有3个词\n",
    "\n",
    "# 1 0 0 0 \n",
    "# 1 1 0 0 \n",
    "# 1 1 1 0 \n",
    "# 1 1 1 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # optimizer = Adam (Parameter Group 0\n",
    "        #    amsgrad: False\n",
    "        #    betas: (0.9, 0.98)\n",
    "        #    eps: 1e-09\n",
    "        #    lr: 0\n",
    "        #    weight_decay: 0\n",
    "        #)\n",
    "        self._step = 0\n",
    "        self.warmup = warmup # e.g., 4000 轮 热身\n",
    "        self.factor = factor # e.g., 2\n",
    "        self.model_size = model_size # 512\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate`(learning rate) above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, \n",
    "            betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), \n",
    "  [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('learning rate')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![学习率对比](https://i.imgur.com/8A81P3t.png)\n",
    "# 16.LabelSmoothing标签平滑 正则化\n",
    "这个方法虽然会损害Perplexity（困惑度，交叉熵指数形式），但是会增加最后的BLEU score.\n",
    "\n",
    "另外，这个名字不好，无论如何需要加上 KLLoss, 类似于：\n",
    "\n",
    "LabelSmoothing -> LabelSmoothingKLLoss 其实是最终计算损失函数的。\n",
    "\n",
    "是一种正则化方法, 为了降低模型过拟合(overfitting)\n",
    "\n",
    "出自inception v3，Transformer中就用到了\n",
    "\n",
    "我们用softmax最后去输出一个概率的时候，label是正确的是1，错误的是0。也就是说,我们的训练是想让正确的那个分类的softmax的值逼近于1. 但我们知道softmax是很难逼近于1的，需要输出接近无限大的时候，才能逼近于1，使得训练比较困难。那么我们就不要搞成0和1这样，把1的值稍微降一些，降成0.9。在Transformer中，甚至降成了0.1，即对于正确的词，需要softmax输出>0.1就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx # '<blank>' 的id\n",
    "        self.confidence = 1.0 - smoothing # 自留的概率值、得分 e.g. 0.6\n",
    "        self.smoothing = smoothing # 均分出去的概率值，得分 e.g. 0.4\n",
    "        self.size = size # target vocab size 目标语言词表大小\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        \"in real-world case: 真实情况下\"\n",
    "        #  x的shape为(batch.size * seq.len, target.vocab.size)\n",
    "        # y的shape是(batch.size * seq.len)\n",
    "        \n",
    "        # x=logits，(seq.len, target.vocab.size)\n",
    "        # 每一行，代表一个位置的词\n",
    "        # 类似于：假设seq.len=3, target.vocab.size=5\n",
    "        # x中保存的是log(prob)\n",
    "        #x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "        \n",
    "        # target 类似于：\n",
    "        # target = tensor([2, 1, 0])，torch.size=(3)\n",
    "        \n",
    "        assert x.size(1) == self.size # 目标语言词表大小\n",
    "        true_dist = x.data.clone()\n",
    "        # true_dist = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "        \n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # true_dist = tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        \n",
    "        # 注意，这里分母target.vocab.size-2是因为\n",
    "        # (1) 最优值 0.6要占一个位置；\n",
    "        # (2) 填充词 <blank> 要被排除在外\n",
    "        # 所以被激活的目标语言词表大小就是self.size-2\n",
    "        \n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), \n",
    "          self.confidence)\n",
    "        # target.data.unsqueeze(1) -> \n",
    "        # tensor([[2],\n",
    "        #[1],\n",
    "        #[0]]); shape=torch.Size([3, 1])  \n",
    "        # self.confidence = 0.6\n",
    "        \n",
    "        # 根据target.data的指示，按照列优先(1)的原则，把0.6这个值\n",
    "        # 填入true_dist: 因为target.data是2,1,0的内容，\n",
    "        # 所以，0.6填入第0行的第2列（列号，行号都是0开始）\n",
    "        # 0.6填入第1行的第1列\n",
    "        # 0.6填入第2行的第0列：\n",
    "        # true_dist = tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.6000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "          \n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "        #[0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.0000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        # 设置true_dist这个tensor的第一列的值全为0\n",
    "        # 因为这个是填充词'<blank>'所在的id位置，不应该计入\n",
    "        # 目标词表。需要注意的是，true_dist的每一列，代表目标语言词表\n",
    "        #中的一个词的id\n",
    "        \n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        # mask = tensor([[2]]), 也就是说，最后一个词 2,1,0中的0，\n",
    "        # 因为是'<blank>'的id，所以通过上面的一步，把他们找出来\n",
    "        \n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "            # 当target reference序列中有0这个'<blank>'的时候，则需要把\n",
    "            # 这一行的值都清空。\n",
    "            # 在一个batch里面的时候，可能两个序列长度不一，所以短的序列需要\n",
    "            # pad '<blank>'来填充，所以会出现类似于(2,1,0)这样的情况\n",
    "            # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "            # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "            # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, \n",
    "          Variable(true_dist, requires_grad=False))\n",
    "          # 这一步就是调用KL loss来计算\n",
    "          # x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "          #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "          #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "          \n",
    "          # true_dist=tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "          # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "          # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "          # 之间的loss了。细节可以参考我的那篇illustrated transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of label smoothing.\n",
    "crit = LabelSmoothing(5, 0, 0.4) # trg.vocab.size=5, pad_idx=0, \n",
    "# smooth=0.4 and thus confidence=0.6\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "                             # predicted logits tensor\n",
    "\"smooth only and important\"\n",
    "predict = predict.masked_fill(predict == 0, 1e-9) # \"smooth only\"\n",
    "\n",
    "v = crit(Variable(predict.log()), \n",
    "         Variable(torch.LongTensor([2, 1, 0])))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "plt.imshow(crit.true_dist)\n",
    "#true_dist=tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "#        [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "#        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]), \n",
    "# true_dist.shape=torch.Size([3, 5])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中，第0列，都是0，代表了'<blank>'的id位置；最后一行都是0，代表的是(2,1,0)中的最后一个词，对应的也是'<blank>'，所以最后一行都被设置为0.\n",
    "\n",
    "![picture19](https://i.imgur.com/2n111Kg.jpg)![picture20](https://i.imgur.com/s0clt7O.jpg)\n",
    "\n",
    "这个类LabelSmoothing，一方面对label进行平滑，如果Model对于一个结果非常确信，则loss反而惩罚它（貌似缺少了多样性）；另外一方面则是对loss进行计算的。\n",
    "下图很好的说明了，“过犹不及”的思想："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 500), [loss(x) for x in range(1, 500)])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture20](https://i.imgur.com/HnQMKO2.jpg)\n",
    "\n",
    "下面通过一个简单的例子来整体串一下transformer:\n",
    "\n",
    "假设我们的目的是构建一个copy network，输入和输出完全一致：\n",
    "\n",
    "先构建数据集：\n",
    "\n",
    "例如：\n",
    "\n",
    "src = tensor([[1, 4, 2, 1],\n",
    "[1, 4, 4, 4]], dtype=torch.int32)\n",
    "\n",
    "tgt = tensor([[1, 4, 2, 1],\n",
    "[1, 4, 4, 4]], dtype=torch.int32)\n",
    "\n",
    "假设batch.size=2, source.vocab.size=4=target.vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy\n",
    "            (np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装的基于batch来计算loss的类：\n",
    "需要注意，新版本pytorch，使用.item()来取具体的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator # Generator 对象, linear+softmax\n",
    "        self.criterion = criterion # LabelSmooth对象，计算loss\n",
    "        self.opt = opt # NormOpt对象，优化算法对象\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        # e.g., x为(2,3,8), batch.size=2, seq.len=3, d_model=8\n",
    "        # y = tensor([[4, 2, 1],\n",
    "        #[4, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # norm: (y=trg_y中非'<blank>'的token的个数)\n",
    "        \"attention here\"\n",
    "        \n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm.item()\n",
    "        # 变形后，x类似于(batch.size*seq.len, target.vocab.size)\n",
    "        # y为(target.vocab.size)\n",
    "        # 然后调用LabelSmooth来计算loss\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        #return loss.data[0] * norm\n",
    "        \"attention here\"\n",
    "        return loss.data.item() * norm.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面看一个相对完整的训练loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(aepoch, data_iter, model, loss_compute):\n",
    "\n",
    "    \"Standard Training and Logging Function\"\n",
    "    # data_iter = 所有数据的打包\n",
    "    # model = EncoderDecoder 对象\n",
    "    # loss_compute = SimpleLossCompute对象\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # 对每个batch循环\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        # 使用目前的model，对batch.src+batch.trg进行forward\n",
    "                            \n",
    "        # e.g.,\n",
    "        # batch.src (2,4) = tensor([[1, 4, 2, 1],\n",
    "        # [1, 4, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # batch.trg (2,3) = tensor([[1, 4, 2],\n",
    "        # [1, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # batch.src_mask (2,1,4) = tensor([[[1, 1, 1, 1]],\n",
    "        # [[1, 1, 1, 1]]], dtype=torch.uint8)\n",
    "        \n",
    "        # batch.trg_mask (2,3,3) = tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]],\n",
    "\n",
    "        #[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "         #[1, 1, 1]]], dtype=torch.uint8)\n",
    "         \n",
    "        # and out (2,3,8):\n",
    "        # out = tensor([[[-0.4749, -0.4887,  0.1245, -0.4042,  0.5301,  \n",
    "        #   1.7662, -1.6224, 0.5694],\n",
    "        # [ 0.4683, -0.7813,  0.2845,  0.4464, -0.3088, -0.1751, -1.6643,\n",
    "        #   1.7303],\n",
    "         #[-1.1600, -0.2348,  1.0631,  1.3192, -0.9453,  0.3538,  0.7051...                 \n",
    "        \n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        # out和trg_y计算Loss\n",
    "        # ntokens = 6 (trg_y中非'<blank>'的token的个数)\n",
    "        # 注意，这里是token,不是unique word\n",
    "        # 例如[ [ [1, 2, 3], [2,3,4] ]中有6个token,而只有4个unique word\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            \"attention here 这里隐藏一个bug\"\n",
    "            #print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "            #        (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            print ('epoch step: {}:{} Loss: {}/{}, tokens per sec: {}/{}'\n",
    "                    .format(aepoch, i, loss, batch.ntokens, \n",
    "                    tokens, elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练的准备工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the simple copy task.\n",
    "V = 5 # here V is the vocab size of source and target languages (sequences)\n",
    "criterion = LabelSmoothing(size=V, \n",
    "    padding_idx=0, smoothing=0.01) # 创建损失函数计算对象\n",
    "    \n",
    "model = make_model(V, V, N=2, d_model=8, d_ff=16, h=2) \n",
    "# EncoderDecoder对象构造\n",
    "'''\n",
    "in make_model: src_vocab_size=11, tgt_vocab_size=11, \n",
    "    N=2, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "'''\n",
    "\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), \n",
    "        lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "# 模型最优化算法的对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练流程代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfun = SimpleLossCompute(model.generator, \n",
    "        criterion, model_opt)\n",
    "if True:\n",
    "    print ('start model training...')\n",
    "    for epoch in range(10):\n",
    "        print ('epoch={}, training...'.format(epoch))\n",
    "        model.train() # set the model into \"train\" mode\n",
    "        # 设置模型进入训练模式\n",
    "        \n",
    "        #lossfun = SimpleLossCompute(model.generator, \n",
    "        #    criterion, model_opt) # 不需要在这里定义lossfun\n",
    "        \n",
    "        run_epoch(epoch, data_gen(V, 4, 2, 2), model, lossfun)\n",
    "        # 重新构造一批数据，并执行训练\n",
    "        \n",
    "        model.eval() # 模型进入evaluation模式 (dropout，反向传播无效）\n",
    "        print ('evaluating...')\n",
    "        print(run_epoch(epoch, data_gen(V, 4, 2, 2), model, \n",
    "                        SimpleLossCompute(model.generator, \n",
    "                        criterion, None)))\n",
    "        # 这里None表示优化函数为None，所以不进行参数更新 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贪心解码算法（只看top-1的结果）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask) \n",
    "    # 源语言的一个batch\n",
    "    # 执行encode编码工作，得到memory \n",
    "    # shape=(batch.size, src.seq.len, d_model)\n",
    "    \n",
    "    # src = (1,4), batch.size=1, seq.len=4\n",
    "    # src_mask = (1,1,4) with all ones\n",
    "    # start_symbol=1\n",
    "    \n",
    "    print ('memory={}, memory.shape={}'.format(memory, \n",
    "        memory.shape))\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 最初ys=[[1]], size=(1,1); 这里start_symbol=1\n",
    "    print ('ys={}, ys.shape={}'.format(ys, ys.shape))\n",
    "    for i in range(max_len-1): # max_len = 5\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        # memory, (1, 4, 8), 1=batch.size, 4=src.seq.len, 8=d_model\n",
    "        # src_mask = (1,1,4) with all ones\n",
    "        # out, (1, 1, 8), 1=batch.size, 1=seq.len, 8=d_model                             \n",
    "        print ('out={}, out.shape={}'.format(out, out.shape))\n",
    "        prob = model.generator(out[:, -1]) \n",
    "        # pick the right-most word\n",
    "        # (1=batch.size,8) -> generator -> prob=(1,5) 5=trg.vocab.size\n",
    "        # -1 for ? only look at the final (out) word's vector\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        # word id of \"next_word\"\n",
    "        ys = torch.cat([ys, \n",
    "          torch.ones(1, 1).type_as(src.data).fill_(next_word)], \n",
    "          dim=1)\n",
    "        # ys is in shape of (1,2) now, i.e., 2 words in current seq\n",
    "    return ys\n",
    "\n",
    "if True:\n",
    "    model.eval()\n",
    "    src = Variable(torch.LongTensor([[1,2,3,4]]))\n",
    "    src_mask = Variable(torch.ones(1, 1, 4))\n",
    "    print(greedy_decode(model, src, src_mask, max_len=5, \n",
    "        start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
