#! https://zhuanlan.zhihu.com/p/547725524
- [3.PyTorch中搭建分类网络实例](#3pytorch中搭建分类网络实例)
  - [搭建神经网络](#搭建神经网络)
    - [获取训练设备](#获取训练设备)
    - [定义分类模型网络](#定义分类模型网络)
    - [网络层详解](#网络层详解)

# 3.PyTorch中搭建分类网络实例
[官方教程](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)

## 搭建神经网络
`torch.nn`命名空间提供了搭建神经网络的所有的模块，Pytorch中每个模块都是`nn.Module`的子类。
一个神经网络本身可以看作一个模块，同时又可能包含其他模块，比如GAN模型包含一个判别器一个生成器。
接下来解释如何就FashionMNIST构建一个分类数据集：
```python
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
```

### 获取训练设备
```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")
```

### 定义分类模型网络
每个网络都需要继承自`nn.Module`父类，且需要实现两个方法`__init__`和`forward`
`forward`不需要显式调用，实例化一个对象以后，可以像`call`一样，直接object（）即可

最后一层输出一般称为`logits`

```python
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__() #调用父类的初始化函数
        self.flatten = nn.Flatten()  
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x) #
        return logits
```

实例化对象，打印模型每一层的信息
```python
'''
调用init函数，把所有参数放到device里面
'''
model = NeuralNetwork().to(device) 
'''
每个Model的子模块，

'''
print(model) 
```
Out:
```python
NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)
```
tensorflow中的summary还可以看到可训练、不可训练的参数数目
pytorch有人实现了类似的方法，安装torchsummary库即可，调用summary(model)实现类似效果
![pytorch summary模块](https://pic4.zhimg.com/80/v2-9bd9861a7171960d9d5a52b1394ae6e0.png)

再次强调：如果想要调用`forward`方法，这里直接传入输入数据即可，同时系统会自动执行一些反向传播的操作，不需要显式地调用`model.forward`

接下来进行多分类预测：
```python
X = torch.rand(1, 28, 28, device=device)
logits = model(X) #有人翻译为“能量”
pred_probab = nn.Softmax(dim=1)(logits)
y_pred = pred_probab.argmax(1)  #取最大值
print(f"Predicted class: {y_pred}")
```

### 网络层详解
```python
input_image = torch.rand(3,28,28) #(0,1)均匀分布
print(input_image.size())

Output:
torch.Size([3, 28, 28])
```

[`nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)
这里`Flatten`层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。
```python
'''
torch.nn.Flatten(start_dim=1, end_dim=- 1)
保留第0维度的batch_size，其他压平
'''
```

实例：
```python
flatten = nn.Flatten()
flat_image = flatten(input_image)
print(flat_image.size())

Output:
torch.Size([3, 784])
```

[`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)

`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`
Linear有两个属性变量：
1. Linear.weight() -> 权重
2. Linear.bias() -> 偏置



实例：
```python
layer1 = nn.Linear(in_features=28*28, out_features=20)
hidden1 = layer1(flat_image)
print(hidden1.size())

Output:
torch.Size([3, 20])
```

[`nn.Relu`](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#nn-relu)
```python
print(f"Before ReLU: {hidden1}\n\n")
'''
注意下面其实是先初始化一个relu对象
'''
hidden1 = nn.ReLU()(hidden1)
print(f"After ReLU: {hidden1}")
```

[`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)
有序容器，和其他容器一起放在之后看源码
```python
seq_modules = nn.Sequential(
    flatten,
    layer1,
    nn.ReLU(),
    nn.Linear(20, 10)
)
input_image = torch.rand(3,28,28)
logits = seq_modules(input_image)
```

[`nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)

```python
softmax = nn.Softmax(dim=1)
pred_probab = softmax(logits)
```

`named_parameters()` -> tuple

查看模型参数
```python
print(f"Model structure: {model}\n\n")

for name, param in model.named_parameters():
    print(f"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \n")


Output:
Model structure: NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)


Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0111,  0.0082,  0.0263,  ...,  0.0186, -0.0031,  0.0357],
        [ 0.0269, -0.0089, -0.0169,  ...,  0.0106,  0.0155,  0.0056]],
       device='cuda:0', grad_fn=<SliceBackward0>)

Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0088, -0.0355], device='cuda:0', grad_fn=<SliceBackward0>)

Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0253, -0.0208,  0.0284,  ..., -0.0284, -0.0303, -0.0415],
        [ 0.0411,  0.0193,  0.0100,  ..., -0.0380,  0.0095,  0.0436]],
       device='cuda:0', grad_fn=<SliceBackward0>)

Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0363, 0.0002], device='cuda:0', grad_fn=<SliceBackward0>)

Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0291,  0.0439,  0.0203,  ..., -0.0407, -0.0169, -0.0238],
        [ 0.0045,  0.0351,  0.0213,  ..., -0.0369, -0.0230, -0.0066]],
       device='cuda:0', grad_fn=<SliceBackward0>)

Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0253,  0.0213], device='cuda:0', grad_fn=<SliceBackward0>)
```