#! https://zhuanlan.zhihu.com/p/553455186
# 9.详细推导自动微分Forward与Reverse模式
本次讲了一篇机器学习自动微分的文章：

*Automatic Differentiation in Machine Learning: a Survey*

[arxiv链接](https://arxiv.org/abs/1502.05767)

本节课不涉及代码

**重点掌握：**
1. 自动微分和符号微分、数值微分的区别
2. 自动微分的前向模式和后向模式

![自动微分和符号微分、数值微分的区别](https://pic4.zhimg.com/80/v2-21c7fda785013aa1eb0685e995d36971.png)

前向传播是由前往后计算，输入节点的变化对所有的中间节点和输出节点的影响

也就是计算所有节点对于输入节点的导数，这里计算对于x1的导数

由于是有向无环图，在前向传播中不会涉及梯度累积的情况

**可以看出前向传播的特点：**
1. 前向传播(forward mode)和前向计算是可以同时进行的，每算出一个前向计算的值，根据链式法则forward mode就可以算出当前节点对于输入节点的偏导数
2. 每次只能算出一个输入节点的导数
![前向计算和前向传播](https://pic4.zhimg.com/80/v2-785afe2655e783464bec95d074472727.png)

我们可以借助**对偶数**同时进行前向计算和前向传播

对偶数可以看这篇[博客](https://zhuanlan.zhihu.com/p/380140763)

最后我们可以推导出一个公式：
$$
f\left(a_{1}+b_{1} \varepsilon\right)=f\left(a_{1}\right)+f^{\prime}\left(a_{1}\right)\left(b_{1} \varepsilon\right)
$$

我们令a1=x，b1=1，带入可得：
$$
f(x+\varepsilon)=f(x)+f^{\prime}(x) \varepsilon
$$

也就是说，我们使用对偶数计算，最后**常数部分就是前向计算的值**，**对偶数系数就是前向传播的梯度**

举个例子:

假设f(x) = x^2 + 2*x，求f(x)在x=2的导数

f(2 + eps) = (2 + eps)^2 + 2 * (2 + eps) = 4 + 4 * eps + eps^2 + 4 + 2*eps = 8 + 6 * eps

f(2) = 8

f'(2) = 6
![对偶数](https://pic4.zhimg.com/80/v2-45407a368ccd63b12b27469bb8c5b71e.png)

pytorch和tensorflow这些深度学习框架用的都是**前向计算**+**反向传播**，分成两步来做
下面是反向传播的过程

计算输出节点关于每一个中间节点和输入节点的导数，也就是中间节点和输入节点对于输出节点的影响

**个人总结：前向传播和反向传播其实就是链式法则从前往后结合和从后往前结合**

注意：反向传播可能有**梯度累加**的情况发生，在后向推的时候，有两个节点可能会同时汇入一个节点

比方说：$\frac{\partial y}{\partial v_{0}}=\frac{\partial y}{\partial v_{2}} \frac{\partial v_{2}}{\partial v_{0}}+\frac{\partial y}{\partial v_{3}} \frac{\partial v_{3}}{\partial v_{0}}$

其中y对于v0的偏导数，和v2,v3都有关系，也就是说对于父节点的梯度要算两次，并且累积起来

又如下图的v0,有两个子节点v2和v3，在最上面的一行v0要把下面的v0加起来

**可以看出反向传播的特点：**
1. 可以一次性算出所有输入节点的导数
2. 必须先把前向运算的所有节点值算出后才能进行反向传播
![反向传播](https://pic4.zhimg.com/80/v2-c34786a797785ae1a502f2ff0fc8cd47.png)

前向传播和反向传播在**计算量**上有什么区别呢？（直接贴图，懒得手打了）
![计算量比较](https://pic4.zhimg.com/80/v2-fdceb2395e4e6bcfb9fb58cee352099c.png)
上图做了一些假设（虽然大部分情况下均成立），实际上还是需要具体分析

矩阵从小维度到大维度乘会节省计算量，这也是矩阵乘法交换律的一个用处