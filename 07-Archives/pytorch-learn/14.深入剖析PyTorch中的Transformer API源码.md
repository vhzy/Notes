#! https://zhuanlan.zhihu.com/p/554726371
# 14.深入剖析PyTorch中的Transformer API源码

seq2seq，可以用不同的网络来做，其各自特点如下：
![CNN](https://pic4.zhimg.com/80/v2-a7200f5d1804361343e0338be4288d1e.png)

![RNN](https://pic4.zhimg.com/80/v2-4c0e4849c735f28c90bbb5ffb59ebe53.png)

![transformer](https://pic4.zhimg.com/80/v2-f3d839a7c4869a60bf37ac9317f69e57.png)

推荐一篇讲解transformer的[博客](https://wmathor.com/index.php/archives/1438/)，自顶而下的
实现了transformer

也可以看看[中文版解析](https://zhuanlan.zhihu.com/p/107889011)


![transformer架构](https://pic4.zhimg.com/80/v2-1b832599b33d4f885c9b7651dbfc60d9.png)

## 整体结构

Encoder:
1. input word embedding
   - 进入一个不带bias的FFN得到一个稠密的连续向量

2. position encoding
    - 通过sin/cos来固定表征
        - 每个位置确定性的（长度为5和10的两个位置，它们1~5位置表示一样）
        - 对于不同（长度）的句子，相同位置的距离一致
        - 可以推广到更长的测试句子
    - pe(pos+k)可以写成pe(pos)的线性组合，从来处理没有见过的句子
    - 通过残差连接使得位置信息流入深层

3. multi-head self-attention
    - 不同的头学不同的内容，使得建模能力更强，表征空间更丰富
    - 由多组QKV组成，每组单独计算一个attention向量
    - 把每组的attention向量拼起来，并进入一个不带bias的FFN得到最终向量

4. feed-forward network
    - 只考虑每个单独位置进行建模，即不考虑词向量之间的关系(不考虑周围的字符)
    - 不同位置参数共享
    - 类似于1*1pointwise convolution（权值共享的那个FFN就看出1x1的卷积核）

Decoder:
1. output word embedding，
2. masked multi-head self-attention mask是下三角全为0，上三角全为负无穷的矩阵
3. multi-head cross-attention encoder表示作为K和V，output输入作为Q
4. FFN
5. softmax

Transformer[pytorch官网api](https://pytorch.org/docs/stable/nn.html#transformer-layers)

## 缩放点积注意力
两个最常用的注意力函数是加法注意力和点积(乘法)注意力。除了缩放因子 $ \frac{1}{\sqrt{d_{k}}}$ ，点积注意力与作者的算法是一致的。加法注意力使用一个具有单个隐藏层的前馈网络计算兼容性函数。虽然两者在理论复杂性上相似，但是点积注意力在实际中会更快更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。

尽管当$d_{k}$ 取值较小时，两种注意力机制表现相似，但是当$d_{k}$取值较大且不进行缩放时，加法注意力要优于点积注意力。

然而，由于底层矩阵运算的优化，我们计算点积注意力计算量更小，我们想要使用这种计算量小的方式。

作者猜测，当 $d_{k}$较大值时，点积会大幅度增大，并且将 softmax 函数推入其梯度极小的区域(即其值趋于1或0)。为了缓解该影响，作者通过  $ \frac{1}{\sqrt{d_{k}}}$缩放点积。


## 为什么使用三角函数做位置编码
**为了更好的泛化能力**
如图所示，TRM中的pos对于下图的t，TRM位置编码的分母部分(频率)对应下图的w
可以看出下图的公式是一个线性组合的形式，考虑这样一个问题：
我们训练的时候长度是2000，但是测试遇到了2001长度的位置，这个位置编码是不是无法表示了呢？
如果使用三角函数的话，我们就可以将2001写成之前训练过的某些位置线性组合的形式，把新的位置编码算出来

![三角函数wei'zhi'bian'ma![Image](https://pic4.zhimg.com/80/v2-a4de2f99b53f0eb0dfe03a位置编码](https://pic4.zhimg.com/80/v2-a4de2f99b53f0eb0dfe03a6b42f47c29.png)

## SA和FFN
对标一下ConvMixer(其实是ConvMixer对标TRM)
1. SA自注意力是位置上的混合，对标depthwise conv
2. FFN是特征维度的混合，对标pointwise conv

![明确encoder decoder信息流动](https://pic4.zhimg.com/80/v2-576e37e4cd6ed2d8923b3e274417e5e2.png)