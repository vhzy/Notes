#! https://zhuanlan.zhihu.com/p/553408190
- [8.如何在PyTorch中训练模型](#8如何在pytorch中训练模型)
  - [训练网络](#训练网络)
  - [embedding初探](#embedding初探)
# 8.如何在PyTorch中训练模型
本节对应[官网教程](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)

## 训练网络
1.读入数据，定义网络
```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda

training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)

train_dataloader = DataLoader(training_data, batch_size=64)
test_dataloader = DataLoader(test_data, batch_size=64)

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__() #父类初始化
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork()
```

2.超参数
```python
learning_rate = 1e-3
batch_size = 64
epochs = 5
```

3.loss函数
```python
'''
回归：MSE等
分类：负对数似然nn.NLLLOSS,交叉熵nn.CrossEntropyLoss等
'''
# Initialize the loss function
loss_fn = nn.CrossEntropyLoss()
```

4.优化器
```python
#随机梯度下降,传入一个参数函数和学习率
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
```
在每一轮训练过程中，我们要做以下三步：
1. 调用`optimizer.zero_grad()`，对于模型的梯度清零，防止计算两倍
2. 调用`loss.backwards()`反向传播计算梯度,这里Loss是一个标量
3. 调用`optimizer.step()`函数，对模型的所有参数进行更新


5.完整实现
```python
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)#dataloader样本个数
    for batch, (X, y) in enumerate(dataloader):#每次传入一个小批次
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:#打印loss日志，current是当前算了多少样本
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)#注意这两句话的区别，这是计算总共样本个数
    num_batches = len(dataloader)#这是计算batch个数
    test_loss, correct = 0, 0#correct正确率

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()#argmax计算类别

    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```

6.训练多个epoch
```python
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

epochs = 10
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
print("Done!")
```

总结一下训练过程主要可以分成三部分：
1. 构建dataset和dataloader
2. 构建网络，分成多个class来写，这样某个部分固定住就把这部分`require_grad=false`即可
3. 训练部分

深度学习代码使用了大量python的面向对象编程，可以好好学学。

## embedding初探
插入说一下`embedding`,[官网教程](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)

```python
CLASS torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)

num_embeddings:token数目，词库大小等等
embedding_dim：embedding维度
```
实例化一个`embedding`就是得到了一个`embedding_table`,它与输入的`onehot`向量相乘，就得到
了最后的`embedding_vector`表示
可以这样理解：这是一个没有bias的全连接，权重就是`embedding_table`，我们训练embedding也就是训练这一部分
```python
n, d, m = 3, 5, 7
embedding = nn.Embedding(n, d, max_norm=True)
W = torch.randn((m, d), requires_grad=True)
idx = torch.tensor([1, 2])
a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable
b = embedding(idx) @ W.t()  # modifies weight in-place
out = (a.unsqueeze(0) + b.unsqueeze(1))
loss = out.sigmoid().prod()
loss.backward()
```
