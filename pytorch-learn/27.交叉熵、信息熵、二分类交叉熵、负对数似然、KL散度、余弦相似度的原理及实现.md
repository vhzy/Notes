#! https://zhuanlan.zhihu.com/p/556155109
- [27.交叉熵、信息熵、二分类交叉熵、负对数似然、KL散度、余弦相似度的原理及实现](#27交叉熵信息熵二分类交叉熵负对数似然kl散度余弦相似度的原理及实现)
  - [交叉熵](#交叉熵)
  - [负对数似然损失](#负对数似然损失)
  - [KL散度损失](#kl散度损失)
  - [二分类交叉熵`BCELoss`](#二分类交叉熵bceloss)
  - [余弦相似度`COSINESIMILARITY`和`COSINEEMBEDDINGLOSS`](#余弦相似度cosinesimilarity和cosineembeddingloss)

# 27.交叉熵、信息熵、二分类交叉熵、负对数似然、KL散度、余弦相似度的原理及实现

## 交叉熵

[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
[中文博客](https://www.cnblogs.com/marsggbo/p/10401215.html)

在信息论中，基于相同事件测度的两个概率分布${\displaystyle p}$和${\displaystyle q}$的交叉熵是指，当基于一个“非自然”（相对于“真实”分布${\displaystyle p}$而言）的概率分布${\displaystyle q}$进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。

给定两个概率分布 $p$ 和 $q ， p$ 相对于 $q$ 的交叉樀定义为:
$$
H(p, q)=\mathrm{E}_{p}[-\log q]=H(p)+D_{\mathrm{KL}}(p \| q),
$$
其中 $H(p)$ 是 $p$ 的熵， $D_{\mathrm{KL}}(p \| q)$ 是从 $p$ 与的KL散度(也被称为 $p$ 相对于 $q$ 的相对熵。
对于离散分布 $p$ 和 $q$ ，这意味着:
$$
H(p, q)=-\sum p(x) \log q(x)
$$

Pytorch中面对target是one-hot和概率分布的情况，有两种计算方式：
1. one-hot只计算y为1的位置
$\ell(x, y)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top}, \quad l_{n}=-w_{y_{n}} \log \frac{\exp \left(x_{n, y_{n}}\right)}{\sum_{c=1}^{C} \exp \left(x_{n, c}\right)} \cdot 1\left\{y_{n} \neq\right.$ ignore_index $\}$

2. 概率分布
$\ell(x, y)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top}, \quad l_{n}=-\sum_{c=1}^{C} w_{c} \log \frac{\exp \left(x_{n, c}\right)}{\sum_{i=1}^{C} \exp \left(x_{n, i}\right)} y_{n, c}$

```python
torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100,
 reduce=None, reduction='mean', label_smoothing=0.0)

'''
weight:1d tensor，维度和类别一样，改变每个类的权重，在unbalanced data里面常用

ignore_index:和pad里面一样，如果目标是-100,那么就不会计算它，最终不计入loss中

reduction:对损失处理的方式，有mean sum none三种

label_smoothing:把目标概率值降低，降低的值随机移到别的类上

input的shape:
[C]   [N,C]   [N,C,d1,d2,...,dk]，这里d可以理解为时空维度等其他高维信息，或者例如计算 2D 图
像的每像素交叉熵损失。


对应target的shape
（）， （N），(N,d1,...dk)
'''
#官方演示
# Example of target with class indices
loss = nn.CrossEntropyLoss()
input = torch.randn(3, 5, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)
output = loss(input, target)
output.backward()
# Example of target with class probabilities
input = torch.randn(3, 5, requires_grad=True)
target = torch.randn(3, 5).softmax(dim=1)
output = loss(input, target)
output.backward()

```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

#logits shape:[bs, nc] 分类层之前的shape
batchsize = 2
num_class = 4

logits = torch.randn(batchsize, num_class)
target = torch.randint(num_class, size = (batchsize,))#delta目标分别

target_logits = torch.randn(batchsize, num_class) #非delta目标分布


#计算方法一调用CE LOSS
ce_loss_fn = torch.nn.CrossEntropyLoss()
ce_loss = ce_loss_fn(logits, target)
print(f"cross entropy loss1: {ce_loss}")

#计算方法二 target传入的是一个概率分布
ce_loss = ce_loss_fn(logits, torch.softmax(target_logits,dim = -1))
print(f"cross entropy loss2: {ce_loss}")
```

## 负对数似然损失

[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)

负对数似然是一种用于解决分类问题的  损失函数 ，它是似然函数得一种自然对数形式，可用于测量两种概率分布之间的相似性，其取负号是为了让最大似然值和最小损失相对应，是最大似然估计及相关领域的常见函数形式。

似然函数：
$$
L=\theta\left(y_{1} x_{1}\right) \cdot \theta\left(y_{2} x_{2}\right) \cdot \ldots \cdot \theta\left(y_{N} x_{N}\right)=\prod_{n=1}^{N} \theta\left(y_{n} x_{n}\right)
$$

对上述的似然函数取负数，再取log，可以得到最终的**负对数似然损失函数**：
Nagative Maximum Likehood
$$
M L E=\frac{1}{N} \sum_{n=1}^{N}-\ln \theta\left(y_{n} w^{T} x_{n}\right)=\frac{1}{N} \sum_{n=1}^{N} \ln \left(1+e^{-y_{n} w^{T} x_{n}}\right)
$$

在Pytorch中的计算公式如下:
$\ell(x, y)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top}, \quad l_{n}=-w_{y_{n}} x_{n, y_{n}}, \quad w_{c}=$ weight $[c] \cdot 1\{c \neq$ ignore_index $\}$
```python
torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean')

'''
注意NLL的input,交叉熵中我们输入一个正常的未归一化的logits即可
然而NLL的input需要包含了每个类别的对数化概率 即x = log p
'''
#官方案例
m = nn.LogSoftmax(dim=1)
loss = nn.NLLLoss()
# input is of size N x C = 3 x 5
input = torch.randn(3, 5, requires_grad=True)
# each element in target has to have 0 <= value < C
target = torch.tensor([1, 0, 4])
output = loss(m(input), target)
output.backward()
# 2D loss example (used, for example, with image inputs)
N, C = 5, 4
loss = nn.NLLLoss()
# input is of size N x C x height x width
data = torch.randn(N, 16, 10, 10)
conv = nn.Conv2d(16, C, (3, 3))
m = nn.LogSoftmax(dim=1)
# each element in target has to have 0 <= value < C
target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)
output = loss(m(conv(data)), target)
output.backward()
```

```python
'''
和上面ce loss代码一起运行，去掉下面的极小量，我们发现，和ce_loss计算结果一样
所以，pytorch中能用ce_loss的地方一般都能用nll_loss
用什么取决于上一步神经网络输出是什么，如果是概率值甚至是对数概率，就用nll_loss
'''
#2调用Negative Log Likelihood loss(NLL Loss)
nll_fn = torch.nn.NLLLoss()
#进行归一化后log，为了让Log稳定，我们需要加入一个极小量,target只能传入整形的索引
nll_loss = nll_fn(torch.log(torch.softmax(logits, dim = -1)+1e-7), target_indices) 
print(f"negative log_likelihood loss: {nll_loss}")
```

我们看一下[wiki](https://en.wikipedia.org/wiki/Cross_entropy)上面交叉熵和负对数似然的关系，他们是殊途同归的
Relation to maximum likelihood：
可以看看这篇文章[交叉熵和极大似然估计的再理解](https://zhuanlan.zhihu.com/p/165139520)

这篇文章其实也说到了交叉熵和KL散度的关系
交叉熵 = 信息熵 + KL散度
接下来我们看一下KL散度

## KL散度损失

Kullback–Leibler divergence
VAE等和高斯分布打交道的生成模型大量用到了KL散度，这里来看一下
[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)

离散的概率分布P和Q，定义在同一个概率空间，则Q对P的相对熵
$D_{\mathrm{KL}}(P \| Q)=\sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right)$

pytorch中的计算

$L\left(y_{\text {pred }}, y_{\text {true }}\right)=y_{\text {true }} \cdot \log \frac{y_{\text {true }}}{y_{\text {pred }}}=y_{\text {true }} \cdot\left(\log y_{\text {true }}-\log y_{\text {pred }}\right)$

```python
torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)

'''
input可以是任意形状，target与input形状一致即可
为了防止下溢，input希望是在log的空间传入进来的,target可以是log空间也可以是线性空间

'''

#官方案例
kl_loss = nn.KLDivLoss(reduction="batchmean")
# input should be a distribution in the log space
input = F.log_softmax(torch.randn(3, 5, requires_grad=True))
# Sample a batch of distributions. Usually this would come from the dataset
target = F.softmax(torch.rand(3, 5))
output = kl_loss(input, target)

kl_loss = nn.KLDivLoss(reduction="batchmean", log_target=True)
log_target = F.log_softmax(torch.rand(3, 5))
output = kl_loss(input, log_target)
```

```python
#3. klloss,接着前面的负对数似然代码
kld_loss_fn = torch.nn.KLDivLoss()
kld_loss = kld_loss_fn\
    (torch.log(torch.softmax(logits, dim = -1)), torch.softmax(target_logits,dim = -1))

print(f'Kullback–Leibler divergence loss:{kld_loss}')
```

下面来验证$CE = IE + KLD$

真实分布（p）和预测分布(q)的交叉熵 = 真实分布的信息熵 + 真实分布和预测分布的KL散度

$H(p, q)=-\mathrm{E}_{p}[\log q]$

$H(p, q)=H(p)+D_{\mathrm{KL}}(p \| q)$

在做机器学习的时候，如果真实分布是一个One-hot的，也就是delta分布，它的信息熵其实是0，计算不是delta分布，信息熵也只是分散在各个类上的常数

所以，优化CE loss和优化KLD loss目标上一般没有区别

```python
#4.验证 CE = IE + KLD
#不做batch，对每个样本单独返回交叉熵
ce_loss_fn_sample = torch.nn.CrossEntropyLoss(reduction = "none")
ce_loss_sample = ce_loss_fn_sample(logits, torch.softmax(target_logits,dim = -1))
print(f"cross entropy loss sample: {ce_loss_sample}")

#计算KL散度
kld_loss_fn_sample = torch.nn.KLDivLoss(reduction="none")
kld_loss_sample = kld_loss_fn_sample\
    (torch.log(torch.softmax(logits, dim = -1)), torch.softmax(target_logits,dim = -1)).sum(-1)
print(f'Kullback–Leibler divergence loss sample:{kld_loss_sample}')

#信息熵
target_information_entropy = torch.distributions.Categorical\
    (probs = torch.softmax(target_logits,dim = -1)).entropy()
print(f'information entropy loss sample:{target_information_entropy}')

print(torch.allclose(ce_loss_sample,kld_loss_sample + target_information_entropy))
```

## 二分类交叉熵`BCELoss`

二项交叉熵[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)

$\ell(x, y)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top}, \quad l_{n}=-w_{n}\left[y_{n} \cdot \log x_{n}+\left(1-y_{n}\right) \cdot \log \left(1-x_{n}\right)\right]$

```python
'''
input任意维度，target任意维度
二分类激活函数用signoid
多分类激活函数用softmax
bce的一般形式就是nll loss，用nll loss算出来是一样的
'''


torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')

m = nn.Sigmoid()
loss = nn.BCELoss()
input = torch.randn(3, requires_grad=True)
target = torch.empty(3).random_(2)
output = loss(m(input), target)
output.backward()
```

```python
# 5.调用binary cross entropy loss
batchsize = 2
bce_loss_fn = torch.nn.BCELoss()
logits = torch.randn(batchsize)
prob_1 = torch.sigmoid(logits)
target = torch.randint(2, size = (batchsize, )) #target只能是0，1
bce_loss = bce_loss_fn(prob_1, target.float())
print(prob_1.shape)
print(target)
print(f'bce_loss:{bce_loss}')
#bce的一般形式就是nll loss，用nll loss算出来是一样的，用Nll loss代替
prob_0 = 1 - prob_1.unsqueeze(-1)
prob = torch.cat([prob_0, prob_1.unsqueeze(-1)], dim=-1)
nll_loss_binary = nll_fn(torch.log(prob), target)
print(f'nll_loss_binary:{nll_loss_binary}')

```

## 余弦相似度`COSINESIMILARITY`和`COSINEEMBEDDINGLOSS`
[COSINESIMILARITY官方文档](https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html)

来评估两个量的余弦相似度
$$
\text { similarity }=\frac{x_{1} \cdot x_{2}}{\max \left(\left\|x_{1}\right\|_{2} \cdot\left\|x_{2}\right\|_{2}, \epsilon\right)}
$$

[COSINEEMBEDDINGLOSS官方文档](https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss)

基于余弦距离，评估两个输入量是相似的还是不相似的，在自监督学习、对比学习、
相似度检索、相似度匹配中常用。
y=1表示相似
y=-1表示不相似
target只能是1或者-1
$$
\operatorname{loss}(x, y)= \begin{cases}1-\cos \left(x_{1}, x_{2}\right), & \text { if } y=1 \\ \max \left(0, \cos \left(x_{1}, x_{2}\right)-\operatorname{margin}\right), & \text { if } y=-1\end{cases}
$$

```python
torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean'
```

```python

#6.余弦相似度cosine similarity loss
import torch
batchsize = 2
consine_loss_fn = torch.nn.CosineEmbeddingLoss()
v1 = torch.randn(batchsize, 512)
v2 = torch.randn(batchsize, 512)
target = torch.randint(2, size = (batchsize,)) * 2 -1 #(0,1) -> (0,2) -> (-1,1)
consine_loss = consine_loss_fn(v1,v2,target)
print(f'consine_loss:{consine_loss}')
```