#! https://zhuanlan.zhihu.com/p/555108740
# 16.PyTorch nn.Conv2d卷积网络使用教程

接下来几节课回归到卷积，围绕以下几个方面：

![](https://pic4.zhimg.com/80/v2-3965677567474d555e2d550638e218ac.png)
![卷积大纲](https://pic4.zhimg.com/80/v2-e7e14e8eea87d903637b8113691a2cd0.png)

$$
\operatorname{out}\left(N_{i}, C_{\text {out }_{j}}\right)=\operatorname{bias}\left(C_{\text {out }_{j}}\right)+\sum_{k=0}^{C_{\text {in }}-1} \text { weight }\left(C_{\text {out }_{j}}, k\right) \star \operatorname{input}\left(N_{i}, k\right)
$$

 ```python
 torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, 
 dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
 ```
官方示例
```python
# With square kernels and equal stride
m = nn.Conv2d(16, 33, 3, stride=2)
# non-square kernels and unequal stride and with padding
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
# non-square kernels and unequal stride and with padding and dilation
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
input = torch.randn(20, 16, 50, 100)
output = m(input)
```
卷积里面的计算
1. 仅考虑卷积核
输出大小等于输入大小 $n_{h} \times n_{w}$ 减去卷积核大小 $k_{h} \times k_{w}$, 即:
$$
\left(n_{h}-k_{h}+1\right) \times\left(n_{w}-k_{w}+1\right) .
$$
可以看出，卷积核大小是1，也就是point-wise的时候，输入输出维度不变


2. 考虑填充和步幅
- 考虑padding：
通常, 如果我们添加 $p_{h}$ 行填充（大约一半在顶部, 一半在底部）和 $p_{w}$ 列填充（左侧大约一半, 右侧一半）, 则输出形状将为
$$
\left(n_{h}-k_{h}+p_{h}+1\right) \times\left(n_{w}-k_{w}+p_{w}+1\right) 。
$$
这意味着输出的高度和宽度将分别增加 $p_{h}$ 和 $p_{w}$ 。
在许多情况下, 我们需要设置 $p_{h}=k_{h}-1$ 和 $p_{w}=k_{w}-1$, 使输入和输出具有相同的高度和宽度。这样可以在构建网络时更容易地预测 每个图层的输出形状。假设 $k_{h}$ 是奇数, 我们将在高度的两侧填充 $p_{h} / 2$ 行。如果 $k_{h}$ 是偶数, 则一种可能性是在输入顶部填充 $\left\lceil p_{h} / 2\right\rceil$ 行, 在 底部填充 $\left\lfloor p_{h} / 2\right\rfloor$ 行。同理, 我们填充宽度的两侧。
卷积神经网络中卷积核的高度和宽度通常为奇数, 例如 $1 、 3 、 5$ 或7。 选择奇数的好处是, 保持空间维度的同时, 我们可以在顶部和底部填 充相同数量的行, 在左侧和右侧填充相同数量的列。


- 考虑stride:
通常, 当垂直步幅为 $s_{h}$ 、水平步幅为 $s_{w}$ 时, 输出形状为
$$
\left\lfloor\left(n_{h}-k_{h}+p_{h}+s_{h}\right) / s_{h}\right\rfloor \times\left\lfloor\left(n_{w}-k_{w}+p_{w}+s_{w}\right) / s_{w}\right\rfloor .
$$

以上公式代入步幅为1就得到之前的公式

如果我们设置了 $p_{h}=k_{h}-1$ 和 $p_{w}=k_{w}-1$, 则输出形状将简化为 $\left\lfloor\left(n_{h}+s_{h}-1\right) / s_{h}\right\rfloor \times\left\lfloor\left(n_{w}+s_{w}-1\right) / s_{w}\right\rfloor$ 。更进一步, 如果输入 的高度和宽度可以被垂直和水平步幅整除, 则输出形状将为 $\left(n_{h} / s_{h}\right) \times\left(n_{w} / s_{w}\right)$ 。
我们将高度和宽度的步幅设置为 2 , 从而将输入的高度和宽度减半。

公式推导：
1.  扫描区域大小： W+2P(下面针对padding在上下或左右都相等的情况，padding的P是之前说的一般)
2.  图中第一行卷积核扫描的过程中，暂时不考虑第1次卷积核扫描，现在考虑第2次到第一行扫描结束的扫描区域大小，我称为“**剩余扫描区域大小**”，**剩余扫描区域**大小：W+2P−F
3.  2步骤得到了**剩余扫描区域**大小，用卷积核以步长 S 扫描**剩余扫描区域**，得到**剩余扫描区域**卷积后的大小：(W+2P−F)/S
4.  最后考虑第一行的第1次卷积核扫描，与步骤3的**剩余扫描区域**卷积后的大小相加可得输出的大小： N=(W+2P−F)/S+1

接下来考虑padding填充不相等的情况，思考方式不变，从分析实际扫描区域大小出发，假设输入图片大小是 W ，填充大小 P 在图像左边填充了 P1，在图像右边填充了 P2，那么扫描区域大小就变为：W+P1+P2，最后的输出大小：N=(W+P1+P2−F)/S+1
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

batch_size = 1
in_channels = 1
out_channels = 1
kernel_size = 3
bias = False
input_size = [batch_size, in_channels, 4, 4]

conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias = bias)
input_feature_map = torch.randn(input_size)
output_feature_map = conv_layer(input_feature_map)

print(input_feature_map)
print(conv_layer.weight)  #1*3*3*3 = out_channels*in_channels*height*width
print(output_feature_map)

#使用nn.functional中的卷积，作为函数调用
output_feature_map1 = F.conv2d(input_feature_map, conv_layer.weight)
print(output_feature_map1)
```