#! https://zhuanlan.zhihu.com/p/548364684
# 7.PyTorch autograd使用教程
推荐材料:
论文:*Automatic Differentiation in Machine Learning: a Survey*
CSC321 Lecture10:Automatic Differentiation

左侧是前向运算，右侧是L对它们的导数
![自动微分](https://pic4.zhimg.com/80/v2-8b4666aa68adddc7b4be40c067b7e905.png)

同时框架还分解为一系列元操作
![分解元操作](https://pic4.zhimg.com/80/v2-afc81c754049025b875b7f39fbcf9816.png)


## [自动微分](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)
```python
import torch

x = torch.ones(5)  # input tensor
y = torch.zeros(3)  # expected output
w = torch.randn(5, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)
z = torch.matmul(x, w)+b
loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)

```
![对应的计算图](https://pic4.zhimg.com/80/v2-d237dcc62e019baf9e347ee5ac5ef300.png)

打印`gradient function`
```python
print(f"Gradient function for z = {z.grad_fn}")
print(f"Gradient function for loss = {loss.grad_fn}")

out:
Gradient function for z = <AddBackward0 object at 0x7f5d1051db50>
Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f5d1051f6d0>
```

如何计算每个参数的梯度？
```python
'''
#backward是tensor类方法
backward括号里面没有任何参数，Loss必须是一个标量才能使用，
如果loss是张量的话，必须传入一个shape一致的向量（里面值一般是1）
'''
loss.backward() #可以先用sum转成一个标量
print(w.grad)
print(b.grad)
```

因为pytorch是动态图，`backward`在一个计算图里面只能调用一次，想多次调用需要加入`retain_graph = True`

`with torch.no_grad():`和`.detach()`都可以实现`requires_grad = false`的效果
在fintune,inference的时候很实用

如果是张量对张量求梯度呢？
从数学上讲，autograd类只是一个雅可比向量积计算引擎。雅可比矩阵是一个非常简单的单词，它表示两个向量所有可能的偏导数。它是一个向量相对于另一个向量的梯度。

注意：在这个过程中，PyTorch从不显式地构造整个雅可比矩阵。直接计算JVP (Jacobian vector product)
$v^T · J$通常更简单、更有效。

这个过程就是说我们提供一个vector，我们将输入张量进行运算后得到LOSS标量，（一般来说这个操作就是sum,对应传入的这个vector是sum()操作）这个LOSS对于张量的偏导数就是这个vector.

对应实际场景中，我们也是需要求和的，因为我们想要计算的是一个batch，即每个样本的偏导数之和.

```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```

如果一个向量$X = [x1, x2，…xn]$通过$f(X) = [f1, f2，…fn]$来计算其他向量，则雅可比矩阵(J)包含以下所有偏导组合：
![雅可比矩阵](https://pic4.zhimg.com/80/v2-613023f87440eb30311946e6ae8649be.png)

```python
inp = torch.eye(5, requires_grad=True)
out = (inp+1).pow(2)
out.backward(torch.ones_like(inp), retain_graph=True)
print(f"First call\n{inp.grad}")
out.backward(torch.ones_like(inp), retain_graph=True)
print(f"\nSecond call\n{inp.grad}")
inp.grad.zero_()
out.backward(torch.ones_like(inp), retain_graph=True)
print(f"\nCall after zeroing gradients\n{inp.grad}")

out:
First call
tensor([[4., 2., 2., 2., 2.],
        [2., 4., 2., 2., 2.],
        [2., 2., 4., 2., 2.],
        [2., 2., 2., 4., 2.],
        [2., 2., 2., 2., 4.]])

Second call
tensor([[8., 4., 4., 4., 4.],
        [4., 8., 4., 4., 4.],
        [4., 4., 8., 4., 4.],
        [4., 4., 4., 8., 4.],
        [4., 4., 4., 4., 8.]])

Call after zeroing gradients
tensor([[4., 2., 2., 2., 2.],
        [2., 4., 2., 2., 2.],
        [2., 2., 4., 2., 2.],
        [2., 2., 2., 4., 2.],
        [2., 2., 2., 2., 4.]])
```

想要求jacobian矩阵的话可以通过`torch.autograd.functional.jacobian`来求
```python
def exp_reducer(x):
  return x.exp().sum(dim=1)
inputs = torch.rand(2, 2)
jacobian(exp_reducer, inputs)

out:
tensor([[[1.4917, 2.4352],
         [0.0000, 0.0000]],
        [[0.0000, 0.0000],
         [2.4369, 2.3799]]])
```
可以验证`oneslike * jacobian`结果和调用`.grad结果`一样