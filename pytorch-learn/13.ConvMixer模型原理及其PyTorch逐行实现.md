# 13.ConvMixer模型原理及其PyTorch逐行实现

ConvMixer来自论文：[OpenReview链接](https://openreview.net/forum?id=TVHS5Y4dNvM)()

*~~Convolutions Attention MLPs~~ Patches Are All You Need?*(骚名字哈哈哈)

ViT的效果到底是transformer架构带来的还是patch带来的？

这篇文章的结论是：patch带来比较好的效果，卷积+patch也可以取得好效果

这里ConvMixer只用卷积层，使用patch的形式
在计算过程中,patch特征大小不变，没有对特征做下采样,一般的卷积层特征都是越来越小，这里就很像TRM的各向一致性（第一层到最后一层，序列长度不变）

同时ConvMixer还把通道混合(pointwise-convolution)和空间混合(depthwise-convolution)分离开了
也就是说同时用这两个操作，既节省了计算量(少了一半)，也是相当于做了两种模型的混合

![ConvMixer](https://pic4.zhimg.com/80/v2-8e9fa298f2cf8bb02c2c1ab7db5cc666.png)
上图中，Patchembedding不同于VIT的操作，这里也是通过卷积实现的，卷积核p*p,步长也是p

首先在depthwise-convolution将通道分成h组进行空间混合，然后再pointwise-convolution进行通道混合，整个ConvMixer Layer直观上就对应一个TRM encoder

```python
def ConvMixer(h, depth, kernel_size=9, patch_size=7, n_classes=1000):
    Seq, ActBn = nn.Sequential, lambda x: Seq(x, nn.GELU(), nn.BatchNorm2d(h))
    Residual = type('Residual', (Seq,), {'forward': lambda self, x: self[0](x) + x})
    return Seq(ActBn(nn.Conv2d(3, h, patch_size, stride=patch_size)),
        *[Seq(Residual(ActBn(nn.Conv2d(h, h, kernel_size, groups=h, padding="same"))),
        ActBn(nn.Conv2d(h, h, 1))) for i in range(depth)],
        nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(h, n_classes))
```

看一下使用这两种卷积参数少了多少
![参数对比](https://pic4.zhimg.com/80/v2-67f9906a548435ee4fe8516abdfe97a3.png)