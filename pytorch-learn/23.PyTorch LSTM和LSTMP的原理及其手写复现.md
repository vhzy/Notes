#! https://zhuanlan.zhihu.com/p/555767896
# 23.PyTorch LSTM和LSTMP的原理及其手写复现
主要讲解LSTM模型和LSTM with projection的原理、逐行实现这两个模型

补充一下上一节，除了RNN api还有RNNCELL api，就是RNN的一个单步迭代
```python
rnn = nn.RNNCell(10, 20)
input = torch.randn(6, 3, 10)
hx = torch.randn(3, 20)
output = []
for i in range(6):
```

LSTM可以看这篇[博客](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
也可以看沐神的[动手学](https://zh-v2.d2l.ai/chapter_recurrent-modern/lstm.html)

![LSTM](https://i.imgur.com/raj5979.png)

LSTM的公式
$$
\begin{aligned}
输入门:i_{t} &=\sigma\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{t-1}+b_{h i}\right) \\
遗忘门:f_{t} &=\sigma\left(W_{i f} x_{t}+b_{i f}+W_{h f} h_{t-1}+b_{h f}\right) \\
候选记忆元:g_{t} &=\tanh \left(W_{i g} x_{t}+b_{i g}+W_{h g} h_{t-1}+b_{h g}\right) \\
输出门:o_{t} &=\sigma\left(W_{i o} x_{t}+b_{i o}+W_{h o} h_{t-1}+b_{h o}\right) \\
记忆元:c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot g_{t} \\
隐状态:h_{t} &=o_{t} \odot \tanh \left(c_{t}\right)
\end{aligned}
$$

**个人总结:**
1. 使用和RNN每个单元一样的方法计算出输入门，遗忘门，输出门，候选记忆元，区别在于前三者的激活函数
是`sigmod`，候选记忆元的激活函数是`tanh`
2. 通过逐元素乘（遗忘门 和 上一轮记忆元）加 （输入门 和 候选记忆元）得到本轮 记忆元
3. 最后记忆元通过激活函数`tanh`后和输出门求哈达玛积，得到本轮的隐状态

可以看出参数比RNN多了很多，RNN的参数是共享的，相当于只有一组
实际运算的时候，我们可以把四个W组成一个矩阵一起和x计算，这样并行计算性能会有优化
可以看出h0和c0是初始值，这个值是可以通过meta learning学来的

注意参数:proj_size，这个就是让最后ht的维度变小，也让中间的计算量变小

```python
rnn = nn.LSTM(10, 20, 2)
input = torch.randn(5, 3, 10)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)
output, (hn, cn) = rnn(input, (h0, c0))
```

**官方api实现lstm**
```python
#定义常量
bs, T, i_size, h_size = 2, 3, 4, 5
proj_size = 3#比hidden_size小
input = torch.randn(bs, T, i_size) #输入序列
c0 = torch.randn(bs, h_size) #初始值不参与训练，api里面维度(D∗num_layers,N,Hout)
h0 = torch.randn(bs, h_size)

#调用官方api
lstm_layer = nn.LSTM(i_size, h_size, batch_first = True)
output, (h_final, c_final) = lstm_layer(input, (h0.unsqueeze(0), c0.unsqueeze(0)))
# print(output)
# print(h_final)
'''
weight_ih_l0 torch.Size([20, 4]) 4个W拼起来 4个5*4
weight_hh_l0 torch.Size([20, 5]) 4个W拼起来 4个5*5
bias_ih_l0 torch.Size([20])
bias_hh_l0 torch.Size([20])
'''
for k, v in lstm_layer.named_parameters():
    print(k, v.shape)

```

**手写LSTM**
```python
#自己写一个LSTM模型
def lstm_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh):
    h0, c0 = initial_states #初始状态
    bs, T, i_size = input.shape
    h_size = w_ih.shape[0] // 4

    prev_h = h0
    prev_c = c0
    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1) #bs,4*hidden_size,i_size
    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)#bs,4*size, h_size

    output_size = h_size
    output = torch.zeros(bs, T, output_size) #输出序列

    for t in range(T):
        x = input[:,t, :] #当前时刻的输入向量,[bs, i_size],矩阵相乘后面加一个维度
        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))  #bs, 4*h_size,1
        w_times_x = w_times_x.squeeze(-1) #bs, 4*h_size

        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  #bs, 4*h_size,1
        w_times_h_prev = w_times_h_prev.squeeze(-1) #bs, 4*h_size

        #分别计算输入们(i)\遗忘门(f)\cell门(g)\输出门(o)
        i_t = torch.sigmoid(w_times_x[:, :h_size] + \
            w_times_h_prev[:, :h_size] +b_ih[:h_size] + b_hh[:h_size])
        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + \
            w_times_h_prev[:, h_size:2*h_size] +b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])
        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:,2*h_size:3*h_size]\
             +b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])
        o_t = torch.sigmoid(w_times_x[:, 3*h_size:] + \
            w_times_h_prev[:, 3*h_size:] +b_ih[3*h_size:] + b_hh[3*h_size:])
        
        #然后算记忆元ct,迭代实现
        prev_c = f_t * prev_c + i_t * g_t
        prev_h = o_t * torch.tanh(prev_c)
        output[:, t, :] = prev_h
    return output, (prev_h, prev_c)

output_custom, (h_final_custom, c_final_custom) = lstm_forward(input, (h0,c0), \
    lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0)

print(output)
print(output_custom)

```

**官方api实现lstmp**
```python
#加入proj_size
#定义常量
bs, T, i_size, h_size = 2, 3, 4, 5
proj_size = 3#比hidden_size小
input = torch.randn(bs, T, i_size) #输入序列
c0 = torch.randn(bs, h_size) #初始值不参与训练，api里面维度(D∗num_layers,N,Hout)
h0 = torch.randn(bs, proj_size) #修改输出

#调用官方api
lstm_layer = nn.LSTM(i_size, h_size, batch_first = True, proj_size=proj_size)
output, (h_final, c_final) = lstm_layer(input, (h0.unsqueeze(0), c0.unsqueeze(0)))
# print(output)
# print(h_final)
print(output.shape, h_final.shape, c_final.shape)#记忆元的输出大小不变，最终hidden_states变小
'''
weight_ih_l0 torch.Size([20, 4])
weight_hh_l0 torch.Size([20, 3]) 这里维度减少了
bias_ih_l0 torch.Size([20])
bias_hh_l0 torch.Size([20])
weight_hr_l0 torch.Size([3, 5]) 实现对hidden_states的压缩
'''
for k, v in lstm_layer.named_parameters():
    print(k, v.shape)
```

**手写实现lstmp**
```python
#自己写一个LSTM模型
def lstm_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh, w_hr = None):
    h0, c0 = initial_states #初始状态
    bs, T, i_size = input.shape
    h_size = w_ih.shape[0] // 4

    prev_h = h0
    prev_c = c0
    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1) #bs,4*hidden_size,i_size
    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)#bs,4*size, h_size

    if w_hr is not None:
        p_size = w_hr.shape[0]
        output_size = p_size
        batch_w_hr = w_hr.unsqueeze(0).tile(bs, 1, 1) #[bs, p_size, h_size]
    else:
        output_size = h_size

    output = torch.zeros(bs, T, output_size) #输出序列

    for t in range(T):
        x = input[:,t, :] #当前时刻的输入向量,[bs, i_size],矩阵相乘后面加一个维度
        
        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))  #bs, 4*h_size,1,
        w_times_x = w_times_x.squeeze(-1) #bs, 4*h_size

#4*hidden_size , p_size   @   p_size * 1,有prev_h的地方计算量会减少
        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  #bs, 4*h_size,1，加入proj这里计算量小了
        w_times_h_prev = w_times_h_prev.squeeze(-1) #bs, 4*h_size

        #分别计算输入们(i)\遗忘门(f)\cell门(g)\输出门(o)
        i_t = torch.sigmoid(w_times_x[:, :h_size] + \
            w_times_h_prev[:, :h_size] +b_ih[:h_size] + b_hh[:h_size])
        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + \
            w_times_h_prev[:, h_size:2*h_size] +b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])
        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:,2*h_size:3*h_size]\
             +b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])
        o_t = torch.sigmoid(w_times_x[:, 3*h_size:] + \
            w_times_h_prev[:, 3*h_size:] +b_ih[3*h_size:] + b_hh[3*h_size:])
        
        #然后算记忆元ct,迭代实现
        prev_c = f_t * prev_c + i_t * g_t
        prev_h = o_t * torch.tanh(prev_c) #[bs, h_size]
        if w_hr is not None:  #做proj
            prev_h = torch.bmm(batch_w_hr, prev_h.unsqueeze(-1)) #[bs, p_size, 1]
            prev_h = prev_h.squeeze(-1)  #[bs, p_size]

        output[:, t, :] = prev_h
    return output, (prev_h, prev_c)

output_custom, (h_final_custom, c_final_custom) = lstm_forward(input, (h0,c0), \
    lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0\
        ,lstm_layer.weight_hr_l0)

print(output)
print(output_custom)

```