#! https://zhuanlan.zhihu.com/p/556283575
- [28. MAE原理及代码](#28-mae原理及代码)
  - [MAE原理](#mae原理)
  - [MAE代码](#mae代码)
    - [代码流程](#代码流程)
    - [工程代码](#工程代码)
      - [models_mae.py](#models_maepy)
      - [main_pretrain.py](#main_pretrainpy)
      - [main_finetune.py](#main_finetunepy)

# 28. MAE原理及代码

## MAE原理
原理部分可以看这篇[博客](https://www.cvmart.net/community/detail/5864)

**摘要：**
1. 文章写作意图：提出视觉自监督学习的模型
2. 对模型的描述：随机mask patch,任务是重构patch
3. 模型效果：ImageNet-1K 87.8%

**Introduction：**
image2patch 已经在timm库里面实现了，可以直接调用。
![MAE架构](https://pic4.zhimg.com/80/v2-621e5950bdeda0bf799f96bd2afc51ac.png)

**Approach：**
MAE编码器没有用到mask的信息，在解码器中用到了。这与BERT不同

encoder就是一个ViT,也就是patch_embedding + transformer的encoder

decoder接受所有的tokens,包含编码器的输出和mask,需要unshuffle重新拼起来,解码器也要位置编码
解码器只在预训练的时候使用，当作一个回归任务来做。
解码器的最后就是一个全连接层，映射到一个patch里面的像素个数
损失函数就是 MSE mean squared error,只预测被mask的部分
这个过程我们可以使用mask实现，也就是说我们不需要计算未被mask部分的loss

预测原始像素还是预测归一化后的像素？
答案是：对每个patch做均值标准差归一化后的效果更好。让模型预测这个新的目标

**整体流程总结：**

1. 将图像划分成 patches：(B,C,H,W)->(B,N,PxPxC)；

2. 对各个 patch 进行 embedding(实质是通过全连接层)，生成 token，并加入位置信息(position embeddings)：(B,N,PxPxC)->(B,N,dim)；

3. 根据预设的掩码比例(paper 中提倡的是 75%)，使用服从均匀分布的随机采样策略采样一部分 token 送给 Encoder，另一部分“扔掉”(mask 掉)；

4. 将 Encoder 编码后的 token 与 加入位置信息后的 mask token 按照原先在 patch 形态时对应的次序拼在一起，然后喂给 Decoder 玩(如果 Encoder 编码后的 token 的维度与 Decoder 要求的输入维度不一致，则需要先经过 linear projection 将维度映射到符合 Decoder 的要求)；

5. Decoder 解码后取出 mask tokens 对应的部分送入到全连接层，对 masked patches 的像素值进行预测，最后将预测结果与 masked patches 进行比较，计算 MSE loss

**实验部分：**

fine-tuning:既训练编码器也训练分类头

linear probing:冻结整个预训练的编码器，只训练分类线性头

![ViT区别](https://pic4.zhimg.com/80/v2-1e21b00a7e1ba8bd6a93f74ee66156ad.png)


## MAE代码

[官方代码](https://github.com/facebookresearch/mae)

MAE的代码写的很规范，建议学习一下作为日后开发的模板。


### 代码流程
数据预处理：
1. image2tensor,可以用PIL读入，也可以用torchvision的ImageFolder，这种方式要求数据根目录
下面有一个train文件夹
输入网络中的不是0~255,而是通过最小最大归一化，或者减去均值除以标准差，得到0，1正态分布

2. 数据增强：翻转、裁剪、高斯模糊等

3. 转化成0到1之间的浮点数

4. 归一化，这里是按照通道归一化，有的任务按照特征维度归一化

![data process1](https://pic4.zhimg.com/80/v2-85536bedd37193406f51ea802af5106b.png)

![data process2](https://pic4.zhimg.com/80/v2-b1286e8f70c7b0dd24f48dadf5094cbe.png)

编码器：
1. img2patch embedding
2. 位置编码
3. 通过shuffle 实现 random masking
4. 加上用于分类的cls token
5. transformer的重复

![model encoder](https://pic4.zhimg.com/80/v2-4b3673ca7873aef8e2de8c7919bd02b2.png)

解码器：
1. 可能编码器特征维度大于解码器特征维度，需要投影
2. 通过unshuffle把encoder output还原到图片的位置，其他的用共享的mask embedding代替
3. 加入Position embedding
4. transformer虽然输入数据多，但是可以用更少的层
5. 回归mlp层，每个patch的输出投影到像素空间
6. mse loss

![model decoder and forward functions](https://pic4.zhimg.com/80/v2-2716185f159fddc91eb5f5b236938660.png)

训练过程
1. dataset
2. data_loader
3. model
4. optimizer
5. load_model
6. train_one_epoch:每个周期内遍历dataloader，计算
7. save_model

![training1](https://pic4.zhimg.com/80/v2-57fe33c220c2573cf876bc75ba0cb88d.png)

![training2](https://pic4.zhimg.com/80/v2-1e162f1981283b447bb3b50ec9c04331.png)

finetuning:
1. MAE增广较少，这里做了强增强
2. 只用编码器构建新模型
3. 位置编码，使用双线性差值，比如训练用224x224大小，测试用448x448大小，再用16x16的
patch，则序列长度会变长，训练的位置编码可能不够用
4. 加载模型，strict=False.pytorch中，只要设置非严格模式，就算两个模型大小不一致，只要
有共同的层，都会加载进来。
5. 更新所有参数
6. adamW
7. 微调阶段使用标签平滑的交叉熵loss

![finetuning](https://pic4.zhimg.com/80/v2-9ec0ab899950dcd63988a729f5ad0ef4.png)

linear probing:
1. 微弱增广
2. 构建模型，注意这里归一化仅仅是归一化，不加参数，不进行仿射变换
3. 位置编码
4. 非严格加载
5. 冻结编码器参数
6. LARS优化器
7. 交叉熵

![linear probing](https://pic4.zhimg.com/80/v2-9ee773d5ca6868732921ce3a0d9d8f69.png)

评估：
1. 用no_grad，不计算梯度
2. model.eval,把training参数设置成False,对于bn和dropout起到正确的作用
3. 分类任务通过top_k指标
top_5取前五个概率，与答案对比，有一个正确就算正确，top_1更严格

![evaluation](https://pic4.zhimg.com/80/v2-1c364e99cbb2d4492897f89b7e54395b.png)


### 工程代码
engine:引擎文件
main:训练入口文件
model:模型文件
submit:facebook在特定集群提交任务的文件

先看模型部分：
models_mae.py:训练的模型
models_vit.py:微调阶段的encoder
但是二者参数基本是一样的

#### models_mae.py
使用了timm库 pytorch-image-models
timm的介绍可以看[博客](https://zhuanlan.zhihu.com/p/350837279)

具体的到源码里面看,这里只讲一点我注意到的：

默认用的是vit的large模型，depth = 24

timm里面的PatchEmbed, Block

```python
from timm.models.vision_transformer import PatchEmbed, Block
self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)
num_patches = self.patch_embed.num_patches
```

在patchify和unpatchify使用了爱因斯坦标定法
`x = torch.einsum('nchpwq->nhwpqc', x)`

#### main_pretrain.py
通过get_args_parser()获得命令行参数

初始化分布式训练，facebook代码的好处就是，无论是单卡，单机多卡，多机多卡都可以跑
非常通用，可以在自己的任务上改写

```python
misc.init_distributed_mode(args)
```

misc在util/misc.py里面
247行`torch.distributed.barrier()`等待所有卡的进程都完成

train_one_epoch 在engine_pretrain里面

注意最新版的timm要把models_mae.py中出现的qk_scale注释掉

单卡运行：
`python main_pretrain.py --data="/" --model = "mae_vit_base_patch16" --batch_size=32`

多卡运行：
`python -m torch.distributed.launch --nproc_per_node = 2 main_pretrain.py.....`

#### main_finetune.py

微调只用了encoder，没有使用decoder

build_dataset()在util/dataset.py中