# 18.手写并验证向量内积实现PyTorch二维卷积
1. 向量内积实现二维卷积
```python
# 18手写滑动相乘
import math
input = torch.randn(5, 5)
kernel = torch.randn(3, 3)
bias = torch.randn(1) #偏置，默认通道为1

#step2 内积实现
def matrix_multiplication_for_conv2d_flatten(input, kernel,bias = 0,  stride = 1, padding = 0):

    if padding > 0:
        input = F.pad(input, (padding, padding, padding, padding))

    input_h, input_w = input.shape
    kernel_h, kernel_w = kernel.shape


    output_h = (math.floor((input_h - kernel_h ) /stride ) + 1)
    output_w = (math.floor((input_w - kernel_w ) /stride ) + 1)
    output = torch.zeros(output_h, output_w) #初始化输出矩阵

    region_matrix = torch.zeros(output.numel(), kernel.numel())#存储所有拉平后的特征区域
    kernel_matrix = kernel.reshape(kernel.numel(), 1)
    for i in range(0, input_h - kernel_h + 1, stride):  #对高度维进行遍历
        for j in range(0, input_w - kernel_w + 1, stride): #对宽度维进行遍历
            region  = input[i : i + kernel_h , j : j + kernel_w]
            region_vector = torch.flatten(region)
            region_matrix[i * output_w+j] = region_vector

            output_matrix = region_matrix @ kernel_matrix
            output = output_matrix.reshape(output_h, output_w) + bias
            #output[int(i / stride), int(j/stride)] = torch.sum(region * kernel) + bias#逐元素相乘
    return output
#矩阵运算实现卷积结果
mat_mul_conv_output_flatten = matrix_multiplication_for_conv2d_flatten(input, kernel, padding = 1,bias = bias)
print(mat_mul_conv_output_flatten)

#调用API结果
pytorch_api_conv_output = F.conv2d(input.reshape(1,1, input.shape[0], input.shape[1]),\
    kernel.reshape(1,1,kernel.shape[0],kernel.shape[1]),\
        padding=1,bias = bias).squeeze(0).squeeze(0)
flag = torch.allclose(mat_mul_conv_output_flatten, pytorch_api_conv_output)
flag
```

2. 考虑batch_size和channel维度

```python
# 引入batch_size和channel维度
import math
input = torch.randn(2, 2, 5, 5)
kernel = torch.randn(3, 2, 3, 3)
bias = torch.randn(3) #偏置，默认通道为1

#step1 用原始的矩阵运算实现二维卷积,先不考虑batch_size和channel
def matrix_multiplication_for_conv2d_full(input, kernel,bias = 0,  stride = 1, padding = 0):

    if padding > 0:
        input = F.pad(input, (padding, padding, padding, padding,0,0,0,0))

    bs, in_channel, input_h, input_w = input.shape
    out_channel, in_channel, kernel_h, kernel_w = kernel.shape
    if bias is None:
        bias = torch.zeros(out_channel)

    output_h = (math.floor((input_h - kernel_h ) /stride ) + 1)
    output_w = (math.floor((input_w - kernel_w ) /stride ) + 1)
    output = torch.zeros(bs, out_channel, output_h, output_w) #初始化输出矩阵

    for ind in range(bs):    
        for oc in range(out_channel):
            for ic in range(in_channel):
                for i in range(0, input_h - kernel_h + 1, stride):  #对高度维进行遍历
                    for j in range(0, input_w - kernel_w + 1, stride): #对宽度维进行遍历
                        region  = input[ind, ic, i : i + kernel_h , j : j + kernel_w]
                        output[ind, oc, int(i / stride), int(j/stride)] += torch.sum(region * kernel[oc,ic]) #逐元素相乘

            output[ind, oc] +=bias[oc]
    return output
#矩阵运算实现卷积结果
mat_mul_conv_output = matrix_multiplication_for_conv2d_full(input, kernel, padding = 1,bias = bias,stride = 2)
print(mat_mul_conv_output)

#调用API结果
pytorcg_api_conv_output = F.conv2d(input,kernel,padding=1,bias = bias,stride = 2)
print(pytorcg_api_conv_output)
flag = torch.isclose(mat_mul_conv_output, pytorcg_api_conv_output)
flag
```