{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss1: 1.775658369064331\n",
      "cross entropy loss2: 1.5176959037780762\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#logits shape:[bs, nc] 分类层之前的shape\n",
    "batchsize = 2\n",
    "num_class = 4\n",
    "\n",
    "logits = torch.randn(batchsize, num_class) #input unnormalized score\n",
    "target_indices = torch.randint(num_class, size = (batchsize,))#delta目标分别\n",
    "\n",
    "target_logits = torch.randn(batchsize, num_class) #非delta目标分布\n",
    "\n",
    "\n",
    "#方法一调用CE LOSS\n",
    "ce_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "ce_loss = ce_loss_fn(logits, target_indices)\n",
    "print(f\"cross entropy loss1: {ce_loss}\")\n",
    "\n",
    "#方法二 target传入的是一个概率分布\n",
    "ce_loss = ce_loss_fn(logits, torch.softmax(target_logits,dim = -1))\n",
    "print(f\"cross entropy loss2: {ce_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log_likelihood loss: 1.7756574153900146\n"
     ]
    }
   ],
   "source": [
    "#2调用Negative Log Likelihood loss(NLL Loss)\n",
    "nll_fn = torch.nn.NLLLoss()\n",
    "#进行归一化后log，为了让Log稳定，我们需要加入一个极小量,target只能传入整形的索引\n",
    "nll_loss = nll_fn(torch.log(torch.softmax(logits, dim = -1)+1e-7), target_indices) \n",
    "print(f\"negative log_likelihood loss: {nll_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullback–Leibler divergence loss:0.09055455774068832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#3. klloss\n",
    "kld_loss_fn = torch.nn.KLDivLoss()\n",
    "kld_loss = kld_loss_fn\\\n",
    "    (torch.log(torch.softmax(logits, dim = -1)), torch.softmax(target_logits,dim = -1))\n",
    "\n",
    "print(f'Kullback–Leibler divergence loss:{kld_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss sample: tensor([1.2287, 1.8067])\n",
      "Kullback–Leibler divergence loss sample:tensor([0.2415, 0.4829])\n",
      "information entropy loss sample:tensor([0.9872, 1.3237])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#4.验证 CE = IE + KLD\n",
    "#不做batch，对每个样本单独返回交叉熵\n",
    "ce_loss_fn_sample = torch.nn.CrossEntropyLoss(reduction = \"none\")\n",
    "ce_loss_sample = ce_loss_fn_sample(logits, torch.softmax(target_logits,dim = -1))\n",
    "print(f\"cross entropy loss sample: {ce_loss_sample}\")\n",
    "\n",
    "#计算KL散度\n",
    "kld_loss_fn_sample = torch.nn.KLDivLoss(reduction=\"none\")\n",
    "kld_loss_sample = kld_loss_fn_sample\\\n",
    "    (torch.log(torch.softmax(logits, dim = -1)), torch.softmax(target_logits,dim = -1)).sum(-1)\n",
    "print(f'Kullback–Leibler divergence loss sample:{kld_loss_sample}')\n",
    "\n",
    "#信息熵\n",
    "target_information_entropy = torch.distributions.Categorical\\\n",
    "    (probs = torch.softmax(target_logits,dim = -1)).entropy()\n",
    "print(f'information entropy loss sample:{target_information_entropy}')\n",
    "\n",
    "print(torch.allclose(ce_loss_sample,kld_loss_sample + target_information_entropy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([1, 0])\n",
      "bce_loss:0.547231137752533\n",
      "nll_loss_binary:0.547231137752533\n"
     ]
    }
   ],
   "source": [
    "# 5.调用binary cross entropy loss\n",
    "batchsize = 2\n",
    "bce_loss_fn = torch.nn.BCELoss()\n",
    "logits = torch.randn(batchsize)\n",
    "prob_1 = torch.sigmoid(logits)\n",
    "target = torch.randint(2, size = (batchsize, )) #target只能是0，1\n",
    "bce_loss = bce_loss_fn(prob_1, target.float())\n",
    "print(prob_1.shape)\n",
    "print(target)\n",
    "print(f'bce_loss:{bce_loss}')\n",
    "#bce的一般形式就是nll loss，用nll loss算出来是一样的，用Nll loss代替\n",
    "prob_0 = 1 - prob_1.unsqueeze(-1)\n",
    "prob = torch.cat([prob_0, prob_1.unsqueeze(-1)], dim=-1)\n",
    "nll_loss_binary = nll_fn(torch.log(prob), target)\n",
    "print(f'nll_loss_binary:{nll_loss_binary}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consine_loss:0.944987416267395\n"
     ]
    }
   ],
   "source": [
    "#6.余弦相似度cosine similarity loss\n",
    "import torch\n",
    "batchsize = 2\n",
    "consine_loss_fn = torch.nn.CosineEmbeddingLoss()\n",
    "v1 = torch.randn(batchsize, 512)\n",
    "v2 = torch.randn(batchsize, 512)\n",
    "target = torch.randint(2, size = (batchsize,)) * 2 -1 #(0,1) -> (0,2) -> (-1,1)\n",
    "consine_loss = consine_loss_fn(v1,v2,target)\n",
    "print(f'consine_loss:{consine_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
