- [11.Dropout原理以及其TF/Torch/Numpy源码实现](#11dropout原理以及其tftorchnumpy源码实现)
  - [Dropout文档](#dropout文档)
    - [`nn.Dropout`](#nndropout)
    - [`nn.functional.dropout`](#nnfunctionaldropout)
    - [其他框架的dropout实现](#其他框架的dropout实现)

# 11.Dropout原理以及其TF/Torch/Numpy源码实现

Pytorch中有两处dropout:
1. `nn.Dropout`,这里的Dropout以类的形式实现
2. `nn.functional.dropout`，以函数的形式实现

## Dropout文档
### [`nn.Dropout`](https://pytorch.org/docs/stable/nn.html#dropout-layers)
`self.training`来自与`Module`类，在Drouput中默认有这个成员变量

不需要显式地传入这个变量，只要在更高层面，设置成`eval`模式，Module的training就会默认设置false
```python
torch.nn.Dropout(p=0.5, inplace=False)
```

```python
#需要实例化
m = nn.Dropout(p=0.2)
input = torch.randn(20, 16)
output = m(input)
```

### [`nn.functional.dropout`](https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout)

训练和推理的`dropout`是不一样的，推理一般不需要dropout
如左图所示，在训练阶段以概率p丢弃神经元，会随机生成一个mask矩阵
为了让测试阶段的期望接近训练阶段，需要在权重前面乘一个p
这种方法会在测试时增加计算量，所以我们想要把p放在训练阶段，在训练时候除以一个（1-p）,这样测试的时候就不用乘以p了
![训练和推理的dropout](https://pic4.zhimg.com/80/v2-9e0b138764ce255a178ad65a0062c221.png)

*   **p** – probability of an element to be zeroed. Default: 0.5

*   **training** – apply dropout if is `True`. Default: `True`

*   **inplace** – If set to `True`, will do this operation in-place. Default: `False`
```python
torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)

```

### 其他框架的dropout实现

在Pytorch1.0之后，caffe2的代码集成到了里面，可以在其中看到C语言的底层实现：
caffe2/operators/dropout_op.cc

`float scale = 1. / (1. - ratio_)`就是在训练阶段除以一个scale

在Tensorflow中也有两个dropout
1. `tf.nn.dropout` 只能在训练的时候使用
2. `tf.keras.layers.Dropout`


我们也可以用numpy手写一个dropout函数
```python
import numpy as np


def train(rate, x, w1, b1, w2, b2):
    layer1 = np.maximum(0,p.dop(w1, x) + b1)
    mask1 = np.random.binomial(1, 1 - rate, layer1.shape)
    layer1 = layer1 * mask1
    layer2 = np.maximum(0,p.dop(w2, x) + b2)
    mask2 = np.random.binomial(1, 1 - rate, layer2.shape)
    layer2 = layer2 * mask2

    return layer2


def train2(rate, x, w1, b1, w2, b2):
    layer1 = np.maximum(0,p.dop(w1, x) + b1)
    mask1 = np.random.binomial(1, 1 - rate, layer1.shape)
    layer1 = layer1 * mask1
    layer1 = layer1 / (1 - rate)
    layer2 = np.maximum(0,p.dop(w2, x) + b2)
    mask2 = np.random.binomial(1, 1 - rate, layer2.shape)
    layer2 = layer2 * mask2
    layer2 = layer2 / (1 - rate)

    return layer2


#把缩放放在测试阶段
def test(rate, x, w1, b1, w2, b2):
    layer1 = np.maximum(0,p.dop(w1, x) + b1)
    layer1 = layer1 * (1 - rate)
    layer2 = np.maximum(0,p.dop(w2, x) + b2)
    layer2 = layer2 * (1 - rate)

    return layer2


def test( x, w1, b1, w2, b2):
    layer1 = np.maximum(0,p.dop(w1, x) + b1)

    layer2 = np.maximum(0,p.dop(w2, x) + b2)

    return layer2
```