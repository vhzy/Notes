# 24.PyTorch GRU的原理及其手写复现

$$
\begin{aligned}
r_{t} &=\sigma\left(W_{i r} x_{t}+b_{i r}+W_{h r} h_{(t-1)}+b_{h r}\right) \\
z_{t} &=\sigma\left(W_{i z} x_{t}+b_{i z}+W_{h z} h_{(t-1)}+b_{h z}\right) \\
n_{t} &=\tanh \left(W_{i n} x_{t}+b_{i n}+r_{t} *\left(W_{h n} h_{(t-1)}+b_{h n}\right)\right) \\
h_{t} &=\left(1-z_{t}\right) * n_{t}+z_{t} * h_{(t-1)}
\end{aligned}
$$
![GRU](https://pic4.zhimg.com/80/v2-d3faf867c6ed7d294c93f3f0bcbbe988.png)

GRU和LSTM对比：
1. LSTM中有四个门，GRU只有两个门
2. LSTM中有记忆元c，GRU没有
3. GRU的参数量约等于LSTM的0.75倍

GRU中的nt可以和LSTM的候选记忆元类比

**对比GRU和LSTM大小**
```python
#step5 逐步实现GRU网络

#对比GRU和LSTM大小
import torch
import torch.nn as nn
lstm_layer = nn.LSTM(3, 5)
gru_layer = nn.GRU(3,5)
#在此强调parameters()是一个函数！
sum(p.numel() for p in lstm_layer.parameters()) #200
sum(p.numel() for p in gru_layer.parameters()) #150

```

**下面开始手写GRU**
```python
#开始实现
def gru_forward(input, initial_states,w_ih,w_hh,b_ih, b_hh):
    prev_h = initial_states
    bs, T, i_size = input.shape
    h_size = w_ih.shape[0] // 3
    #对权重扩维，变成batch_size倍
    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)
    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)

    output = torch.zeros(bs, T, h_size)  #GRU网络输出序列

    for t in range(T):
        x = input[:,t,:]   #t时刻输入 [bs, i_size]
        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)) #[bs,3*hs,1]
        w_times_x = w_times_x.squeeze(-1)

        w_times_h_prev= torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)) #[bs,3*hs,1]
        w_times_h_prev = w_times_h_prev.squeeze(-1)

        r_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] + b_ih[:h_size] + b_hh[:h_size])#重置门
        z_t = torch.sigmoid(w_times_x[:,h_size:2*h_size] + w_times_h_prev[:,h_size:2*h_size] \
            + b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])#更新门
        
        n_t = torch.tanh(w_times_x[:,2*h_size:3*h_size] + b_ih[2*h_size:3*h_size] +\
            r_t * (w_times_h_prev[:,2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])) #候选状态

        prev_h = (1-z_t) * n_t + z_t * prev_h  #更新隐藏状态
        output[:,t,:] = prev_h
    
    return output,prev_h


#测试函数正确性
bs, T, i_size, h_size = 2, 3, 4, 5
input = torch.randn(bs, T, i_size) #输入序列
h0 = torch.randn(bs, h_size) #初始值不参与训练，api里面维度(D∗num_layers,N,Hout)

gru_layer = nn.GRU(i_size, h_size, batch_first = True)
output, h_final = gru_layer(input,h0.unsqueeze(0))
print(output)
'''
weight_ih_l0 torch.Size([15, 4])
weight_hh_l0 torch.Size([15, 5])
bias_ih_l0 torch.Size([15])
bias_hh_l0 torch.Size([15])
'''
for k, v in gru_layer.named_parameters():
    print(k, v.shape)

output_custom, h_final_custom = gru_forward(input, h0, gru_layer.weight_ih_l0, \
    gru_layer.weight_hh_l0, gru_layer.bias_ih_l0, gru_layer.bias_hh_l0)

print(output_custom)

```