#! https://zhuanlan.zhihu.com/p/555709668
# 22.PyTorch RNN的原理及其手写复现

![](https://pic4.zhimg.com/80/v2-b3e9592be808bacbbfd9cd095c67cddd.png)
![](https://pic4.zhimg.com/80/v2-bea1cbcea76cec02eb68d5272d32c634.png)


RNN能处理变长序列的原因是：各部分**权重共享**
模型大小与**隐含单元**有关

参数量基本相同的情况下，序列模型性能对比
摘自《Supervised Sequence Labelling with Recurrent Nerual Network》
![序列模型对比](https://pic4.zhimg.com/80/v2-6438c3793211c0078dd1db3b422d0607.png)

上图中的delay理解为下图"many to many"的形式，模型看到更多上下文效果更好：
one to many :AI诗歌生成
many to one:情感文本分类
many to many:词法识别（图5）、机器翻译（图4，二者细节还是有区别，机器翻译是seq2seq）
![](https://pic4.zhimg.com/80/v2-c9537f45f4da5a07fbd918a041a31723.png)

[RNN官网介绍](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)

$$
h_{t}=\tanh \left(x_{t} W_{i h}^{T}+b_{i h}+h_{t-1} W_{h h}^{T}+b_{h h}\right)
$$

要注意的参数：
`bidirectional – If True, becomes a bidirectional RNN. Default: False`


`batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False`
设置了batch_first，则batch维度在最前面，默认为false

官网示例
```python
rnn = nn.RNN(10, 20, 2) #input_size * hidden_size * num_layers
input = torch.randn(5, 3, 10) #seq_len * batch_size * input_size
h0 = torch.randn(2, 3, 20) #D*numlayers, batch_size, hidden_size 单向D=1,双向D=2
output, hn = rnn(input, h0) #
```

个人示例
```python
import torch
# 单向、单层RNN
import torch.nn as nn
# 1.单向、单层RNN
single_rnn = nn.RNN(4, 3, 1,batch_size = True)
input = torch.randn(1, 2, 4)  #bs * sl *feature_size
output, h_n = single_rnn(input, )
print(output)  # 1* 2* 3 bs*sl*output_size
print(h_n) #1*1*3  1*1,1,output_size
```

```python
#2.双向、单层RNN
from unicodedata import bidirectional

bidirectional_rnn = nn.RNN(4,3,1,batch_first = True, bidirectional = True)
bi_output,bi_h_n = bidirectional_rnn(input)
print(bi_output.shape)  #1,2,6 6是因为最后把forward和backward layer两个输出拼起来
print(bi_h_n.shape)   #2,1,3

```

**单向RNN**
```python
import torch
import torch.nn as nn
bs, T = 2, 3#batch_size和序列长度
input_size , hidden_size = 2, 3
input = torch.randn(bs, T, input_size) #随机初始化一个输入特征序列
h_prev = torch.zeros(bs , hidden_size) #初始（第0时刻）隐含状态

#step1 调用pytorch rnn api
rnn = nn.RNN(input_size, hidden_size,batch_first = True)
rnn_output, state_final = rnn(input, h_prev.unsqueeze(0))
# print(h_prev.unsqueeze(0).shape)
print(rnn_output)
print(state_final)


#step2 手写一个rnn_forward函数,由于1.0版本之后RNN的核心函数是用C语言写的，这里先不看了
def rnn_forward(input, weight_ih,  weight_hh, bias_ih,bias_hh, h_prev):
    bs, T, input_size = input.shape
    h_dim = weight_ih.shape[0]
    h_out = torch.zeros(bs, T, h_dim) #初始化一个输出

    for t in range(T):
        x = input[:,t, :].unsqueeze(2) #获取当前时刻输入 bs * input_size * 1
        w_ih_batch = weight_ih.unsqueeze(0).tile(bs,1,1) #bs*h_dim*input_size1,2,3
        w_hh_batch = weight_hh.unsqueeze(0).tile(bs,1,1) #bs * h_dim *h_dim

        w_times_x = torch.bmm(w_ih_batch, x).squeeze(-1) #去掉最后1维 bs*h_dim
        w_times_h = torch.bmm(w_hh_batch, h_prev.unsqueeze(2)).squeeze(-1) #bs*h_dim
        # print(w_times_h.shape)
        # print(bias_ih.shape)
        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)#t时刻的输出
        h_out[:, t, :] = h_prev

    return h_out, h_prev.unsqueeze(0)

#验证手写的准确性
# for k,v in rnn.named_parameters():
#     print(k,v)
custom_rnn_output, custom_state_final = \
rnn_forward(input, rnn.weight_ih_l0, rnn.weight_hh_l0, rnn.bias_ih_l0, rnn.bias_hh_l0, h_prev)
print(custom_rnn_output)
print(custom_state_final)
```
**双向RNN**
```python
#step3 手写一个bidirectional_rnn_forward函数，实现双向rnn计算原理
def bidirectional_rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev,\
    weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse):
    bs, T, input_size = input.shape
    h_dim = weight_ih.shape[0]
    h_out = torch.zeros(bs, T, h_dim * 2) #初始化一个输出,双向是两倍
    
    forward_output = rnn_forward(input,weight_ih,weight_hh,bias_ih, bias_hh, h_prev)[0]
    #对于input在长度维度上翻转
    # print(input.shape)
    backward_output = rnn_forward(torch.flip(input, [1]),\
        weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse)[0]

    # print(backward_output.type)
    backward_output = torch.flip(backward_output, [1])
    h_out[:,:,:h_dim] = forward_output
    h_out[:,:,h_dim:] = backward_output

    return h_out, h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0, 1)

bi_rnn = nn.RNN(input_size, hidden_size,batch_first = True, bidirectional = True)
h_prev = torch.zeros(2, bs, hidden_size)
bi_rnn_output, bi_rnn_state_final = bi_rnn(input, h_prev)
# for k,v in bi_rnn.named_parameters():
#     print(k,v)

custom_bi_rnn_output, customm_bi_state_final = \
bidirectional_rnn_forward(input, bi_rnn.weight_ih_l0, bi_rnn.weight_hh_l0, bi_rnn.bias_ih_l0, bi_rnn.bias_hh_l0, h_prev[0],\
    bi_rnn.weight_ih_l0_reverse, bi_rnn.weight_hh_l0_reverse, bi_rnn.bias_ih_l0_reverse, bi_rnn.bias_hh_l0_reverse, h_prev[1]   )

print(bi_rnn_output)
print(custom_bi_rnn_output)
print(bi_rnn_state_final)
print(customm_bi_state_final)

```
