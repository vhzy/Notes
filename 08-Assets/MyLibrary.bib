@article{abachaVQAMedOverviewMedical,
  title = {{{VQA-Med}}: {{Overview}} of the {{Medical Visual Question Answering Task}} at {{ImageCLEF}} 2019},
  author = {Abacha, Asma Ben and Hasan, Sadid A and Datla, Vivek V and Liu, Joey and Muller, Henning},
  pages = {11},
  abstract = {This paper presents an overview of the Medical Visual Question Answering task (VQA-Med) at ImageCLEF 2019. Participating systems were tasked with answering medical questions based on the visual content of radiology images. In this second edition of VQA-Med, we focused on four categories of clinical questions: Modality, Plane, Organ System, and Abnormality. These categories are designed with different degrees of difficulty leveraging both classification and text generation approaches. We also ensured that all questions can be answered from the image content without requiring additional medical knowledge or domain-specific inference. We created a new dataset of 4,200 radiology images and 15,292 question-answer pairs following these guidelines. The challenge was well received with 17 participating teams who applied a wide range of approaches such as transfer learning, multi-task learning, and ensemble methods. The best team achieved a BLEU score of 64.4\% and an accuracy of 62.4\%. In future editions, we will consider designing more goal-oriented datasets and tackling new aspects such as contextual information and domain-specific inference.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\HXUF95SY\\Abacha 等。 - VQA-Med Overview of the Medical Visual Question A.pdf}
}

@inproceedings{abbasnejadCounterfactualVisionLanguage2020,
  title = {Counterfactual {{Vision}} and {{Language Learning}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Abbasnejad, Ehsan and Teney, Damien and Parvaneh, Amin and Shi, Javen and van den Hengel, Anton},
  options = {useprefix=true},
  date = {2020-06},
  pages = {10041--10051},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01006},
  url = {https://ieeexplore.ieee.org/document/9156448/},
  urldate = {2021-09-09},
  abstract = {The ongoing success of visual question answering methods has been somewhat surprising given that, at its most general, the problem requires understanding the entire variety of both visual and language stimuli. It is particularly remarkable that this success has been achieved on the basis of comparatively small datasets, given the scale of the problem. One explanation is that this has been accomplished partly by exploiting bias in the datasets rather than developing deeper multi-modal reasoning. This fundamentally limits the generalization of the method, and thus its practical applicability. We propose a method that addresses this problem by introducing counterfactuals in the training. In doing so we leverage structural causal models for counterfactual evaluation to formulate alternatives, for instance, questions that could be asked of the same image set. We show that simulating plausible alternative training data through this process results in better generalization.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  keywords = {bias},
  annotation = {4 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\VQ6PSJB4\\Abbasnejad 等。 - 2020 - Counterfactual Vision and Language Learning.pdf}
}

@unpublished{aberdamSequencetoSequenceContrastiveLearning2020,
  title = {Sequence-to-{{Sequence Contrastive Learning}} for {{Text Recognition}}},
  author = {Aberdam, Aviad and Litman, Ron and Tsiper, Shahar and Anschel, Oron and Slossberg, Ron and Mazor, Shai and Manmatha, R. and Perona, Pietro},
  date = {2020-12-20},
  eprint = {2012.10873},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.10873},
  urldate = {2022-04-02},
  abstract = {We propose a framework for sequence-to-sequence contrastive learning (SeqCLR) of visual representations, which we apply to text recognition. To account for the sequence-to-sequence structure, each feature map is divided into different instances over which the contrastive loss is computed. This operation enables us to contrast in a sub-word level, where from each image we extract several positive pairs and multiple negative examples. To yield effective visual representations for text recognition, we further suggest novel augmentation heuristics, different encoder architectures and custom projection heads. Experiments on handwritten text and on scene text show that when a text decoder is trained on the learned representations, our method outperforms non-sequential contrastive methods. In addition, when the amount of supervision is reduced, SeqCLR significantly improves performance compared with supervised training, and when fine-tuned with 100\% of the labels, our method achieves state-of-the-art results on standard handwritten text recognition benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\MFWT3UG4\\Aberdam et al_2020_Sequence-to-Sequence Contrastive Learning for Text Recognition.pdf;D\:\\Zotero\\storage\\B2DKWG5B\\2012.html}
}

@article{adityaExplicitReasoningEndtoEnd,
  title = {Explicit {{Reasoning}} over {{End-to-End Neural Architectures}} for {{Visual Question Answering}}},
  author = {Aditya, Somak and Yang, Yezhou and Baral, Chitta},
  pages = {9},
  abstract = {Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language processing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a question in natural language about an image. Current state-ofthe-art systems attempted to solve the task using deep neural architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle in understanding questions for which extra knowledge is required. In this paper, we present an explicit reasoning layer on top of a set of penultimate neural network based systems. The reasoning layer enables reasoning and answering questions where additional knowledge is required, and at the same time provides an interpretable interface to the end users. Specifically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based engine to reason over a basket of inputs: visual relations, the semantic parse of the question, and background ontological knowledge from word2vec and ConceptNet. Experimental analysis of the answers and the key evidential predicates generated on the VQA dataset validate our approach.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\27QUBS6E\\Aditya 等。 - Explicit Reasoning over End-to-End Neural Architec.pdf}
}

@inproceedings{agarwalCausalVQARevealing2020,
  title = {Towards {{Causal VQA}}: {{Revealing}} and {{Reducing Spurious Correlations}} by {{Invariant}} and {{Covariant Semantic Editing}}},
  shorttitle = {Towards {{Causal VQA}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Agarwal, Vedika and Shetty, Rakshith and Fritz, Mario},
  date = {2020-06},
  pages = {9687--9695},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00971},
  url = {https://ieeexplore.ieee.org/document/9156407/},
  urldate = {2021-09-09},
  abstract = {Despite significant success in Visual Question Answering (VQA), VQA models have been shown to be notoriously brittle to linguistic variations in the questions. Due to deficiencies in models and datasets, today’s models often rely on correlations rather than predictions that are causal w.r.t. relevant evidence. In this paper, we propose a novel way to analyze and measure the robustness of the state of the art models w.r.t semantic visual variations as well as propose ways to make models more robust against spurious correlations. Our method performs automated semantic image manipulations and tests for consistency in model predictions to quantify the model robustness as well as generate synthetic data to counter these problems. We perform our analysis on three diverse, state of the art VQA models and diverse question types with a particular focus on challenging counting questions. In addition, we show that models can be made significantly more robust against inconsistent predictions using our edited data. Finally, we show that results also translate to real-world error cases of state of the art models, which results in improved overall performance.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {4 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\J8XU85GK\\Agarwal 等。 - 2020 - Towards Causal VQA Revealing and Reducing Spuriou.pdf}
}

@inproceedings{agarwalHistoryVisualDialog2020,
  title = {History for {{Visual Dialog}}: {{Do}} We Really Need It?},
  shorttitle = {History for {{Visual Dialog}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Agarwal, Shubham and Bui, Trung and Lee, Joon-Young and Konstas, Ioannis and Rieser, Verena},
  date = {2020-07},
  pages = {8182--8197},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.728},
  url = {https://aclanthology.org/2020.acl-main.728},
  urldate = {2021-12-08},
  abstract = {Visual Dialogue involves “understanding” the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response. In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don't, achieving state-of-the-art performance (72 \% NDCG on val set). However, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue history is indeed only required for a small amount of the data, and that the current evaluation metric encourages generic replies. To that end, we propose a challenging subset (VisdialConv) of the VisdialVal set and the benchmark NDCG of 63\%.},
  eventtitle = {{{ACL}} 2020},
  file = {D\:\\Zotero\\storage\\GWVYZ7HV\\Agarwal et al_2020_History for Visual Dialog.pdf}
}

@unpublished{agrawalAnalyzingBehaviorVisual2016,
  title = {Analyzing the {{Behavior}} of {{Visual Question Answering Models}}},
  author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi},
  date = {2016-09-27},
  eprint = {1606.07356},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.07356},
  urldate = {2021-09-11},
  abstract = {Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70\%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016. Our behavior analysis reveals that despite recent progress, today's VQA models are "myopic" (tend to fail on sufficiently novel instances), often "jump to conclusions" (converge on a predicted answer after 'listening' to just half the question), and are "stubborn" (do not change their answers across images).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\UAVX9QI4\\Agrawal et al_2016_Analyzing the Behavior of Visual Question Answering Models.pdf;D\:\\Zotero\\storage\\6WMIGIT7\\1606.html}
}

@inproceedings{agrawalDonJustAssume2018,
  title = {Don't {{Just Assume}}; {{Look}} and {{Answer}}: {{Overcoming Priors}} for {{Visual Question Answering}}},
  shorttitle = {Don't {{Just Assume}}; {{Look}} and {{Answer}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  date = {2018-06},
  pages = {4971--4980},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00522},
  url = {https://ieeexplore.ieee.org/document/8578620/},
  urldate = {2021-09-09},
  abstract = {A number of studies have found that today’s Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQACP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from ‘cheating’ by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model – Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {53 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\5BVQTF9L\\Agrawal 等。 - 2018 - Don't Just Assume\; Look and Answer Overcoming Pri.pdf}
}

@unpublished{ahmadGATEGraphAttention2021,
  title = {{{GATE}}: {{Graph Attention Transformer Encoder}} for {{Cross-lingual Relation}} and {{Event Extraction}}},
  shorttitle = {{{GATE}}},
  author = {Ahmad, Wasi Uddin and Peng, Nanyun and Chang, Kai-Wei},
  date = {2021-02-17},
  eprint = {2010.03009},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.03009},
  urldate = {2022-03-26},
  abstract = {Recent progress in cross-lingual relation and event extraction use graph convolutional networks (GCNs) with universal dependency parses to learn language-agnostic sentence representations such that models trained on one language can be applied to other languages. However, GCNs struggle to model words with long-range dependencies or are not directly connected in the dependency tree. To address these challenges, we propose to utilize the self-attention mechanism where we explicitly fuse structural information to learn the dependencies between words with different syntactic distances. We introduce GATE, a \{\textbackslash bf G\}raph \{\textbackslash bf A\}ttention \{\textbackslash bf T\}ransformer \{\textbackslash bf E\}ncoder, and test its cross-lingual transferability on relation and event extraction tasks. We perform experiments on the ACE05 dataset that includes three typologically different languages: English, Chinese, and Arabic. The evaluation results show that GATE outperforms three recently proposed methods by a large margin. Our detailed analysis reveals that due to the reliance on syntactic dependencies, GATE produces robust representations that facilitate transfer across languages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\D3EQJJWH\\Ahmad et al_2021_GATE.pdf;D\:\\Zotero\\storage\\NSNWM7WX\\2010.html}
}

@misc{alayracFlamingoVisualLanguage2022,
  title = {Flamingo: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  shorttitle = {Flamingo},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  date = {2022-04-29},
  number = {arXiv:2204.14198},
  eprint = {2204.14198},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.14198},
  urldate = {2022-07-06},
  abstract = {Building models that can be rapidly adapted to numerous tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. Flamingo models include key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of the proposed Flamingo models, exploring and measuring their ability to rapidly adapt to a variety of image and video understanding benchmarks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple choice visual question-answering. For tasks lying anywhere on this spectrum, we demonstrate that a single Flamingo model can achieve a new state of the art for few-shot learning, simply by prompting the model with task-specific examples. On many of these benchmarks, Flamingo actually surpasses the performance of models that are fine-tuned on thousands of times more task-specific data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\KZ5IJPPF\\Alayrac et al_2022_Flamingo.pdf;D\:\\Zotero\\storage\\8LFQJKS2\\2204.html}
}

@inproceedings{albertiFusionDetectedObjects2019,
  title = {Fusion of {{Detected Objects}} in {{Text}} for {{Visual Question Answering}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Alberti, Chris and Ling, Jeffrey and Collins, Michael and Reitter, David},
  date = {2019},
  pages = {2131--2140},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1219},
  url = {https://www.aclweb.org/anthology/D19-1219},
  urldate = {2021-03-11},
  abstract = {To advance models of multimodal context, we introduce a simple yet powerful neural architecture for data that combines vision and natural language. The “Bounding Boxes in Text Transformer” (B2T2) also leverages referential information binding words to portions of the image in a single unified architecture. B2T2 is highly effective on the Visual Commonsense Reasoning benchmark1, achieving a new state-of-the-art with a 25\% relative reduction in error rate compared to published baselines and obtaining the best performance to date on the public leaderboard (as of May 22, 2019). A detailed ablation analysis shows that the early integration of the visual features into the text analysis is key to the effectiveness of the new architecture. A reference implementation of our models is provided2.},
  eventtitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  langid = {english},
  file = {D\:\\Zotero\\storage\\REENKL84\\Alberti 等。 - 2019 - Fusion of Detected Objects in Text for Visual Ques.pdf}
}

@inproceedings{alyHierarchicalMultilabelClassification2019,
  title = {Hierarchical {{Multi-label Classification}} of {{Text}} with {{Capsule Networks}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Student Research Workshop}}},
  author = {Aly, Rami and Remus, Steffen and Biemann, Chris},
  date = {2019-07},
  pages = {323--330},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-2045},
  url = {https://aclanthology.org/P19-2045},
  urldate = {2021-10-10},
  abstract = {Capsule networks have been shown to demonstrate good performance on structured data in the area of visual inference. In this paper we apply and compare simple shallow capsule networks for hierarchical multi-label text classification and show that they can perform superior to other neural networks, such as CNNs and LSTMs, and non-neural network architectures such as SVMs. For our experiments, we use the established Web of Science (WOS) dataset and introduce a new real-world scenario dataset, the BlurbGenreCollection (BGC). Our results confirm the hypothesis that capsule networks are especially advantageous for rare events and structurally diverse categories, which we attribute to their ability to combine latent encoded information.},
  file = {D\:\\Zotero\\storage\\R95JILAA\\Aly et al_2019_Hierarchical Multi-label Classification of Text with Capsule Networks.pdf}
}

@inproceedings{andersonBottomUpTopDownAttention2018,
  title = {Bottom-{{Up}} and {{Top-Down Attention}} for {{Image Captioning}} and {{Visual Question Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  date = {2018-06},
  pages = {6077--6086},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00636},
  url = {https://ieeexplore.ieee.org/document/8578734/},
  urldate = {2021-09-09},
  abstract = {Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and topdown attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  keywords = {feature},
  annotation = {707 citations (Crossref) [2021-09-10] SC: None[s0]},
  file = {D\:\\Zotero\\storage\\SPFZSEGW\\Anderson 等。 - 2018 - Bottom-Up and Top-Down Attention for Image Caption.pdf}
}

@unpublished{andersonBottomUpTopDownAttention2018a,
  title = {Bottom-{{Up}} and {{Top-Down Attention}} for {{Image Captioning}} and {{Visual Question Answering}}},
  author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  date = {2018-03-14},
  eprint = {1707.07998},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.07998},
  urldate = {2021-08-13},
  abstract = {Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\ZLH3BG5B\\Anderson et al_2018_Bottom-Up and Top-Down Attention for Image Captioning and Visual Question.pdf;D\:\\Zotero\\storage\\5Z748MH8\\1707.html}
}

@article{antolVQAVisualQuestion,
  title = {{{VQA}}: {{Visual Question Answering}}},
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  pages = {9},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZBINSQWN\\Antol 等。 - VQA Visual Question Answering.pdf}
}

@inproceedings{antolVQAVisualQuestion2015,
  title = {{{VQA}}: {{Visual Question Answering}}},
  shorttitle = {{{VQA}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
  date = {2015-12},
  pages = {2425--2433},
  publisher = {{IEEE}},
  location = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.279},
  url = {http://ieeexplore.ieee.org/document/7410636/},
  urldate = {2021-09-11},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4673-8391-2},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{antolVQAVisualQuestion2015a,
  title = {{{VQA}}: {{Visual Question Answering}}},
  shorttitle = {{{VQA}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
  date = {2015-12},
  pages = {2425--2433},
  publisher = {{IEEE}},
  location = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.279},
  url = {http://ieeexplore.ieee.org/document/7410636/},
  urldate = {2021-09-11},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4673-8391-2},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{antolVQAVisualQuestion2015b,
  title = {{{VQA}}: {{Visual Question Answering}}},
  shorttitle = {{{VQA}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
  date = {2015-12},
  pages = {2425--2433},
  publisher = {{IEEE}},
  location = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.279},
  url = {http://ieeexplore.ieee.org/document/7410636/},
  urldate = {2021-09-11},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4673-8391-2},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{antolVQAVisualQuestion2015c,
  title = {{{VQA}}: {{Visual Question Answering}}},
  shorttitle = {{{VQA}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
  date = {2015-12},
  pages = {2425--2433},
  publisher = {{IEEE}},
  location = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.279},
  url = {http://ieeexplore.ieee.org/document/7410636/},
  urldate = {2021-03-09},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4673-8391-2},
  langid = {english}
}

@misc{arjovskyInvariantRiskMinimization2020,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  date = {2020-03-27},
  number = {arXiv:1907.02893},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1907.02893},
  urldate = {2022-10-10},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\8GDY63BY\\Arjovsky 等。 - 2020 - Invariant Risk Minimization.pdf}
}

@unpublished{azumaScanQA3DQuestion2022,
  title = {{{ScanQA}}: {{3D Question Answering}} for {{Spatial Scene Understanding}}},
  shorttitle = {{{ScanQA}}},
  author = {Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoaki},
  date = {2022-05-07},
  number = {arXiv:2112.10482},
  eprint = {2112.10482},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.10482},
  urldate = {2022-06-02},
  abstract = {We propose a new 3D spatial understanding task of 3D Question Answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of the rich RGB-D indoor scan and answer the given textual questions about the 3D scene. Unlike the 2D-question answering of VQA, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail the object identification from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model, where the model learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates the language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine described objects in textual questions and outputs correct answers. We collected human-edited question-answer pairs with free-form answers that are grounded to 3D objects in each 3D scene. Our new ScanQA dataset contains over 40K question-answer pairs from the 800 indoor scenes drawn from the ScanNet dataset. To the best of our knowledge, the proposed 3D-QA task is the first large-scale effort to perform object-grounded question-answering in 3D environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\7EPJ74JF\\Azuma et al_2022_ScanQA.pdf;D\:\\Zotero\\storage\\EBNY38YM\\2112.html}
}

@inproceedings{baekWhatIfWe2021,
  title = {What {{If We Only Use Real Datasets}} for {{Scene Text Recognition}}? {{Toward Scene Text Recognition With Fewer Labels}}},
  shorttitle = {What {{If We Only Use Real Datasets}} for {{Scene Text Recognition}}?},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Baek, Jeonghun and Matsui, Yusuke and Aizawa, Kiyoharu},
  date = {2021-06},
  pages = {3112--3121},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00313},
  url = {https://ieeexplore.ieee.org/document/9578847/},
  urldate = {2022-04-20},
  abstract = {Scene text recognition (STR) task has a common practice: All state-of-the-art STR models are trained on large synthetic data. In contrast to this practice, training STR models only on fewer real labels (STR with fewer labels) is important when we have to train STR models without synthetic data: for handwritten or artistic texts that are difficult to generate synthetically and for languages other than English for which we do not always have synthetic data. However, there has been implicit common knowledge that training STR models on real data is nearly impossible because real data is insufficient. We consider that this common knowledge has obstructed the study of STR with fewer labels. In this work, we would like to reactivate STR with fewer labels by disproving the common knowledge. We consolidate recently accumulated public real data and show that we can train STR models satisfactorily only with real labeled data. Subsequently, we find simple data augmentation to fully exploit real data. Furthermore, we improve the models by collecting unlabeled data and introducing semi- and self-supervised methods. As a result, we obtain a competitive model to state-of-the-art methods. To the best of our knowledge, this is the first study that 1) shows sufficient performance by only using real labels and 2) introduces semi- and self-supervised methods into STR with fewer labels. Our code and data are available: https: //github.com/ku21fan/STR-Fewer-Labels.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\3HDKZKJ6\\Baek 等。 - 2021 - What If We Only Use Real Datasets for Scene Text R.pdf}
}

@unpublished{baekWhatIfWe2021a,
  title = {What {{If We Only Use Real Datasets}} for {{Scene Text Recognition}}? {{Toward Scene Text Recognition With Fewer Labels}}},
  shorttitle = {What {{If We Only Use Real Datasets}} for {{Scene Text Recognition}}?},
  author = {Baek, Jeonghun and Matsui, Yusuke and Aizawa, Kiyoharu},
  date = {2021-06-05},
  eprint = {2103.04400},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.04400},
  urldate = {2022-04-02},
  abstract = {Scene text recognition (STR) task has a common practice: All state-of-the-art STR models are trained on large synthetic data. In contrast to this practice, training STR models only on fewer real labels (STR with fewer labels) is important when we have to train STR models without synthetic data: for handwritten or artistic texts that are difficult to generate synthetically and for languages other than English for which we do not always have synthetic data. However, there has been implicit common knowledge that training STR models on real data is nearly impossible because real data is insufficient. We consider that this common knowledge has obstructed the study of STR with fewer labels. In this work, we would like to reactivate STR with fewer labels by disproving the common knowledge. We consolidate recently accumulated public real data and show that we can train STR models satisfactorily only with real labeled data. Subsequently, we find simple data augmentation to fully exploit real data. Furthermore, we improve the models by collecting unlabeled data and introducing semi- and self-supervised methods. As a result, we obtain a competitive model to state-of-the-art methods. To the best of our knowledge, this is the first study that 1) shows sufficient performance by only using real labels and 2) introduces semi- and self-supervised methods into STR with fewer labels. Our code and data are available: https://github.com/ku21fan/STR-Fewer-Labels},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\EQ3NYDF8\\Baek et al_2021_What If We Only Use Real Datasets for Scene Text Recognition.pdf;D\:\\Zotero\\storage\\FMBIYTJ4\\2103.html}
}

@incollection{baiDeepAttentionNeural2018,
  title = {Deep {{Attention Neural Tensor Network}} for {{Visual Question Answering}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Bai, Yalong and Fu, Jianlong and Zhao, Tiejun and Mei, Tao},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11216},
  pages = {21--37},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01258-8_2},
  url = {http://link.springer.com/10.1007/978-3-030-01258-8_2},
  urldate = {2021-09-10},
  abstract = {Visual question answering (VQA) has drawn great attention in cross-modal learning problems, which enables a machine to answer a natural language question given a reference image. Significant progress has been made by learning rich embedding features from images and questions by bilinear models, while neglects the key role from answers. In this paper, we propose a novel deep attention neural tensor network (DA-NTN) for visual question answering, which can discover the joint correlations over images, questions and answers with tensor-based representations. First, we model one of the pairwise interaction (e.g., image and question) by bilinear features, which is further encoded with the third dimension (e.g., answer) to be a triplet by bilinear tensor product. Second, we decompose the correlation of different triplets by different answer and question types, and further propose a slice-wise attention module on tensor to select the most discriminative reasoning process for inference. Third, we optimize the proposed DA-NTN by learning a label regression with KL-divergence losses. Such a design enables scalable training and fast convergence over a large number of answer set. We integrate the proposed DA-NTN structure into the state-of-the-art VQA models (e.g., MLB and MUTAN). Extensive experiments demonstrate the superior accuracy than the original MLB and MUTAN models, with 1.98\%, 1.70\% relative increases on VQA-2.0 dataset, respectively.},
  isbn = {978-3-030-01257-1 978-3-030-01258-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\7B68IBMS\\Bai 等。 - 2018 - Deep Attention Neural Tensor Network for Visual Qu.pdf}
}

@article{baldiDropoutLearningAlgorithm2014,
  title = {The Dropout Learning Algorithm},
  author = {Baldi, Pierre and Sadowski, Peter},
  date = {2014-05},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {210},
  pages = {78--122},
  issn = {00043702},
  doi = {10.1016/j.artint.2014.02.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370214000216},
  urldate = {2022-10-04},
  abstract = {Dropout is a recently introduced algorithm for training neural networks by randomly dropping units during training to prevent their co-adaptation. A mathematical analysis of some of the static and dynamic properties of dropout is provided using Bernoulli gating variables, general enough to accommodate dropout on units or connections, and with variable rates. The framework allows a complete analysis of the ensemble averaging properties of dropout in linear networks, which is useful to understand the non-linear case. The ensemble averaging properties of dropout in non-linear logistic networks result from three fundamental equations: (1) the approximation of the expectations of logistic functions by normalized geometric means, for which bounds and estimates are derived; (2) the algebraic equality between normalized geometric means of logistic functions with the logistic of the means, which mathematically characterizes logistic functions; and (3) the linearity of the means with respect to sums, as well as products of independent variables. The results are also extended to other classes of transfer functions, including rectified linear functions. Approximation errors tend to cancel each other and do not accumulate. Dropout can also be connected to stochastic neurons and used to predict firing rates, and to backpropagation by viewing the backward propagation as ensemble averaging in a dropout linear network. Moreover, the convergence properties of dropout can be understood in terms of stochastic gradient descent. Finally, for the regularization properties of dropout, the expectation of the dropout gradient is the gradient of the corresponding approximation ensemble, regularized by an adaptive weight decay term with a propensity for self-consistent variance minimization and sparse representations.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\VG6T5MLF\\Baldi 和 Sadowski - 2014 - The dropout learning algorithm.pdf}
}

@unpublished{banerjeeWeaklySupervisedRelative2021,
  title = {Weakly {{Supervised Relative Spatial Reasoning}} for {{Visual Question Answering}}},
  author = {Banerjee, Pratyay and Gokhale, Tejas and Yang, Yezhou and Baral, Chitta},
  date = {2021-09-04},
  eprint = {2109.01934},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.01934},
  urldate = {2022-03-22},
  abstract = {Vision-and-language (V\textbackslash\&L) reasoning necessitates perception of visual concepts such as objects and actions, understanding semantics and language grounding, and reasoning about the interplay between the two modalities. One crucial aspect of visual reasoning is spatial understanding, which involves understanding relative locations of objects, i.e.\textbackslash{} implicitly learning the geometry of the scene. In this work, we evaluate the faithfulness of V\textbackslash\&L models to such geometric understanding, by formulating the prediction of pair-wise relative locations of objects as a classification as well as a regression task. Our findings suggest that state-of-the-art transformer-based V\textbackslash\&L models lack sufficient abilities to excel at this task. Motivated by this, we design two objectives as proxies for 3D spatial reasoning (SR) -- object centroid estimation, and relative position estimation, and train V\textbackslash\&L with weak supervision from off-the-shelf depth estimators. This leads to considerable improvements in accuracy for the "GQA" visual question answering challenge (in fully supervised, few-shot, and O.O.D settings) as well as improvements in relative spatial reasoning. Code and data will be released \textbackslash href\{https://github.com/pratyay-banerjee/weak\_sup\_vqa\}\{here\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\KYNC599W\\Banerjee et al_2021_Weakly Supervised Relative Spatial Reasoning for Visual Question Answering.pdf;D\:\\Zotero\\storage\\I8S5UNCN\\2109.html}
}

@unpublished{banerjeeWeaklySupervisedRelative2021a,
  title = {Weakly {{Supervised Relative Spatial Reasoning}} for {{Visual Question Answering}}},
  author = {Banerjee, Pratyay and Gokhale, Tejas and Yang, Yezhou and Baral, Chitta},
  date = {2021-09-04},
  eprint = {2109.01934},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.01934},
  urldate = {2021-09-07},
  abstract = {Vision-and-language (V\textbackslash\&L) reasoning necessitates perception of visual concepts such as objects and actions, understanding semantics and language grounding, and reasoning about the interplay between the two modalities. One crucial aspect of visual reasoning is spatial understanding, which involves understanding relative locations of objects, i.e.\textbackslash{} implicitly learning the geometry of the scene. In this work, we evaluate the faithfulness of V\textbackslash\&L models to such geometric understanding, by formulating the prediction of pair-wise relative locations of objects as a classification as well as a regression task. Our findings suggest that state-of-the-art transformer-based V\textbackslash\&L models lack sufficient abilities to excel at this task. Motivated by this, we design two objectives as proxies for 3D spatial reasoning (SR) -- object centroid estimation, and relative position estimation, and train V\textbackslash\&L with weak supervision from off-the-shelf depth estimators. This leads to considerable improvements in accuracy for the "GQA" visual question answering challenge (in fully supervised, few-shot, and O.O.D settings) as well as improvements in relative spatial reasoning. Code and data will be released \textbackslash href\{https://github.com/pratyay-banerjee/weak\_sup\_vqa\}\{here\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\K8LDBZPR\\Banerjee et al_2021_Weakly Supervised Relative Spatial Reasoning for Visual Question Answering.pdf;D\:\\Zotero\\storage\\JPIB8CRE\\2109.html}
}

@incollection{bansalVisualQuestionAnswering2020,
  title = {Visual {{Question Answering}} on {{Image Sets}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Bansal, Ankan and Zhang, Yuting and Chellappa, Rama},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12366},
  pages = {51--67},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58589-1_4},
  url = {https://link.springer.com/10.1007/978-3-030-58589-1_4},
  urldate = {2021-09-10},
  abstract = {We introduce the task of Image-Set Visual Question Answering (ISVQA), which generalizes the commonly studied single-image VQA problem to multi-image settings. Taking a natural language question and a set of images as input, it aims to answer the question based on the content of the images. The questions can be about objects and relationships in one or more images or about the entire scene depicted by the image set. To enable research in this new topic, we introduce two ISVQA datasets – indoor and outdoor scenes. They simulate the real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset contains 91,479 human-annotated questions for 48,138 image sets, and the outdoor-scene dataset has 49,617 questions for 12,746 image sets. We analyze the properties of the two datasets, including question-and-answer distributions, types of questions, biases in dataset, and question-image dependencies. We also build new baseline models to investigate new research challenges in ISVQA.},
  isbn = {978-3-030-58588-4 978-3-030-58589-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\HGN4XVM4\\Bansal 等。 - 2020 - Visual Question Answering on Image Sets.pdf}
}

@unpublished{barillotModellingSemanticsText2022,
  title = {Modelling the Semantics of Text in Complex Document Layouts Using Graph Transformer Networks},
  author = {Barillot, Thomas Roland and Saks, Jacob and Lilyanova, Polena and Torgas, Edward and Hu, Yachen and Liu, Yuanqing and Balupuri, Varun and Gaskell, Paul},
  date = {2022-02-18},
  eprint = {2202.09144},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.09144},
  urldate = {2022-03-26},
  abstract = {Representing structured text from complex documents typically calls for different machine learning techniques, such as language models for paragraphs and convolutional neural networks (CNNs) for table extraction, which prohibits drawing links between text spans from different content types. In this article we propose a model that approximates the human reading pattern of a document and outputs a unique semantic representation for every text span irrespective of the content type they are found in. We base our architecture on a graph representation of the structured text, and we demonstrate that not only can we retrieve semantically similar information across documents but also that the embedding space we generate captures useful semantic information, similar to language models that work only on text sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {D\:\\Zotero\\storage\\X5WES899\\Barillot et al_2022_Modelling the semantics of text in complex document layouts using graph.pdf;D\:\\Zotero\\storage\\HX7BZKIM\\2202.html}
}

@misc{Baydin2018,
  title = {Automatic Differentiation in Machine Learning: A Survey},
  shorttitle = {Automatic Differentiation in Machine Learning},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  date = {2018-02-05},
  number = {arXiv:1502.05767},
  eprint = {1502.05767},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1502.05767},
  urldate = {2022-08-13},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
  archiveprefix = {arXiv},
  keywords = {68W30; 65D25; 68T05,Computer Science - Machine Learning,Computer Science - Symbolic Computation,G.1.4,I.2.6,Statistics - Machine Learning},
  file = {D\:\\Notes\\working\\08-Assets\\Scripts\\@baydinAutomaticDifferentiationMachine2018.md;D\:\\Notes\\working\\08-Assets\\Scripts\\Automatic differentiation in machine learning a survey.md;D\:\\Zotero\\storage\\5YTCWDC6\\Baydin et al_2018_Automatic differentiation in machine learning.pdf;D\:\\Zotero\\storage\\QZ6W8LVT\\1502.html}
}

@article{ben-younesBLOCKBilinearSuperdiagonal2019,
  title = {{{BLOCK}}: {{Bilinear Superdiagonal Fusion}} for {{Visual Question Answering}} and {{Visual Relationship Detection}}},
  shorttitle = {{{BLOCK}}},
  author = {Ben-younes, Hedi and Cadene, Remi and Thome, Nicolas and Cord, Matthieu},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {8102--8109},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33018102},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/4818},
  urldate = {2021-09-10},
  abstract = {Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block.bootstrap.pytorch.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SCP2CYU6\\Ben-younes 等。 - 2019 - BLOCK Bilinear Superdiagonal Fusion for Visual Qu.pdf}
}

@inproceedings{ben-younesMUTANMultimodalTucker2017,
  title = {{{MUTAN}}: {{Multimodal Tucker Fusion}} for {{Visual Question Answering}}},
  shorttitle = {{{MUTAN}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Ben-younes, Hedi and Cadene, Remi and Cord, Matthieu and Thome, Nicolas},
  date = {2017-10},
  pages = {2631--2639},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.285},
  url = {http://ieeexplore.ieee.org/document/8237547/},
  urldate = {2021-09-10},
  abstract = {Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  keywords = {MarioQA: Answering Questions by Watching Gameplay Videos},
  file = {D\:\\Zotero\\storage\\TC5Y5NVB\\Ben-younes 等。 - 2017 - MUTAN Multimodal Tucker Fusion for Visual Questio.pdf}
}

@unpublished{bitenLaTrLayoutAwareTransformer2021,
  title = {{{LaTr}}: {{Layout-Aware Transformer}} for {{Scene-Text VQA}}},
  shorttitle = {{{LaTr}}},
  author = {Biten, Ali Furkan and Litman, Ron and Xie, Yusheng and Appalaraju, Srikar and Manmatha, R.},
  date = {2021-12-24},
  eprint = {2112.12494},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.12494},
  urldate = {2022-02-05},
  abstract = {We propose a novel multimodal architecture for Scene Text Visual Question Answering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA requires models to reason over different modalities. Thus, we first investigate the impact of each modality, and reveal the importance of the language module, especially when enriched with layout information. Accounting for this, we propose a single objective pre-training scheme that requires only text and spatial cues. We show that applying this pre-training scheme on scanned documents has certain advantages over using natural images, despite the domain gap. Scanned documents are easy to procure, text-dense and have a variety of layouts, helping the model learn various spatial cues (e.g. left-of, below etc.) by tying together language and layout information. Compared to existing approaches, our method performs vocabulary-free decoding and, as shown, generalizes well beyond the training vocabulary. We further demonstrate that LaTr improves robustness towards OCR errors, a common reason for failure cases in STVQA. In addition, by leveraging a vision transformer, we eliminate the need for an external object detector. LaTr outperforms state-of-the-art STVQA methods on multiple datasets. In particular, +7.6\% on TextVQA, +10.8\% on ST-VQA and +4.0\% on OCR-VQA (all absolute accuracy numbers).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\AGWZZX5X\\Biten et al_2021_LaTr.pdf;D\:\\Zotero\\storage\\TWE6RUAJ\\2112.html}
}

@unpublished{bitenSceneTextVisual2019,
  title = {Scene {{Text Visual Question Answering}}},
  author = {Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusiñol, Marçal and Valveny, Ernest and Jawahar, C. V. and Karatzas, Dimosthenis},
  date = {2019-10-16},
  eprint = {1905.13648},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1905.13648},
  urldate = {2021-12-08},
  abstract = {Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the VQA process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\74AN5RZE\\Biten et al_2019_Scene Text Visual Question Answering.pdf;D\:\\Zotero\\storage\\SJRHPHQK\\1905.html}
}

@unpublished{buiTreeCapsTreeBasedCapsule2020,
  title = {{{TreeCaps}}: {{Tree-Based Capsule Networks}} for {{Source Code Processing}}},
  shorttitle = {{{TreeCaps}}},
  author = {Bui, Nghi D. Q. and Yu, Yijun and Jiang, Lingxiao},
  date = {2020-12-14},
  eprint = {2009.09777},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2009.09777},
  urldate = {2021-10-04},
  abstract = {Recently program learning techniques have been proposed to process source code based on syntactical structures (e.g., Abstract Syntax Trees) and/or semantic information (e.g., Dependency Graphs). Although graphs may be better at capturing various viewpoints of code semantics than trees, constructing graph inputs from code needs static code semantic analysis that may not be accurate and introduces noise during learning. Although syntax trees are precisely defined according to the language grammar and easier to construct and process than graphs, previous tree-based learning techniques have not been able to learn semantic information from trees to achieve better accuracy than graph-based techniques. We propose a new learning technique, named TreeCaps, by fusing together capsule networks with tree-based convolutional neural networks, to achieve learning accuracy higher than existing graph-based techniques while it is based only on trees. TreeCaps introduces novel variable-to-static routing algorithms into the capsule networks to compensate for the loss of previous routing algorithms. Aside from accuracy, we also find that TreeCaps is the most robust to withstand those semantic-preserving program transformations that change code syntax without modifying the semantics. Evaluated on a large number of Java and C/C++ programs, TreeCaps models outperform prior deep learning models of program source code, in terms of both accuracy and robustness for program comprehension tasks such as code functionality classification and function name prediction},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {D\:\\Zotero\\storage\\9CAI3JV2\\Bui et al_2020_TreeCaps.pdf;D\:\\Zotero\\storage\\MM4GT9H9\\2009.html}
}

@unpublished{cadeneMURELMultimodalRelational2019,
  title = {{{MUREL}}: {{Multimodal Relational Reasoning}} for {{Visual Question Answering}}},
  shorttitle = {{{MUREL}}},
  author = {Cadene, Remi and Ben-younes, Hedi and Cord, Matthieu and Thome, Nicolas},
  date = {2019-02-25},
  eprint = {1902.09487},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.09487},
  urldate = {2021-09-09},
  abstract = {Multimodal attentional networks are currently state-of-the-art models for Visual Question Answering (VQA) tasks involving real images. Although attention allows to focus on the visual content relevant to the question, this simple mechanism is arguably insufficient to model complex reasoning features required for VQA or other high-level tasks. In this paper, we propose MuRel, a multimodal relational network which is learned end-to-end to reason over real images. Our first contribution is the introduction of the MuRel cell, an atomic reasoning primitive representing interactions between question and image regions by a rich vectorial representation, and modeling region relations with pairwise combinations. Secondly, we incorporate the cell into a full MuRel network, which progressively refines visual and question interactions, and can be leveraged to define visualization schemes finer than mere attention maps. We validate the relevance of our approach with various ablation studies, and show its superiority to attention-based methods on three datasets: VQA 2.0, VQA-CP v2 and TDIUC. Our final MuRel network is competitive to or outperforms state-of-the-art results in this challenging context. Our code is available: https://github.com/Cadene/murel.bootstrap.pytorch},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Reasoning},
  file = {D\:\\Zotero\\storage\\47V2S4TK\\Cadene et al_2019_MUREL.pdf;D\:\\Zotero\\storage\\SBUZB2CV\\1902.html}
}

@inproceedings{cadeneMURELMultimodalRelational2019a,
  title = {{{MUREL}}: {{Multimodal Relational Reasoning}} for {{Visual Question Answering}}},
  shorttitle = {{{MUREL}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cadene, Remi and Ben-younes, Hedi and Cord, Matthieu and Thome, Nicolas},
  date = {2019-06},
  pages = {1989--1998},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00209},
  url = {https://ieeexplore.ieee.org/document/8953864/},
  urldate = {2021-03-11},
  abstract = {Multimodal attentional networks are currently state-ofthe-art models for Visual Question Answering (VQA) tasks involving real images. Although attention allows to focus on the visual content relevant to the question, this simple mechanism is arguably insufficient to model complex reasoning features required for VQA or other high-level tasks. In this paper, we propose MuRel, a multimodal relational network which is learned end-to-end to reason over real images. Our first contribution is the introduction of the MuRel cell, an atomic reasoning primitive representing interactions between question and image regions by a rich vectorial representation, and modeling region relations with pairwise combinations. Secondly, we incorporate the cell into a full MuRel network, which progressively refines visual and question interactions, and can be leveraged to define visualization schemes finer than mere attention maps.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\AS3SL3YG\\Cadene 等。 - 2019 - MUREL Multimodal Relational Reasoning for Visual .pdf}
}

@inproceedings{cadeneRUBiReducingUnimodal2019,
  title = {{{RUBi}}: {{Reducing Unimodal Biases}} for {{Visual Question Answering}}},
  shorttitle = {{{RUBi}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cadene, Remi and Dancette, Corentin and Ben younes, Hedi and Cord, Matthieu and Parikh, Devi},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2019/hash/51d92be1c60d1db1d2e5e7a07da55b26-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings.},
  file = {D\:\\Zotero\\storage\\93FIUYAZ\\Cadene et al_2019_RUBi.pdf}
}

@inproceedings{cadeneRUBiReducingUnimodal2019a,
  title = {{{RUBi}}: {{Reducing Unimodal Biases}} for {{Visual Question Answering}}},
  shorttitle = {{{RUBi}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cadene, Remi and Dancette, Corentin and Ben younes, Hedi and Cord, Matthieu and Parikh, Devi},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2019/hash/51d92be1c60d1db1d2e5e7a07da55b26-Abstract.html},
  urldate = {2021-09-10},
  keywords = {l Bias},
  file = {D\:\\Zotero\\storage\\UPR2DSGF\\Cadene et al_2019_RUBi.pdf}
}

@misc{Cai2022b,
  title = {Stabilized Exponential Time Differencing Schemes for the Convective {{Allen-Cahn}} Equation},
  author = {Cai, Yongyong and Ju, Lili and Lan, Rihui and Li, Jingwei},
  date = {2022-10-14},
  number = {arXiv:2210.07827},
  eprint = {2210.07827},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.07827},
  urldate = {2022-10-17},
  abstract = {The convective Allen-Cahn equation has been widely used to simulate multi-phase flows in many phase-field models. As a generalized form of the classic Allen-Cahn equation, the convective Allen-Cahn equation still preserves the maximum bound principle (MBP) in the sense that the time-dependent solution of the equation with appropriate initial and boundary conditions preserves for all time a uniform pointwise bound in absolute value. In this paper, we develop efficient first- and second-order exponential time differencing (ETD) schemes combined with the linear stabilizing technique to preserve the MBP unconditionally in the discrete setting. The space discretization is done using the upwind difference scheme for the convective term and the central difference scheme for the diffusion term, and both the mobility and nonlinear terms are approximated through the linear convex interpolation. The unconditional preservation of the MBP of the proposed schemes is proven, and their convergence analysis is presented. Various numerical experiments in two and three dimensions are also carried out to verify the theoretical results.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {D\:\\Notes\\08-Assets\\pdfs\\2022-Stabilized exponential time differencing schemes for the convective Allen-Cahn.pdf;D\:\\Zotero\\storage\\8VNRC9QL\\2210.html}
}

@unpublished{caiGraphTransformerGraphtoSequence2019,
  title = {Graph {{Transformer}} for {{Graph-to-Sequence Learning}}},
  author = {Cai, Deng and Lam, Wai},
  date = {2019-11-30},
  eprint = {1911.07470},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.07470},
  urldate = {2022-03-26},
  abstract = {The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\QXPUPTEC\\Cai_Lam_2019_Graph Transformer for Graph-to-Sequence Learning.pdf;D\:\\Zotero\\storage\\TAV5ECUK\\1911.html}
}

@unpublished{caiGroupwiseContrastiveLearning2020,
  title = {Group-Wise {{Contrastive Learning}} for {{Neural Dialogue Generation}}},
  author = {Cai, Hengyi and Chen, Hongshen and Song, Yonghao and Ding, Zhuoye and Bao, Yongjun and Yan, Weipeng and Zhao, Xiaofang},
  date = {2020-10-13},
  eprint = {2009.07543},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2009.07543},
  urldate = {2021-10-12},
  abstract = {Neural dialogue response generation has gained much popularity in recent years. Maximum Likelihood Estimation (MLE) objective is widely adopted in existing dialogue model learning. However, models trained with MLE objective function are plagued by the low-diversity issue when it comes to the open-domain conversational setting. Inspired by the observation that humans not only learn from the positive signals but also benefit from correcting behaviors of undesirable actions, in this work, we introduce contrastive learning into dialogue generation, where the model explicitly perceives the difference between the well-chosen positive and negative utterances. Specifically, we employ a pretrained baseline model as a reference. During contrastive learning, the target dialogue model is trained to give higher conditional probabilities for the positive samples, and lower conditional probabilities for those negative samples, compared to the reference model. To manage the multi-mapping relations prevailed in human conversation, we augment contrastive dialogue learning with group-wise dual sampling. Extensive experimental results show that the proposed group-wise contrastive learning framework is suited for training a wide range of neural dialogue generation models with very favorable performance over the baseline training approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\4F3623L7\\Cai et al_2020_Group-wise Contrastive Learning for Neural Dialogue Generation.pdf;D\:\\Zotero\\storage\\GPP63YKW\\2009.html}
}

@unpublished{caoExplainableHighorderVisual2019,
  title = {Explainable {{High-order Visual Question Reasoning}}: {{A New Benchmark}} and {{Knowledge-routed Network}}},
  shorttitle = {Explainable {{High-order Visual Question Reasoning}}},
  author = {Cao, Qingxing and Li, Bailin and Liang, Xiaodan and Lin, Liang},
  date = {2019-09-22},
  eprint = {1909.10128},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1909.10128},
  urldate = {2021-10-08},
  abstract = {Explanation and high-order reasoning capabilities are crucial for real-world visual question answering with diverse levels of inference complexity (e.g., what is the dog that is near the girl playing with?) and important for users to understand and diagnose the trustworthiness of the system. Current VQA benchmarks on natural images with only an accuracy metric end up pushing the models to exploit the dataset biases and cannot provide any interpretable justification, which severally hinders advances in high-level question answering. In this work, we propose a new HVQR benchmark for evaluating explainable and high-order visual question reasoning ability with three distinguishable merits: 1) the questions often contain one or two relationship triplets, which requires the model to have the ability of multistep reasoning to predict plausible answers; 2) we provide an explicit evaluation on a multistep reasoning process that is constructed with image scene graphs and commonsense knowledge bases; and 3) each relationship triplet in a large-scale knowledge base only appears once among all questions, which poses challenges for existing networks that often attempt to overfit the knowledge base that already appears in the training set and enforces the models to handle unseen questions and knowledge fact usage. We also propose a new knowledge-routed modular network (KM-net) that incorporates the multistep reasoning process over a large knowledge base into visual question reasoning. An extensive dataset analysis and comparisons with existing models on the HVQR benchmark show that our benchmark provides explainable evaluations, comprehensive reasoning requirements and realistic challenges of VQA systems, as well as our KM-net's superiority in terms of accuracy and explanation ability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\PCXTHHRH\\Cao et al_2019_Explainable High-order Visual Question Reasoning.pdf;D\:\\Zotero\\storage\\JUN489CQ\\1909.html}
}

@unpublished{caoKnowledgeRoutedVisualQuestion2020,
  title = {Knowledge-{{Routed Visual Question Reasoning}}: {{Challenges}} for {{Deep Representation Embedding}}},
  shorttitle = {Knowledge-{{Routed Visual Question Reasoning}}},
  author = {Cao, Qingxing and Li, Bailin and Liang, Xiaodan and Wang, Keze and Lin, Liang},
  date = {2020-12-13},
  eprint = {2012.07192},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.07192},
  urldate = {2021-10-08},
  abstract = {Though beneficial for encouraging the Visual Question Answering (VQA) models to discover the underlying knowledge by exploiting the input-output correlation beyond image and text contexts, the existing knowledge VQA datasets are mostly annotated in a crowdsource way, e.g., collecting questions and external reasons from different users via the internet. In addition to the challenge of knowledge reasoning, how to deal with the annotator bias also remains unsolved, which often leads to superficial over-fitted correlations between questions and answers. To address this issue, we propose a novel dataset named Knowledge-Routed Visual Question Reasoning for VQA model evaluation. Considering that a desirable VQA model should correctly perceive the image context, understand the question, and incorporate its learned knowledge, our proposed dataset aims to cutoff the shortcut learning exploited by the current deep embedding models and push the research boundary of the knowledge-based visual question reasoning. Specifically, we generate the question-answer pair based on both the Visual Genome scene graph and an external knowledge base with controlled programs to disentangle the knowledge from other biases. The programs can select one or two triplets from the scene graph or knowledge base to push multi-step reasoning, avoid answer ambiguity, and balanced the answer distribution. In contrast to the existing VQA datasets, we further imply the following two major constraints on the programs to incorporate knowledge reasoning: i) multiple knowledge triplets can be related to the question, but only one knowledge relates to the image object. This can enforce the VQA model to correctly perceive the image instead of guessing the knowledge based on the given question solely; ii) all questions are based on different knowledge, but the candidate answers are the same for both the training and test sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\G5RWAU7Y\\Cao et al_2020_Knowledge-Routed Visual Question Reasoning.pdf;D\:\\Zotero\\storage\\MKYQP6R9\\2012.html}
}

@unpublished{caoLinguisticallyDrivenGraph2020,
  title = {Linguistically {{Driven Graph Capsule Network}} for {{Visual Question Reasoning}}},
  author = {Cao, Qingxing and Liang, Xiaodan and Wang, Keze and Lin, Liang},
  date = {2020-03-22},
  eprint = {2003.10065},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.10065},
  urldate = {2021-09-26},
  abstract = {Recently, studies of visual question answering have explored various architectures of end-to-end networks and achieved promising results on both natural and synthetic datasets, which require explicitly compositional reasoning. However, it has been argued that these black-box approaches lack interpretability of results, and thus cannot perform well on generalization tasks due to overfitting the dataset bias. In this work, we aim to combine the benefits of both sides and overcome their limitations to achieve an end-to-end interpretable structural reasoning for general images without the requirement of layout annotations. Inspired by the property of a capsule network that can carve a tree structure inside a regular convolutional neural network (CNN), we propose a hierarchical compositional reasoning model called the "Linguistically driven Graph Capsule Network", where the compositional process is guided by the linguistic parse tree. Specifically, we bind each capsule in the lowest layer to bridge the linguistic embedding of a single word in the original question with visual evidence and then route them to the same capsule if they are siblings in the parse tree. This compositional process is achieved by performing inference on a linguistically driven conditional random field (CRF) and is performed across multiple graph capsule layers, which results in a compositional reasoning process inside a CNN. Experiments on the CLEVR dataset, CLEVR compositional generation test, and FigureQA dataset demonstrate the effectiveness and composition generalization ability of our end-to-end model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\8KRL89RD\\Cao et al_2020_Linguistically Driven Graph Capsule Network for Visual Question Reasoning.pdf;D\:\\Zotero\\storage\\RQNSY2PM\\2003.html}
}

@article{caoLinguisticallyRoutingCapsule,
  title = {Linguistically {{Routing Capsule Network}} for {{Out-of-Distribution Visual Question Answering}}},
  author = {Cao, Qingxing and Wan, Wentao and Wang, Keze and Liang, Xiaodan and Lin, Liang},
  pages = {10},
  abstract = {Generalization on out-of-distribution (OOD) test data is an essential but underexplored topic in visual question answering. Current state-of-the-art VQA models often exploit the biased correlation between data and labels, which results in a large performance drop when the test and training data have different distributions. Inspired by the fact that humans can recognize novel concepts by composing existed concepts and capsule network’s ability of representing part-whole hierarchies, we propose to use capsules to represent parts and introduce “Linguistically Routing” to merge parts with human-prior hierarchies. Specifically, we first fuse visual features with a single question word as atomic parts. Then we introduce the “Linguistically Routing” to reweight the capsule connections between two layers such that: 1) the lower layer capsules can transfer their outputs to the most compatible higher capsules, and 2) two capsules can be merged if their corresponding words are merged in the question parse tree. The routing process maximizes the above unary and binary potentials across multiple layers and finally carves a tree structure inside the capsule network. We evaluate our proposed routing method on the CLEVR compositional generation test, the VQA-CP2 dataset and the VQAv2 dataset. The experimental results show that our proposed method can improve current VQA models on OOD split without losing performance on the indomain test data.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2YRYYRV7\\Cao 等。 - Linguistically Routing Capsule Network for Out-of-.pdf}
}

@article{caoLinguisticallyRoutingCapsulea,
  title = {Linguistically {{Routing Capsule Network}} for {{Out-of-Distribution Visual Question Answering}}},
  author = {Cao, Qingxing and Wan, Wentao and Wang, Keze and Liang, Xiaodan and Lin, Liang},
  pages = {10},
  abstract = {Generalization on out-of-distribution (OOD) test data is an essential but underexplored topic in visual question answering. Current state-of-the-art VQA models often exploit the biased correlation between data and labels, which results in a large performance drop when the test and training data have different distributions. Inspired by the fact that humans can recognize novel concepts by composing existed concepts and capsule network’s ability of representing part-whole hierarchies, we propose to use capsules to represent parts and introduce “Linguistically Routing” to merge parts with human-prior hierarchies. Specifically, we first fuse visual features with a single question word as atomic parts. Then we introduce the “Linguistically Routing” to reweight the capsule connections between two layers such that: 1) the lower layer capsules can transfer their outputs to the most compatible higher capsules, and 2) two capsules can be merged if their corresponding words are merged in the question parse tree. The routing process maximizes the above unary and binary potentials across multiple layers and finally carves a tree structure inside the capsule network. We evaluate our proposed routing method on the CLEVR compositional generation test, the VQA-CP2 dataset and the VQAv2 dataset. The experimental results show that our proposed method can improve current VQA models on OOD split without losing performance on the indomain test data.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2LSDEVZM\\Cao 等。 - Linguistically Routing Capsule Network for Out-of-.pdf}
}

@inproceedings{caoVisualQuestionReasoning2018,
  title = {Visual {{Question Reasoning}} on {{General Dependency Tree}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cao, Qingxing and Liang, Xiaodan and Li, Bailin and Li, Guanbin and Lin, Liang},
  date = {2018-06},
  pages = {7249--7257},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00757},
  url = {https://ieeexplore.ieee.org/document/8578855/},
  urldate = {2021-10-11},
  abstract = {The collaborative reasoning for understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previous mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2E3BQ6E6\\Cao 等。 - 2018 - Visual Question Reasoning on General Dependency Tr.pdf}
}

@inproceedings{caoVisualQuestionReasoning2018a,
  title = {Visual {{Question Reasoning}} on {{General Dependency Tree}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cao, Qingxing and Liang, Xiaodan and Li, Bailin and Li, Guanbin and Lin, Liang},
  date = {2018-06},
  pages = {7249--7257},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00757},
  url = {https://ieeexplore.ieee.org/document/8578855/},
  urldate = {2021-09-09},
  abstract = {The collaborative reasoning for understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previous mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {13 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\ABYU9H22\\Cao 等。 - 2018 - Visual Question Reasoning on General Dependency Tr.pdf}
}

@unpublished{cascante-bonillaSimVQAExploringSimulated2022,
  title = {{{SimVQA}}: {{Exploring Simulated Environments}} for {{Visual Question Answering}}},
  shorttitle = {{{SimVQA}}},
  author = {Cascante-Bonilla, Paola and Wu, Hui and Wang, Letao and Feris, Rogerio and Ordonez, Vicente},
  date = {2022-03-31},
  number = {arXiv:2203.17219},
  eprint = {2203.17219},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.17219},
  urldate = {2022-05-26},
  abstract = {Existing work on VQA explores data augmentation to achieve better generalization by perturbing the images in the dataset or modifying the existing questions and answers. While these methods exhibit good performance, the diversity of the questions and answers are constrained by the available image set. In this work we explore using synthetic computer-generated data to fully control the visual and language space, allowing us to provide more diverse scenarios. We quantify the effect of synthetic data in real-world VQA benchmarks and to which extent it produces results that generalize to real data. By exploiting 3D and physics simulation platforms, we provide a pipeline to generate synthetic data to expand and replace type-specific questions and answers without risking the exposure of sensitive or personal data that might be present in real images. We offer a comprehensive analysis while expanding existing hyper-realistic datasets to be used for VQA. We also propose Feature Swapping (F-SWAP) -- where we randomly switch object-level features during training to make a VQA model more domain invariant. We show that F-SWAP is effective for enhancing a currently existing VQA dataset of real images without compromising on the accuracy to answer existing questions in the dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\QS3KGPDA\\Cascante-Bonilla et al_2022_SimVQA.pdf;D\:\\Zotero\\storage\\PRC7TP98\\2203.html}
}

@unpublished{cascante-bonillaSimVQAExploringSimulated2022a,
  title = {{{SimVQA}}: {{Exploring Simulated Environments}} for {{Visual Question Answering}}},
  shorttitle = {{{SimVQA}}},
  author = {Cascante-Bonilla, Paola and Wu, Hui and Wang, Letao and Feris, Rogerio and Ordonez, Vicente},
  date = {2022-03-31},
  number = {arXiv:2203.17219},
  eprint = {2203.17219},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.17219},
  urldate = {2022-05-26},
  abstract = {Existing work on VQA explores data augmentation to achieve better generalization by perturbing the images in the dataset or modifying the existing questions and answers. While these methods exhibit good performance, the diversity of the questions and answers are constrained by the available image set. In this work we explore using synthetic computer-generated data to fully control the visual and language space, allowing us to provide more diverse scenarios. We quantify the effect of synthetic data in real-world VQA benchmarks and to which extent it produces results that generalize to real data. By exploiting 3D and physics simulation platforms, we provide a pipeline to generate synthetic data to expand and replace type-specific questions and answers without risking the exposure of sensitive or personal data that might be present in real images. We offer a comprehensive analysis while expanding existing hyper-realistic datasets to be used for VQA. We also propose Feature Swapping (F-SWAP) -- where we randomly switch object-level features during training to make a VQA model more domain invariant. We show that F-SWAP is effective for enhancing a currently existing VQA dataset of real images without compromising on the accuracy to answer existing questions in the dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\UBE9WBNH\\Cascante-Bonilla et al_2022_SimVQA.pdf;D\:\\Zotero\\storage\\2MJK68Q7\\2203.html}
}

@online{ChainReasoningVisual,
  title = {Chain of {{Reasoning}} for {{Visual Question Answering}}. - {{AMiner}}},
  url = {https://www.aminer.cn/pub/5c2348ceda562935fc1d5877/chain-of-reasoning-for-visual-question-answering},
  urldate = {2021-10-11},
  file = {D\:\\Zotero\\storage\\6K7HZDQT\\chain-of-reasoning-for-visual-question-answering.html}
}

@article{chaoCrossDatasetAdaptationVisual,
  title = {Cross-{{Dataset Adaptation}} for {{Visual Question Answering}}},
  author = {Chao, Wei-Lun and Hu, Hexiang and Sha, Fei},
  pages = {10},
  abstract = {We investigate the problem of cross-dataset adaptation for visual question answering (Visual QA). Our goal is to train a Visual QA model on a source dataset but apply it to another target one. Analogous to domain adaptation for visual recognition, this setting is appealing when the target dataset does not have a sufficient amount of labeled data to learn an “in-domain” model. The key challenge is that the two datasets are constructed differently, resulting in the cross-dataset mismatch on images, questions, or answers.},
  langid = {english},
  keywords = {Adaptation},
  annotation = {00000},
  file = {D\:\\Zotero\\storage\\5HNX6797\\Chao 等。 - Cross-Dataset Adaptation for Visual Question Answe.pdf}
}

@unpublished{chaplotObjectGoalNavigation2020,
  title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  date = {2020-07-01},
  number = {arXiv:2007.00643},
  eprint = {2007.00643},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2007.00643},
  urldate = {2022-06-02},
  abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {D\:\\Zotero\\storage\\67M2SRYZ\\Chaplot et al_2020_Object Goal Navigation using Goal-Oriented Semantic Exploration.pdf;D\:\\Zotero\\storage\\UE3CZSNV\\2007.html}
}

@article{chappuisPromptRSVQAPromptingVisual,
  title = {Prompt-{{RSVQA}}: {{Prompting Visual Context}} to a {{Language Model}} for {{Remote Sensing Visual Question Answering}}},
  author = {Chappuis, Christel and Zermatten, Valerie and Lobry, Sylvain and Saux, Bertrand Le and Tuia, Devis},
  pages = {10},
  abstract = {Remote sensing visual question answering (RSVQA) was recently proposed with the aim of interfacing natural language and vision to ease the access of information contained in Earth Observation data for a wide audience, which is granted by simple questions in natural language. The traditional vision/language interface is an embedding obtained by fusing features from two deep models, one processing the image and another the question. Despite the success of early VQA models, it remains difficult to control the adequacy of the visual information extracted by its deep model, which should act as a context regularizing the work of the language model. We propose to extract this context information with a visual model, convert it to text and inject it, i.e. prompt it, into a language model. The language model is therefore responsible to process the question with the visual context, and extract features, which are useful to find the answer. We study the effect of prompting with respect to a black-box visual extractor and discuss the importance of training a visual model producing accurate context.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\WUCV339P\\Chappuis 等。 - Prompt-RSVQA Prompting Visual Context to a Langua.pdf}
}

@inproceedings{chaudhryLEAFQALocateEncode2020,
  title = {{{LEAF-QA}}: {{Locate}}, {{Encode}} \& {{Attend}} for {{Figure Question Answering}}},
  shorttitle = {{{LEAF-QA}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Chaudhry, Ritwick and Shekhar, Sumit and Gupta, Utkarsh and Maneriker, Pranav and Bansal, Prann and Joshi, Ajay},
  date = {2020-03},
  pages = {3501--3510},
  publisher = {{IEEE}},
  location = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093269},
  url = {https://ieeexplore.ieee.org/document/9093269/},
  urldate = {2021-09-10},
  eventtitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-72816-553-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SRQL956S\\Chaudhry 等。 - 2020 - LEAF-QA Locate, Encode & Attend for Figure Questi.pdf}
}

@article{chavezDistributedDeepQLearning,
  title = {Distributed {{Deep Q-Learning}}},
  author = {Chavez, Kevin and Ong, Hao Yi and Hong, Augustus},
  pages = {8},
  abstract = {We propose a distributed deep learning model to successfully learn control policies directly from highdimensional sensory input using reinforcement learning. The model is based on the deep Q-network, a convolutional neural network trained with a variant of Q-learning. Its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state. To distribute the deep Q-network training, we adapt the DistBelief software framework to the context of efficiently training reinforcement learning agents. As a result, the method is completely asynchronous and scales well with the number of machines. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to achieve reasonable success on a simple game with minimal parameter tuning.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\YMKTYP7B\\Chavez 等。 - Distributed Deep Q-Learning.pdf}
}

@inproceedings{chenBridgingGapPrior2020,
  title = {Bridging the {{Gap}} between {{Prior}} and {{Posterior Knowledge Selection}} for {{Knowledge-Grounded Dialogue Generation}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Chen, Xiuyi and Meng, Fandong and Li, Peng and Chen, Feilong and Xu, Shuang and Xu, Bo and Zhou, Jie},
  date = {2020-11},
  pages = {3426--3437},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.275},
  url = {https://aclanthology.org/2020.emnlp-main.275},
  urldate = {2021-09-15},
  abstract = {Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.},
  eventtitle = {{{EMNLP}} 2020},
  file = {D\:\\Zotero\\storage\\6ZG94IIL\\Chen et al_2020_Bridging the Gap between Prior and Posterior Knowledge Selection for.pdf}
}

@misc{chenCausalInterventionSubjectDeconfounded2022,
  title = {Causal {{Intervention}} for {{Subject-Deconfounded Facial Action Unit Recognition}}},
  author = {Chen, Yingjie and Chen, Diqi and Wang, Tao and Wang, Yizhou and Liang, Yun},
  date = {2022-04-17},
  number = {arXiv:2204.07935},
  eprint = {2204.07935},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.07935},
  urldate = {2022-08-25},
  abstract = {Subject-invariant facial action unit (AU) recognition remains challenging for the reason that the data distribution varies among subjects. In this paper, we propose a causal inference framework for subject-invariant facial action unit recognition. To illustrate the causal effect existing in AU recognition task, we formulate the causalities among facial images, subjects, latent AU semantic relations, and estimated AU occurrence probabilities via a structural causal model. By constructing such a causal diagram, we clarify the causal effect among variables and propose a plug-in causal intervention module, CIS, to deconfound the confounder \textbackslash emph\{Subject\} in the causal diagram. Extensive experiments conducted on two commonly used AU benchmark datasets, BP4D and DISFA, show the effectiveness of our CIS, and the model with CIS inserted, CISNet, has achieved state-of-the-art performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Notes\\PaperReading\\Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition.md;D\:\\Zotero\\storage\\I3QLVPQY\\Chen et al_2022_Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition.pdf;D\:\\Zotero\\storage\\7CPGHXFB\\2204.html}
}

@misc{chenContactlessElectrocardiogramMonitoring2021,
  title = {Contactless {{Electrocardiogram Monitoring}} with {{Millimeter Wave Radar}}},
  author = {Chen, Jinbo and Zhang, Dongheng and Wu, Zhi and Zhou, Fang and Sun, Qibin and Chen, Yan},
  date = {2021-12-20},
  number = {arXiv:2112.06639},
  eprint = {2112.06639},
  eprinttype = {arxiv},
  primaryclass = {eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.06639},
  urldate = {2022-07-26},
  abstract = {The electrocardiogram (ECG) has always been an important biomedical test to diagnose cardiovascular diseases. Current approaches for ECG monitoring are based on body attached electrodes leading to uncomfortable user experience. Therefore, contactless ECG monitoring has drawn tremendous attention, which however remains unsolved. In fact, cardiac electrical-mechanical activities are coupling in a well-coordinated pattern. In this paper, we achieve contactless ECG monitoring by breaking the boundary between the cardiac mechanical and electrical activity. Specifically, we develop a millimeter-wave radar system to contactlessly measure cardiac mechanical activity and reconstruct ECG without any contact in. To measure the cardiac mechanical activity comprehensively, we propose a series of signal processing algorithms to extract 4D cardiac motions from radio frequency (RF) signals. Furthermore, we design a deep neural network to solve the cardiac related domain transformation problem and achieve end-to-end reconstruction mapping from RF input to the ECG output. The experimental results show that our contactless ECG measurements achieve timing accuracy of cardiac electrical events with median error below 14ms and morphology accuracy with median Pearson-Correlation of 90\% and median Root-Mean-Square-Error of 0.081mv compared to the groudtruth ECG. These results indicate that the system enables the potential of contactless, continuous and accurate ECG monitoring.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing},
  file = {D\:\\Zotero\\storage\\J36YUIF7\\Chen et al_2021_Contactless Electrocardiogram Monitoring with Millimeter Wave Radar.pdf;D\:\\Zotero\\storage\\TKACR4PH\\2112.html}
}

@inproceedings{chenCounterfactualSamplesSynthesizing2020,
  title = {Counterfactual {{Samples Synthesizing}} for {{Robust Visual Question Answering}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Long and Yan, Xin and Xiao, Jun and Zhang, Hanwang and Pu, Shiliang and Zhuang, Yueting},
  date = {2020-06},
  pages = {10797--10806},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01081},
  url = {https://ieeexplore.ieee.org/document/9157377/},
  urldate = {2021-09-09},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {11 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\AFNJ96HG\\Chen 等。 - 2020 - Counterfactual Samples Synthesizing for Robust Vis.pdf}
}

@inproceedings{chenDaDianNaoMachineLearningSupercomputer2014,
  title = {{{DaDianNao}}: {{A Machine-Learning Supercomputer}}},
  shorttitle = {{{DaDianNao}}},
  booktitle = {2014 47th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Chen, Yunji and Luo, Tao and Liu, Shaoli and Zhang, Shijin and He, Liqiang and Wang, Jia and Li, Ling and Chen, Tianshi and Xu, Zhiwei and Sun, Ninghui and Temam, Olivier},
  date = {2014-12},
  pages = {609--622},
  publisher = {{IEEE}},
  location = {{Cambridge, United Kingdom}},
  doi = {10.1109/MICRO.2014.58},
  url = {http://ieeexplore.ieee.org/document/7011421/},
  urldate = {2021-11-24},
  abstract = {Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses.},
  eventtitle = {2014 47th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  isbn = {978-1-4799-6998-2},
  langid = {english}
}

@article{chenDianNaoSmallFootprintHighThroughput,
  title = {{{DianNao}}: {{A Small-Footprint High-Throughput Accelerator}} for {{Ubiquitous Machine-Learning}}},
  author = {Chen, Tianshi and Du, Zidong and Sun, Ninghui},
  pages = {15},
  abstract = {Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machinelearning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\AENM8CJ9\\Chen 等。 - DianNao A Small-Footprint High-Throughput Acceler.pdf}
}

@unpublished{chenDMRMDualchannelMultihop2019,
  title = {{{DMRM}}: {{A Dual-channel Multi-hop Reasoning Model}} for {{Visual Dialog}}},
  shorttitle = {{{DMRM}}},
  author = {Chen, Feilong and Meng, Fandong and Xu, Jiaming and Li, Peng and Xu, Bo and Zhou, Jie},
  date = {2019-12-17},
  eprint = {1912.08360},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1912.08360},
  urldate = {2021-12-08},
  abstract = {Visual Dialog is a vision-language task that requires an AI agent to engage in a conversation with humans grounded in an image. It remains a challenging task since it requires the agent to fully understand a given question before making an appropriate response not only from the textual dialog history, but also from the visually-grounded information. While previous models typically leverage single-hop reasoning or single-channel reasoning to deal with this complex multimodal reasoning task, which is intuitively insufficient. In this paper, we thus propose a novel and more powerful Dual-channel Multi-hop Reasoning Model for Visual Dialog, named DMRM. DMRM synchronously captures information from the dialog history and the image to enrich the semantic representation of the question by exploiting dual-channel reasoning. Specifically, DMRM maintains a dual channel to obtain the question- and history-aware image features and the question- and image-aware dialog history features by a mulit-hop reasoning process in each channel. Additionally, we also design an effective multimodal attention to further enhance the decoder to generate more accurate responses. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that the proposed model is effective and outperforms compared models by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\ZQXQLC4G\\Chen et al_2019_DMRM.pdf;D\:\\Zotero\\storage\\QM3K8THF\\1912.html}
}

@unpublished{chenDMRMDualchannelMultihop2019a,
  title = {{{DMRM}}: {{A Dual-channel Multi-hop Reasoning Model}} for {{Visual Dialog}}},
  shorttitle = {{{DMRM}}},
  author = {Chen, Feilong and Meng, Fandong and Xu, Jiaming and Li, Peng and Xu, Bo and Zhou, Jie},
  date = {2019-12-17},
  eprint = {1912.08360},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1912.08360},
  urldate = {2021-09-15},
  abstract = {Visual Dialog is a vision-language task that requires an AI agent to engage in a conversation with humans grounded in an image. It remains a challenging task since it requires the agent to fully understand a given question before making an appropriate response not only from the textual dialog history, but also from the visually-grounded information. While previous models typically leverage single-hop reasoning or single-channel reasoning to deal with this complex multimodal reasoning task, which is intuitively insufficient. In this paper, we thus propose a novel and more powerful Dual-channel Multi-hop Reasoning Model for Visual Dialog, named DMRM. DMRM synchronously captures information from the dialog history and the image to enrich the semantic representation of the question by exploiting dual-channel reasoning. Specifically, DMRM maintains a dual channel to obtain the question- and history-aware image features and the question- and image-aware dialog history features by a mulit-hop reasoning process in each channel. Additionally, we also design an effective multimodal attention to further enhance the decoder to generate more accurate responses. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that the proposed model is effective and outperforms compared models by a significant margin.},
  archiveprefix = {arXiv},
  version = {1},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\NBUIIS9K\\Chen et al_2019_DMRM.pdf;D\:\\Zotero\\storage\\XCDUFGPN\\1912.html}
}

@article{chenFineGrainedVideoTextRetrieval,
  title = {Fine-{{Grained Video-Text Retrieval With Hierarchical Graph Reasoning}}},
  author = {Chen, Shizhe and Zhao, Yida and Jin, Qin and Wu, Qi},
  pages = {10},
  abstract = {Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach is to learn a joint embedding space to measure cross-modal similarities. However, simple embeddings are insufficient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve fine-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. The model disentangles text into a hierarchical semantic graph including three levels of events, actions, entities, and generates hierarchical textual embeddings via attention-based graph reasoning. Different levels of texts can guide the learning of diverse and hierarchical video representations for cross-modal matching to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish fine-grained semantic differences. Code will be released at https: //github.com/cshizhe/hgr\_v2t.},
  langid = {english}
}

@unpublished{chenFinegrainedVideoTextRetrieval2020,
  title = {Fine-Grained {{Video-Text Retrieval}} with {{Hierarchical Graph Reasoning}}},
  author = {Chen, Shizhe and Zhao, Yida and Jin, Qin and Wu, Qi},
  date = {2020-02-29},
  eprint = {2003.00392},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.00392},
  urldate = {2021-10-10},
  abstract = {Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach for this problem is to learn a joint embedding space to measure cross-modal similarities. However, simple joint embeddings are insufficient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve fine-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. To be specific, the model disentangles texts into hierarchical semantic graph including three levels of events, actions, entities and relationships across levels. Attention-based graph reasoning is utilized to generate hierarchical textual embeddings, which can guide the learning of diverse and hierarchical video representations. The HGR model aggregates matchings from different video-text levels to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish fine-grained semantic differences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\V5GUCL2N\\Chen et al_2020_Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning.pdf;D\:\\Zotero\\storage\\HZGLL8EY\\2003.html}
}

@inproceedings{chenFineGrainedVideoTextRetrieval2020,
  title = {Fine-{{Grained Video-Text Retrieval With Hierarchical Graph Reasoning}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Shizhe and Zhao, Yida and Jin, Qin and Wu, Qi},
  date = {2020-06},
  pages = {10635--10644},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01065},
  url = {https://ieeexplore.ieee.org/document/9156961/},
  urldate = {2022-03-15},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5}
}

@unpublished{chengMultiPerspectiveInferrerReasoning2019,
  title = {Multi-{{Perspective Inferrer}}: {{Reasoning Sentences Relationship}} from {{Holistic Perspective}}},
  shorttitle = {Multi-{{Perspective Inferrer}}},
  author = {Cheng, Zhen and Zheng, Zaixiang and Dai, Xin-Yu and Huang, Shujian and Chen, Jiajun},
  date = {2019-11-09},
  eprint = {1911.03668},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.03668},
  urldate = {2021-10-11},
  abstract = {Natural Language Inference (NLI) aims to determine the logic relationships (i.e., entailment, neutral and contradiction) between a pair of premise and hypothesis. Recently, the alignment mechanism effectively helps NLI by capturing the aligned parts (i.e., the similar segments) in the sentence pairs, which imply the perspective of entailment and contradiction. However, these aligned parts will sometimes mislead the judgment of neutral relations. Intuitively, NLI should rely more on multiple perspectives to form a holistic view to eliminate bias. In this paper, we propose the Multi-Perspective Inferrer (MPI), a novel NLI model that reasons relationships from multiple perspectives associated with the three relationships. The MPI determines the perspectives of different parts of the sentences via a routing-by-agreement policy and makes the final decision from a holistic view. Additionally, we introduce an auxiliary supervised signal to ensure the MPI to learn the expected perspectives. Experiments on SNLI and MultiNLI show that 1) the MPI achieves substantial improvements on the base model, which verifies the motivation of multi-perspective inference; 2) visualized evidence verifies that the MPI learns highly interpretable perspectives as expected; 3) more importantly, the MPI is architecture-free and compatible with the powerful BERT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\2NG83TQA\\Cheng 等。 - 2019 - Multi-Perspective Inferrer Reasoning Sentences Re.pdf;D\:\\Zotero\\storage\\3LRDQWYF\\1911.html}
}

@inproceedings{chenGoGRelationawareGraphoverGraph2021,
  title = {{{GoG}}: {{Relation-aware Graph-over-Graph Network}} for {{Visual Dialog}}},
  shorttitle = {{{GoG}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Chen, Feilong and Chen, Xiuyi and Meng, Fandong and Li, Peng and Zhou, Jie},
  date = {2021-08},
  pages = {230--243},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.20},
  url = {https://aclanthology.org/2021.findings-acl.20},
  urldate = {2021-12-08},
  eventtitle = {Findings 2021},
  file = {D\:\\Zotero\\storage\\62SIXAW9\\Chen et al_2021_GoG.pdf}
}

@inproceedings{chenGoGRelationawareGraphoverGraph2021a,
  title = {{{GoG}}: {{Relation-aware Graph-over-Graph Network}} for {{Visual Dialog}}},
  shorttitle = {{{GoG}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Chen, Feilong and Chen, Xiuyi and Meng, Fandong and Li, Peng and Zhou, Jie},
  date = {2021-08},
  pages = {230--243},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.20},
  url = {https://aclanthology.org/2021.findings-acl.20},
  urldate = {2021-09-15},
  eventtitle = {Findings 2021},
  file = {D\:\\Zotero\\storage\\TRBCJMKS\\Chen et al_2021_GoG.pdf}
}

@inproceedings{chenGoGRelationawareGraphoverGraph2021b,
  title = {{{GoG}}: {{Relation-aware Graph-over-Graph Network}} for {{Visual Dialog}}},
  shorttitle = {{{GoG}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Chen, Feilong and Chen, Xiuyi and Meng, Fandong and Li, Peng and Zhou, Jie},
  date = {2021-08},
  pages = {230--243},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.20},
  url = {https://aclanthology.org/2021.findings-acl.20},
  urldate = {2021-09-15},
  eventtitle = {Findings 2021},
  file = {D\:\\Zotero\\storage\\9DZFTTHS\\Chen et al_2021_GoG.pdf}
}

@unpublished{chenImprovingCrossModalUnderstanding2022,
  title = {Improving {{Cross-Modal Understanding}} in {{Visual Dialog}} via {{Contrastive Learning}}},
  author = {Chen, Feilong and Chen, Xiuyi and Xu, Shuang and Xu, Bo},
  date = {2022-04-14},
  eprint = {2204.07302},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2204.07302},
  urldate = {2022-05-05},
  abstract = {Visual Dialog is a challenging vision-language task since the visual dialog agent needs to answer a series of questions after reasoning over both the image content and dialog history. Though existing methods try to deal with the cross-modal understanding in visual dialog, they are still not enough in ranking candidate answers based on their understanding of visual and textual contexts. In this paper, we analyze the cross-modal understanding in visual dialog based on the vision-language pre-training model VD-BERT and propose a novel approach to improve the cross-modal understanding for visual dialog, named ICMU. ICMU enhances cross-modal understanding by distinguishing different pulled inputs (i.e. pulled images, questions or answers) based on four-way contrastive learning. In addition, ICMU exploits the single-turn visual question answering to enhance the visual dialog model's cross-modal understanding to handle a multi-turn visually-grounded conversation. Experiments show that the proposed approach improves the visual dialog model's cross-modal understanding and brings satisfactory gain to the VisDial dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\CDC5X3B3\\Chen et al_2022_Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning.pdf;D\:\\Zotero\\storage\\8PM4X82B\\2204.html}
}

@unpublished{chenLearningGroundVisual2021,
  title = {Learning to {{Ground Visual Objects}} for {{Visual Dialog}}},
  author = {Chen, Feilong and Chen, Xiuyi and Xu, Can and Jiang, Daxin},
  date = {2021-09-13},
  eprint = {2109.06013},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.06013},
  urldate = {2021-12-08},
  abstract = {Visual dialog is challenging since it needs to answer a series of coherent questions based on understanding the visual environment. How to ground related visual objects is one of the key problems. Previous studies utilize the question and history to attend to the image and achieve satisfactory performance, however these methods are not sufficient to locate related visual objects without any guidance. The inappropriate grounding of visual objects prohibits the performance of visual dialog models. In this paper, we propose a novel approach to Learn to Ground visual objects for visual dialog, which employs a novel visual objects grounding mechanism where both prior and posterior distributions over visual objects are used to facilitate visual objects grounding. Specifically, a posterior distribution over visual objects is inferred from both context (history and questions) and answers, and it ensures the appropriate grounding of visual objects during the training process. Meanwhile, a prior distribution, which is inferred from context only, is used to approximate the posterior distribution so that appropriate visual objects can be grounded even without answers during the inference process. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our approach improves the previous strong models in both generative and discriminative settings by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\EBBU5W9Q\\Chen et al_2021_Learning to Ground Visual Objects for Visual Dialog.pdf;D\:\\Zotero\\storage\\77VBLXTA\\2109.html}
}

@unpublished{chenLearningGroundVisual2021a,
  title = {Learning to {{Ground Visual Objects}} for {{Visual Dialog}}},
  author = {Chen, Feilong and Chen, Xiuyi and Xu, Can and Jiang, Daxin},
  date = {2021-09-13},
  eprint = {2109.06013},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.06013},
  urldate = {2021-09-16},
  abstract = {Visual dialog is challenging since it needs to answer a series of coherent questions based on understanding the visual environment. How to ground related visual objects is one of the key problems. Previous studies utilize the question and history to attend to the image and achieve satisfactory performance, however these methods are not sufficient to locate related visual objects without any guidance. The inappropriate grounding of visual objects prohibits the performance of visual dialog models. In this paper, we propose a novel approach to Learn to Ground visual objects for visual dialog, which employs a novel visual objects grounding mechanism where both prior and posterior distributions over visual objects are used to facilitate visual objects grounding. Specifically, a posterior distribution over visual objects is inferred from both context (history and questions) and answers, and it ensures the appropriate grounding of visual objects during the training process. Meanwhile, a prior distribution, which is inferred from context only, is used to approximate the posterior distribution so that appropriate visual objects can be grounded even without answers during the inference process. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our approach improves the previous strong models in both generative and discriminative settings by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\GQ57ZRSG\\Chen et al_2021_Learning to Ground Visual Objects for Visual Dialog.pdf;D\:\\Zotero\\storage\\W2IJNQRH\\2109.html}
}

@inproceedings{chenLeveragingHumanAttention2021,
  title = {Leveraging {{Human Attention}} in {{Novel Object Captioning}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Chen, Xianyu and Jiang, Ming and Zhao, Qi},
  date = {2021-08},
  pages = {622--628},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Montreal, Canada}},
  doi = {10.24963/ijcai.2021/86},
  url = {https://www.ijcai.org/proceedings/2021/86},
  urldate = {2021-09-23},
  abstract = {Image captioning models depend on training with paired image-text corpora, which poses various challenges in describing images containing novel objects absent from the training data. While previous novel object captioning methods rely on external image taggers or object detectors to describe novel objects, we present the Attentionbased Novel Object Captioner (ANOC) that complements novel object captioners with human attention features that characterize generally important information independent of tasks. It introduces a gating mechanism that adaptively incorporates human attention with self-learned machine attention, with a Constrained Self-Critical Sequence Training method to address the exposure bias while maintaining constraints of novel object descriptions. Extensive experiments conducted on the nocaps and Held-Out COCO datasets demonstrate that our method considerably outperforms the stateof-the-art novel object captioners. Our source code is available at https://github.com/chenxy99/ANOC.},
  eventtitle = {Thirtieth {{International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-21}}\vphantom\{\}},
  isbn = {978-0-9992411-9-6},
  langid = {english}
}

@inproceedings{chenMultimodalIncrementalTransformer2021,
  title = {Multimodal {{Incremental Transformer}} with {{Visual Grounding}} for {{Visual Dialogue Generation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Chen, Feilong and Meng, Fandong and Chen, Xiuyi and Li, Peng and Zhou, Jie},
  date = {2021-08},
  pages = {436--446},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.38},
  url = {https://aclanthology.org/2021.findings-acl.38},
  urldate = {2021-12-08},
  eventtitle = {Findings 2021},
  file = {D\:\\Zotero\\storage\\PBQCXJVM\\Chen et al_2021_Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue.pdf}
}

@unpublished{chenMultimodalIncrementalTransformer2021a,
  title = {Multimodal {{Incremental Transformer}} with {{Visual Grounding}} for {{Visual Dialogue Generation}}},
  author = {Chen, Feilong and Meng, Fandong and Chen, Xiuyi and Li, Peng and Zhou, Jie},
  date = {2021-09-17},
  eprint = {2109.08478},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.08478},
  urldate = {2021-09-24},
  abstract = {Visual dialogue is a challenging task since it needs to answer a series of coherent questions on the basis of understanding the visual environment. Previous studies focus on the implicit exploration of multimodal co-reference by implicitly attending to spatial image features or object-level image features but neglect the importance of locating the objects explicitly in the visual content, which is associated with entities in the textual content. Therefore, in this paper we propose a \{\textbackslash bf M\}ultimodal \{\textbackslash bf I\}ncremental \{\textbackslash bf T\}ransformer with \{\textbackslash bf V\}isual \{\textbackslash bf G\}rounding, named MITVG, which consists of two key parts: visual grounding and multimodal incremental transformer. Visual grounding aims to explicitly locate related objects in the image guided by textual entities, which helps the model exclude the visual content that does not need attention. On the basis of visual grounding, the multimodal incremental transformer encodes the multi-turn dialogue history combined with visual scene step by step according to the order of the dialogue and then generates a contextually and visually coherent response. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the proposed model, which achieves comparable performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\NGT8NUGS\\Chen et al_2021_Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue.pdf;D\:\\Zotero\\storage\\IWF5699N\\2109.html}
}

@misc{chenPaLIJointlyScaledMultilingual2022,
  title = {{{PaLI}}: {{A Jointly-Scaled Multilingual Language-Image Model}}},
  shorttitle = {{{PaLI}}},
  author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, A. J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish and Bradbury, James and Kuo, Weicheng and Seyedhosseini, Mojtaba and Jia, Chao and Ayan, Burcu Karagol and Riquelme, Carlos and Steiner, Andreas and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu},
  date = {2022-09-16},
  number = {arXiv:2209.06794},
  eprint = {2209.06794},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.06794},
  urldate = {2022-10-13},
  abstract = {Effective scaling and a flexible task interface enable large language models to excel at many tasks. PaLI (Pathways Language and Image model) extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.},
  archiveprefix = {arXiv},
  version = {2},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\2KN9RE25\\Chen et al_2022_PaLI.pdf;D\:\\Zotero\\storage\\NKW32V8Z\\2209.html}
}

@article{chenPredictingHumanScanpaths,
  title = {Predicting {{Human Scanpaths}} in {{Visual Question Answering}}},
  author = {Chen, Xianyu and Jiang, Ming and Zhao, Qi},
  pages = {10},
  abstract = {Attention has been an important mechanism for both humans and computer vision systems. While state-of-theart models to predict attention focus on estimating a static probabilistic saliency map with free-viewing behavior, reallife scenarios are filled with tasks of varying types and complexities, and visual exploration is a temporal process that contributes to task performance. To bridge the gap, we conduct a first study to understand and predict the temporal sequences of eye fixations (a.k.a. scanpaths) during performing general tasks, and examine how scanpaths affect task performance. We present a new deep reinforcement learning method to predict scanpaths leading to different performances in visual question answering. Conditioned on a task guidance map, the proposed model learns question-specific attention patterns to generate scanpaths. It addresses the exposure bias in scanpath prediction with self-critical sequence training and designs a Consistency-Divergence loss to generate distinguishable scanpaths between correct and incorrect answers. The proposed model not only accurately predicts the spatio-temporal patterns of human behavior in visual question answering, such as fixation position, duration, and order, but also generalizes to free-viewing and visual search tasks, achieving human-level performance in all tasks and significantly outperforming the state of the art.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\JAXECIHJ\\Chen 等。 - Predicting Human Scanpaths in Visual Question Answ.pdf}
}

@unpublished{chenREXReasoningawareGrounded2022,
  title = {{{REX}}: {{Reasoning-aware}} and {{Grounded Explanation}}},
  shorttitle = {{{REX}}},
  author = {Chen, Shi and Zhao, Qi},
  date = {2022-03-11},
  number = {arXiv:2203.06107},
  eprint = {2203.06107},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.06107},
  urldate = {2022-06-03},
  abstract = {Effectiveness and interpretability are two essential properties for trustworthy AI systems. Most recent studies in visual reasoning are dedicated to improving the accuracy of predicted answers, and less attention is paid to explaining the rationales behind the decisions. As a result, they commonly take advantage of spurious biases instead of actually reasoning on the visual-textual data, and have yet developed the capability to explain their decision making by considering key information from both modalities. This paper aims to close the gap from three distinct perspectives: first, we define a new type of multi-modal explanations that explain the decisions by progressively traversing the reasoning process and grounding keywords in the images. We develop a functional program to sequentially execute different reasoning steps and construct a new dataset with 1,040,830 multi-modal explanations. Second, we identify the critical need to tightly couple important components across the visual and textual modalities for explaining the decisions, and propose a novel explanation generation method that explicitly models the pairwise correspondence between words and regions of interest. It improves the visual grounding capability by a considerable margin, resulting in enhanced interpretability and reasoning performance. Finally, with our new data and method, we perform extensive analyses to study the effectiveness of our explanation under different settings, including multi-task learning and transfer learning. Our code and data are available at https://github.com/szzexpoi/rex.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\PTG4VS7Q\\Chen_Zhao_2022_REX.pdf;D\:\\Zotero\\storage\\72VCZQ6P\\2203.html}
}

@unpublished{chenStructureAwareTransformerGraph2022,
  title = {Structure-{{Aware Transformer}} for {{Graph Representation Learning}}},
  author = {Chen, Dexiong and O'Bray, Leslie and Borgwardt, Karsten},
  date = {2022-02-07},
  eprint = {2202.03036},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2202.03036},
  urldate = {2022-03-24},
  abstract = {The Transformer architecture has gained growing attention in graph representation learning recently, as it naturally overcomes several limitations of graph neural networks (GNNs) by avoiding their strict structural inductive biases and instead only encoding the graph structure via positional encoding. Here, we show that the node representations generated by the Transformer with positional encoding do not necessarily capture structural similarity between them. To address this issue, we propose the Structure-Aware Transformer, a class of simple and flexible graph transformers built upon a new self-attention mechanism. This new self-attention incorporates structural information into the original self-attention by extracting a subgraph representation rooted at each node before computing the attention. We propose several methods for automatically generating the subgraph representation and show theoretically that the resulting representations are at least as expressive as the subgraph representations. Empirically, our method achieves state-of-the-art performance on five graph prediction benchmarks. Our structure-aware framework can leverage any existing GNN to extract the subgraph representation, and we show that it systematically improves performance relative to the base GNN model, successfully combining the advantages of GNNs and transformers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\EY6ZS7NV\\Chen 等。 - 2022 - Structure-Aware Transformer for Graph Representati.pdf}
}

@unpublished{chenStructureAwareTransformerGraph2022a,
  title = {Structure-{{Aware Transformer}} for {{Graph Representation Learning}}},
  author = {Chen, Dexiong and O'Bray, Leslie and Borgwardt, Karsten},
  date = {2022-02-07},
  eprint = {2202.03036},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2202.03036},
  urldate = {2022-02-10},
  abstract = {The Transformer architecture has gained growing attention in graph representation learning recently, as it naturally overcomes several limitations of graph neural networks (GNNs) by avoiding their strict structural inductive biases and instead only encoding the graph structure via positional encoding. Here, we show that the node representations generated by the Transformer with positional encoding do not necessarily capture structural similarity between them. To address this issue, we propose the Structure-Aware Transformer, a class of simple and flexible graph transformers built upon a new self-attention mechanism. This new self-attention incorporates structural information into the original self-attention by extracting a subgraph representation rooted at each node before computing the attention. We propose several methods for automatically generating the subgraph representation and show theoretically that the resulting representations are at least as expressive as the subgraph representations. Empirically, our method achieves state-of-the-art performance on five graph prediction benchmarks. Our structure-aware framework can leverage any existing GNN to extract the subgraph representation, and we show that it systematically improves performance relative to the base GNN model, successfully combining the advantages of GNNs and transformers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\VRB6ZT7V\\Chen et al_2022_Structure-Aware Transformer for Graph Representation Learning.pdf;D\:\\Zotero\\storage\\TJ9DS2BS\\2202.html}
}

@unpublished{chenThinkGlobalAct2022,
  title = {Think {{Global}}, {{Act Local}}: {{Dual-scale Graph Transformer}} for {{Vision-and-Language Navigation}}},
  shorttitle = {Think {{Global}}, {{Act Local}}},
  author = {Chen, Shizhe and Guhur, Pierre-Louis and Tapaswi, Makarand and Schmid, Cordelia and Laptev, Ivan},
  date = {2022-02-23},
  eprint = {2202.11742},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.11742},
  urldate = {2022-05-03},
  abstract = {Following language instructions to navigate in unseen environments is a challenging problem for autonomous embodied agents. The agent not only needs to ground languages in visual scenes, but also should explore the environment to reach its target. In this work, we propose a dual-scale graph transformer (DUET) for joint long-term action planning and fine-grained cross-modal understanding. We build a topological map on-the-fly to enable efficient exploration in global action space. To balance the complexity of large action space reasoning and fine-grained language grounding, we dynamically combine a fine-scale encoding over local observations and a coarse-scale encoding on a global map via graph transformers. The proposed approach, DUET, significantly outperforms state-of-the-art methods on goal-oriented vision-and-language navigation (VLN) benchmarks REVERIE and SOON. It also improves the success rate on the fine-grained VLN benchmark R2R.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\5ZAUZ88L\\Chen et al_2022_Think Global, Act Local.pdf;D\:\\Zotero\\storage\\IW7SFAWW\\2202.html}
}

@unpublished{chenThinkGlobalAct2022a,
  title = {Think {{Global}}, {{Act Local}}: {{Dual-scale Graph Transformer}} for {{Vision-and-Language Navigation}}},
  shorttitle = {Think {{Global}}, {{Act Local}}},
  author = {Chen, Shizhe and Guhur, Pierre-Louis and Tapaswi, Makarand and Schmid, Cordelia and Laptev, Ivan},
  date = {2022-02-23},
  eprint = {2202.11742},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.11742},
  urldate = {2022-03-26},
  abstract = {Following language instructions to navigate in unseen environments is a challenging problem for autonomous embodied agents. The agent not only needs to ground languages in visual scenes, but also should explore the environment to reach its target. In this work, we propose a dual-scale graph transformer (DUET) for joint long-term action planning and fine-grained cross-modal understanding. We build a topological map on-the-fly to enable efficient exploration in global action space. To balance the complexity of large action space reasoning and fine-grained language grounding, we dynamically combine a fine-scale encoding over local observations and a coarse-scale encoding on a global map via graph transformers. The proposed approach, DUET, significantly outperforms state-of-the-art methods on goal-oriented vision-and-language navigation (VLN) benchmarks REVERIE and SOON. It also improves the success rate on the fine-grained VLN benchmark R2R.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\FXK33XNL\\Chen et al_2022_Think Global, Act Local.pdf;D\:\\Zotero\\storage\\JCYQDCGY\\2202.html}
}

@unpublished{chenZeroshotVisualQuestion2021,
  title = {Zero-Shot {{Visual Question Answering}} Using {{Knowledge Graph}}},
  author = {Chen, Zhuo and Chen, Jiaoyan and Geng, Yuxia and Pan, Jeff Z. and Yuan, Zonggang and Chen, Huajun},
  date = {2021-07-14},
  eprint = {2107.05348},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.05348},
  urldate = {2021-09-23},
  abstract = {Incorporating external knowledge to Visual Question Answering (VQA) has become a vital practical need. Existing methods mostly adopt pipeline approaches with different components for knowledge matching and extraction, feature learning, etc.However, such pipeline approaches suffer when some component does not perform well, which leads to error propagation and poor overall performance. Furthermore, the majority of existing approaches ignore the answer bias issue -- many answers may have never appeared during training (i.e., unseen answers) in real-word application. To bridge these gaps, in this paper, we propose a Zero-shot VQA algorithm using knowledge graphs and a mask-based learning mechanism for better incorporating external knowledge, and present new answer-based Zero-shot VQA splits for the F-VQA dataset. Experiments show that our method can achieve state-of-the-art performance in Zero-shot VQA with unseen answers, meanwhile dramatically augment existing end-to-end models on the normal F-VQA task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {D\:\\Zotero\\storage\\QTJJABTF\\Chen et al_2021_Zero-shot Visual Question Answering using Knowledge Graph.pdf;D\:\\Zotero\\storage\\8T9IWUGP\\2107.html}
}

@unpublished{chenZeroshotVisualQuestion2021a,
  title = {Zero-Shot {{Visual Question Answering}} Using {{Knowledge Graph}}},
  author = {Chen, Zhuo and Chen, Jiaoyan and Geng, Yuxia and Pan, Jeff Z. and Yuan, Zonggang and Chen, Huajun},
  date = {2021-07-14},
  eprint = {2107.05348},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.05348},
  urldate = {2021-08-26},
  abstract = {Incorporating external knowledge to Visual Question Answering (VQA) has become a vital practical need. Existing methods mostly adopt pipeline approaches with different components for knowledge matching and extraction, feature learning, etc.However, such pipeline approaches suffer when some component does not perform well, which leads to error propagation and poor overall performance. Furthermore, the majority of existing approaches ignore the answer bias issue -- many answers may have never appeared during training (i.e., unseen answers) in real-word application. To bridge these gaps, in this paper, we propose a Zero-shot VQA algorithm using knowledge graphs and a mask-based learning mechanism for better incorporating external knowledge, and present new answer-based Zero-shot VQA splits for the F-VQA dataset. Experiments show that our method can achieve state-of-the-art performance in Zero-shot VQA with unseen answers, meanwhile dramatically augment existing end-to-end models on the normal F-VQA task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {D\:\\Zotero\\storage\\YMPBJYZG\\Chen et al_2021_Zero-shot Visual Question Answering using Knowledge Graph.pdf;D\:\\Zotero\\storage\\KSC6XLYM\\2107.html}
}

@unpublished{chenZeroshotVisualQuestion2021b,
  title = {Zero-Shot {{Visual Question Answering}} Using {{Knowledge Graph}}},
  author = {Chen, Zhuo and Chen, Jiaoyan and Geng, Yuxia and Pan, Jeff Z. and Yuan, Zonggang and Chen, Huajun},
  date = {2021-07-14},
  eprint = {2107.05348},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.05348},
  urldate = {2021-08-26},
  abstract = {Incorporating external knowledge to Visual Question Answering (VQA) has become a vital practical need. Existing methods mostly adopt pipeline approaches with different components for knowledge matching and extraction, feature learning, etc.However, such pipeline approaches suffer when some component does not perform well, which leads to error propagation and poor overall performance. Furthermore, the majority of existing approaches ignore the answer bias issue -- many answers may have never appeared during training (i.e., unseen answers) in real-word application. To bridge these gaps, in this paper, we propose a Zero-shot VQA algorithm using knowledge graphs and a mask-based learning mechanism for better incorporating external knowledge, and present new answer-based Zero-shot VQA splits for the F-VQA dataset. Experiments show that our method can achieve state-of-the-art performance in Zero-shot VQA with unseen answers, meanwhile dramatically augment existing end-to-end models on the normal F-VQA task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {D\:\\Zotero\\storage\\DA6G98KF\\Chen et al_2021_Zero-shot Visual Question Answering using Knowledge Graph.pdf;D\:\\Zotero\\storage\\6R895W6Q\\2107.html}
}

@unpublished{choUnifyingVisionandLanguageTasks2021,
  title = {Unifying {{Vision-and-Language Tasks}} via {{Text Generation}}},
  author = {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
  date = {2021-05-23},
  eprint = {2102.02779},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.02779},
  urldate = {2022-03-24},
  abstract = {Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\REM392J6\\Cho et al_2021_Unifying Vision-and-Language Tasks via Text Generation.pdf;D\:\\Zotero\\storage\\EAYFIK3C\\2102.html}
}

@inproceedings{chouVisualQuestionAnswering2020,
  title = {Visual {{Question Answering}} on 360° {{Images}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Chou, Shih-Han and Chao, Wei-Lun and Lai, Wei-Sheng and Sun, Min and Yang, Ming-Hsuan},
  date = {2020-03},
  pages = {1596--1605},
  publisher = {{IEEE}},
  location = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093452},
  url = {https://ieeexplore.ieee.org/document/9093452/},
  urldate = {2021-09-10},
  eventtitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-72816-553-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\YTTQZXK7\\Chou 等。 - 2020 - Visual Question Answering on 360° Images.pdf}
}

@inproceedings{cogswellDialogDialogData2020,
  title = {Dialog without {{Dialog Data}}: {{Learning Visual Dialog Agents}} from {{VQA Data}}},
  shorttitle = {Dialog without {{Dialog Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cogswell, Michael and Lu, Jiasen and Jain, Rishabh and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  date = {2020},
  volume = {33},
  pages = {19988--19999},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/e7023ba77a45f7e84c5ee8a28dd63585-Abstract.html},
  urldate = {2021-09-10},
  keywords = {Visual Dialog},
  file = {D\:\\Zotero\\storage\\WP7R44X9\\Cogswell et al_2020_Dialog without Dialog Data.pdf}
}

@unpublished{congRelTRRelationTransformer2022,
  title = {{{RelTR}}: {{Relation Transformer}} for {{Scene Graph Generation}}},
  shorttitle = {{{RelTR}}},
  author = {Cong, Yuren and Yang, Michael Ying and Rosenhahn, Bodo},
  date = {2022-01-27},
  eprint = {2201.11460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.11460},
  urldate = {2022-03-26},
  abstract = {Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by DETR, which excels in object detection, we view scene graph generation as a set prediction problem and propose an end-to-end scene graph generation model RelTR which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a fixed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, RelTR is a one-stage method that predicts a set of relationships directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome and Open Images V6 datasets demonstrate the superior performance and fast inference of our model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\8HRPL4F8\\Cong et al_2022_RelTR.pdf;D\:\\Zotero\\storage\\WJUPCDEX\\2201.html}
}

@inproceedings{daiProgressiveContourRegression2021,
  title = {Progressive {{Contour Regression}} for {{Arbitrary-Shape Scene Text Detection}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Dai, Pengwen and Zhang, Sanyi and Zhang, Hua and Cao, Xiaochun},
  date = {2021-06},
  pages = {7389--7398},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00731},
  url = {https://ieeexplore.ieee.org/document/9578545/},
  urldate = {2022-04-02},
  abstract = {State-of-the-art scene text detection methods usually model the text instance with local pixels or components from the bottom-up perspective and, therefore, are sensitive to noises and dependent on the complicated heuristic post-processing especially for arbitrary-shape texts. To relieve these two issues, instead, we propose to progressively evolve the initial text proposal to arbitrarily shaped text contours in a top-down manner. The initial horizontal text proposals are generated by estimating the center and size of texts. To reduce the range of regression, the first stage of the evolution predicts the corner points of oriented text proposals from the initial horizontal ones. In the second stage, the contours of the oriented text proposals are iteratively regressed to arbitrarily shaped ones. In the last iteration of this stage, we rescore the confidence of the final localized text by utilizing the cues from multiple contour points, rather than the single cue from the initial horizontal proposal center that may be out of arbitrary-shape text regions. Moreover, to facilitate the progressive contour evolution, we design a contour information aggregation mechanism to enrich the feature representation on text contours by considering both the circular topology and semantic context. Experiments conducted on CTW1500, Total-Text, ArT, and TD500 have demonstrated that the proposed method especially excels in line-level arbitrary-shape texts. Code is available at https://github.com/dpengwen/PCR.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\UC3X6YCM\\Dai 等。 - 2021 - Progressive Contour Regression for Arbitrary-Shape.pdf}
}

@unpublished{damodaranUnderstandingRoleScene2021,
  title = {Understanding the {{Role}} of {{Scene Graphs}} in {{Visual Question Answering}}},
  author = {Damodaran, Vinay and Chakravarthy, Sharanya and Kumar, Akshay and Umapathy, Anjana and Mitamura, Teruko and Nakashima, Yuta and Garcia, Noa and Chu, Chenhui},
  date = {2021-01-16},
  eprint = {2101.05479},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.05479},
  urldate = {2021-09-26},
  abstract = {Visual Question Answering (VQA) is of tremendous interest to the research community with important applications such as aiding visually impaired users and image-based search. In this work, we explore the use of scene graphs for solving the VQA task. We conduct experiments on the GQA dataset which presents a challenging set of questions requiring counting, compositionality and advanced reasoning capability, and provides scene graphs for a large number of images. We adopt image + question architectures for use with scene graphs, evaluate various scene graph generation techniques for unseen images, propose a training curriculum to leverage human-annotated and auto-generated scene graphs, and build late fusion architectures to learn from multiple image representations. We present a multi-faceted study into the use of scene graphs for VQA, making this work the first of its kind.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\AJIDWZRD\\Damodaran et al_2021_Understanding the Role of Scene Graphs in Visual Question Answering.pdf;D\:\\Zotero\\storage\\FYI33G3N\\2101.html}
}

@article{dasEmbodiedQuestionAnswering,
  title = {Embodied {{Question Answering}}},
  author = {Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  pages = {10},
  abstract = {We present a new AI task – Embodied Question Answering (EmbodiedQA) – where an agent is spawned at a random location in a 3D environment and asked a question (‘What color is the car?’). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question (‘orange’).},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ADAW7H39\\Das 等。 - Embodied Question Answering.pdf}
}

@article{dasEmbodiedQuestionAnsweringa,
  title = {Embodied {{Question Answering}}},
  author = {Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  pages = {10},
  abstract = {We present a new AI task – Embodied Question Answering (EmbodiedQA) – where an agent is spawned at a random location in a 3D environment and asked a question (‘What color is the car?’). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question (‘orange’).},
  langid = {english},
  annotation = {00000},
  file = {D\:\\Zotero\\storage\\XUF5ZZ9T\\Das 等。 - Embodied Question Answering.pdf}
}

@unpublished{dasVisualDialog2017,
  title = {Visual {{Dialog}}},
  author = {Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, José M. F. and Parikh, Devi and Batra, Dhruv},
  date = {2017-08-01},
  eprint = {1611.08669},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.08669},
  urldate = {2021-12-08},
  abstract = {We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on \textasciitilde 120k images from COCO, with a total of \textasciitilde 1.2M dialog question-answer pairs. We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\UEFN5V34\\Das et al_2017_Visual Dialog.pdf;D\:\\Zotero\\storage\\EK7IILCV\\1611.html}
}

@unpublished{dasVisualDialog2017a,
  title = {Visual {{Dialog}}},
  author = {Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, José M. F. and Parikh, Devi and Batra, Dhruv},
  date = {2017-08-01},
  eprint = {1611.08669},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.08669},
  urldate = {2021-10-15},
  abstract = {We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on \textasciitilde 120k images from COCO, with a total of \textasciitilde 1.2M dialog question-answer pairs. We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\CUMF29D8\\Das et al_2017_Visual Dialog.pdf;D\:\\Zotero\\storage\\5X4GXD2P\\1611.html}
}

@online{DATA130006IntroductiontNatural,
  title = {{{DATA130006 Introductiont}} to {{Natural Language Processing}}},
  url = {http://www.sdspeople.fudan.edu.cn/zywei/DATA130006/index.html},
  urldate = {2022-03-24},
  file = {D\:\\Zotero\\storage\\CG97VFLK\\index.html}
}

@online{DATA130006IntroductiontNaturala,
  title = {{{DATA130006 Introductiont}} to {{Natural Language Processing}}},
  url = {http://www.sdspeople.fudan.edu.cn/zywei/DATA130006/index.html},
  urldate = {2022-03-24},
  file = {D\:\\Zotero\\storage\\94JGKM7E\\index.html}
}

@inproceedings{decaoQuestionAnsweringReasoning2019,
  title = {Question {{Answering}} by {{Reasoning Across Documents}} with {{Graph Convolutional Networks}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  date = {2019-06},
  pages = {2306--2317},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1240},
  url = {https://aclanthology.org/N19-1240},
  urldate = {2021-10-10},
  abstract = {Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WikiHop (Welbl et al., 2018).},
  eventtitle = {{{NAACL-HLT}} 2019},
  file = {D\:\\Zotero\\storage\\2QZYML6S\\De Cao et al_2019_Question Answering by Reasoning Across Documents with Graph Convolutional.pdf}
}

@article{dengPositionAwareTransformerImage2022,
  title = {A {{Position-Aware Transformer}} for {{Image Captioning}}},
  author = {Deng, Zelin and Zhou, Bo and He, Pei and Huang, Jianfeng and Alfarraj, Osama and Tolba, Amr},
  date = {2022},
  journaltitle = {Computers, Materials \& Continua},
  volume = {70},
  number = {1},
  pages = {2065--2081},
  issn = {1546-2226},
  doi = {10.32604/cmc.2022.019328},
  url = {https://www.techscience.com/cmc/v70n1/44395},
  urldate = {2022-02-09},
  abstract = {Image captioning aims to generate a corresponding description of an image. In recent years, neural encoder-decoder models have been the dominant approaches, in which the Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) are used to translate an image into a natural language description. Among these approaches, the visual attention mechanisms are widely used to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. However, most conventional visual attention mechanisms are based on high-level image features, ignoring the effects of other image features, and giving insufficient consideration to the relative positions between image features. In this work, we propose a PositionAware Transformer model with image-feature attention and position-aware attention mechanisms for the above problems. The image-feature attention firstly extracts multi-level features by using Feature Pyramid Network (FPN), then utilizes the scaled-dot-product to fuse these features, which enables our model to detect objects of different scales in the image more effectively without increasing parameters. In the position-aware attention mechanism, the relative positions between image features are obtained at first, afterwards the relative positions are incorporated into the original image features to generate captions more accurately. Experiments are carried out on the MSCOCO dataset and our approach achieves competitive BLEU-4, METEOR, ROUGE-L, CIDEr scores compared with some state-of-the-art approaches, demonstrating the effectiveness of our approach.},
  langid = {english}
}

@unpublished{deyExternalKnowledgeEnabled2021,
  title = {External {{Knowledge}} Enabled {{Text Visual Question Answering}}},
  author = {Dey, Arka Ujjal and Valveny, Ernest and Harit, Gaurav},
  date = {2021-10-20},
  eprint = {2108.09717},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.09717},
  urldate = {2021-12-10},
  abstract = {The open-ended question answering task of Text-VQA requires reading and reasoning about local, often previously unseen, scene-text content of an image to generate answers. In this work, we propose the generalized use of external knowledge to augment our understanding of the said scene-text. We design a framework to extract, validate, and reason with knowledge using a standard multimodal transformer for vision language understanding tasks. Through empirical evidence and qualitative results, we demonstrate how external knowledge can highlight instance-only cues and thus help deal with training data bias, improve answer entity type correctness, and detect multiword named entities. We generate results comparable to the state-of-the-art on two publicly available datasets, under the constraints of similar upstream OCR systems and training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\DAHWXVWY\\Dey et al_2021_External Knowledge enabled Text Visual Question Answering.pdf;D\:\\Zotero\\storage\\Z9543B35\\2108.html}
}

@unpublished{deyKTVQAGeneralizedUse2022,
  title = {{{KTVQA}}: {{Generalized}} Use of {{External Knowledge}} to Empower {{Scene Text}} in {{Text-VQA}}},
  shorttitle = {{{KTVQA}}},
  author = {Dey, Arka Ujjal and Valveny, Ernest and Harit, Gaurav},
  date = {2022-01-28},
  eprint = {2108.09717},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.09717},
  urldate = {2022-02-10},
  abstract = {The open-ended question answering task of Text-VQA requires reading and reasoning about local, often previously unseen, scene-text content of an image. We address this zero-shot nature of the problem by proposing the generalized use of external knowledge to augment our understanding of the said scene-text. We design a framework to extract, validate, and reason with knowledge using a standard multimodal transformer for vision language understanding tasks. Through empirical evidence and qualitative results, we demonstrate how external knowledge can highlight instance-only cues and thus help deal with training data bias, improve answer entity type correctness, and detect multiword named entities. We generate results comparable to the state-of-the-art on three publicly available datasets, under the constraints of similar upstream OCR systems and training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\S4YZX9NA\\Dey et al_2022_KTVQA.pdf;D\:\\Zotero\\storage\\4FCCRETX\\2108.html}
}

@article{deyVisualSemanticsExploring2021,
  title = {Beyond Visual Semantics: {{Exploring}} the Role of Scene Text in Image Understanding},
  shorttitle = {Beyond Visual Semantics},
  author = {Dey, Arka Ujjal and Ghosh, Suman K. and Valveny, Ernest and Harit, Gaurav},
  date = {2021-09},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {149},
  pages = {164--171},
  issn = {01678655},
  doi = {10.1016/j.patrec.2021.06.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865521002178},
  urldate = {2022-02-10},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZYCSG3H4\\Dey 等。 - 2021 - Beyond visual semantics Exploring the role of sce.pdf}
}

@unpublished{dharurSOrTingVQAModels2020,
  title = {{{SOrT-ing VQA Models}} : {{Contrastive Gradient Learning}} for {{Improved Consistency}}},
  shorttitle = {{{SOrT-ing VQA Models}}},
  author = {Dharur, Sameer and Tendulkar, Purva and Batra, Dhruv and Parikh, Devi and Selvaraju, Ramprasaath R.},
  date = {2020-11-30},
  eprint = {2010.10038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.10038},
  urldate = {2021-10-16},
  abstract = {Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world -- they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the higher level question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant sub-questions higher than irrelevant questions for an {$<$}image, reasoning-question{$>$} pair. We show that SOrT improves model consistency by upto 6.5\% points over existing baselines, while also improving visual grounding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\6BIYMIDM\\Dharur et al_2020_SOrT-ing VQA Models.pdf;D\:\\Zotero\\storage\\Y8VW4SGW\\2010.html}
}

@unpublished{dharurSOrTingVQAModels2020a,
  title = {{{SOrT-ing VQA Models}} : {{Contrastive Gradient Learning}} for {{Improved Consistency}}},
  shorttitle = {{{SOrT-ing VQA Models}}},
  author = {Dharur, Sameer and Tendulkar, Purva and Batra, Dhruv and Parikh, Devi and Selvaraju, Ramprasaath R.},
  date = {2020-11-30},
  eprint = {2010.10038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.10038},
  urldate = {2021-09-10},
  abstract = {Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world -- they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the higher level question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant sub-questions higher than irrelevant questions for an {$<$}image, reasoning-question{$>$} pair. We show that SOrT improves model consistency by upto 6.5\% points over existing baselines, while also improving visual grounding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Contrastive},
  file = {D\:\\Zotero\\storage\\YFFP7N4B\\Dharur et al_2020_SOrT-ing VQA Models.pdf;D\:\\Zotero\\storage\\WS99YPMU\\2010.html}
}

@unpublished{dhingraNeuralModelsReasoning2018,
  title = {Neural {{Models}} for {{Reasoning}} over {{Multiple Mentions}} Using {{Coreference}}},
  author = {Dhingra, Bhuwan and Jin, Qiao and Yang, Zhilin and Cohen, William W. and Salakhutdinov, Ruslan},
  date = {2018-04-16},
  eprint = {1804.05922},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1804.05922},
  urldate = {2021-10-10},
  abstract = {Many problems in NLP require aggregating information from multiple mentions of the same entity which may be far apart in the text. Existing Recurrent Neural Network (RNN) layers are biased towards short-term dependencies and hence not suited to such tasks. We present a recurrent layer which is instead biased towards coreferent dependencies. The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster. Incorporating this layer into a state-of-the-art reading comprehension model improves performance on three datasets -- Wikihop, LAMBADA and the bAbi AI tasks -- with large gains when training data is scarce.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\CYRD6RW3\\Dhingra et al_2018_Neural Models for Reasoning over Multiple Mentions using Coreference.pdf;D\:\\Zotero\\storage\\LYPKHEGV\\1804.html}
}

@unpublished{dingCognitiveGraphMultiHop2019,
  title = {Cognitive {{Graph}} for {{Multi-Hop Reading Comprehension}} at {{Scale}}},
  author = {Ding, Ming and Zhou, Chang and Chen, Qibin and Yang, Hongxia and Tang, Jie},
  date = {2019-06-04},
  eprint = {1905.05460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1905.05460},
  urldate = {2021-10-10},
  abstract = {We propose a new CogQA framework for multi-hop question answering in web-scale documents. Inspired by the dual process theory in cognitive science, the framework gradually builds a \textbackslash textit\{cognitive graph\} in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2). While giving accurate answers, our framework further provides explainable reasoning paths. Specifically, our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint \$F\_1\$ score of 34.9 on the leaderboard, compared to 23.6 of the best competitor.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\Q2BDVX6L\\Ding et al_2019_Cognitive Graph for Multi-Hop Reading Comprehension at Scale.pdf;D\:\\Zotero\\storage\\R89PGCX9\\1905.html}
}

@unpublished{dingReasoningChainBased2021,
  title = {Reasoning {{Chain Based Adversarial Attack}} for {{Multi-hop Question Answering}}},
  author = {Ding, Jiayu and Wang, Siyuan and Chen, Qin and Wei, Zhongyu},
  date = {2021-12-17},
  eprint = {2112.09658},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.09658},
  urldate = {2022-05-09},
  abstract = {Recent years have witnessed impressive advances in challenging multi-hop QA tasks. However, these QA models may fail when faced with some disturbance in the input text and their interpretability for conducting multi-hop reasoning remains uncertain. Previous adversarial attack works usually edit the whole question sentence, which has limited effect on testing the entity-based multi-hop inference ability. In this paper, we propose a multi-hop reasoning chain based adversarial attack method. We formulate the multi-hop reasoning chains starting from the query entity to the answer entity in the constructed graph, which allows us to align the question to each reasoning hop and thus attack any hop. We categorize the questions into different reasoning types and adversarially modify part of the question corresponding to the selected reasoning hop to generate the distracting sentence. We test our adversarial scheme on three QA models on HotpotQA dataset. The results demonstrate significant performance reduction on both answer and supporting facts prediction, verifying the effectiveness of our reasoning chain based attack method for multi-hop reasoning models and the vulnerability of them. Our adversarial re-training further improves the performance and robustness of these models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\GVL9N9JR\\Ding et al_2021_Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering.pdf;D\:\\Zotero\\storage\\JS2TCYVT\\2112.html}
}

@article{diVideoBackgroundMusic2021,
  title = {Video {{Background Music Generation}} with {{Controllable Music Transformer}}},
  author = {Di, Shangzhe and Jiang, Zeren and Liu, Si and Wang, Zhaokai and Zhu, Leyan and He, Zexin and Liu, Hongming and Yan, Shuicheng},
  date = {2021-10-17},
  journaltitle = {Proceedings of the 29th ACM International Conference on Multimedia},
  eprint = {2111.08380},
  eprinttype = {arxiv},
  pages = {2037--2045},
  doi = {10.1145/3474085.3475195},
  url = {http://arxiv.org/abs/2111.08380},
  urldate = {2022-02-10},
  abstract = {In this work, we address the task of video background music generation. Some previous works achieve effective music generation but are unable to generate melodious music tailored to a particular video, and none of them considers the video-music rhythmic consistency. To generate the background music that matches the given video, we first establish the rhythmic relations between video and background music. In particular, we connect timing, motion speed, and motion saliency from video with beat, simu-note density, and simu-note strength from music, respectively. We then propose CMT, a Controllable Music Transformer that enables local control of the aforementioned rhythmic features and global control of the music genre and instruments. Objective and subjective evaluations show that the generated background music has achieved satisfactory compatibility with the input videos, and at the same time, impressive music quality. Code and models are available at https://github.com/wzk1015/video-bgm-generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {D\:\\Zotero\\storage\\7WRGGTVA\\Di et al_2021_Video Background Music Generation with Controllable Music Transformer.pdf;D\:\\Zotero\\storage\\IP2SCR2K\\2111.html}
}

@unpublished{doMultipleInteractionLearning2020,
  title = {Multiple Interaction Learning with Question-Type Prior Knowledge for Constraining Answer Search Space in Visual Question Answering},
  author = {Do, Tuong and Nguyen, Binh X. and Tran, Huy and Tjiputra, Erman and Tran, Quang D. and Do, Thanh-Toan},
  date = {2020-09-23},
  eprint = {2009.11118},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2009.11118},
  urldate = {2022-04-20},
  abstract = {Different approaches have been proposed to Visual Question Answering (VQA). However, few works are aware of the behaviors of varying joint modality methods over question type prior knowledge extracted from data in constraining answer search space, of which information gives a reliable cue to reason about answers for questions asked in input images. In this paper, we propose a novel VQA model that utilizes the question-type prior information to improve VQA by leveraging the multiple interactions between different joint modality methods based on their behaviors in answering questions from different types. The solid experiments on two benchmark datasets, i.e., VQA 2.0 and TDIUC, indicate that the proposed method yields the best performance with the most competitive approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\IYWKDANS\\Do et al_2020_Multiple interaction learning with question-type prior knowledge for.pdf;D\:\\Zotero\\storage\\L4BRDAR2\\2009.html}
}

@article{dongDualEncodingVideo2021,
  title = {Dual {{Encoding}} for {{Video Retrieval}} by {{Text}}},
  author = {Dong, Jianfeng and Li, Xirong and Xu, Chaoxi and Yang, Xun and Yang, Gang and Wang, Xun and Wang, Meng},
  date = {2021},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  eprint = {2009.05381},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3059295},
  url = {http://arxiv.org/abs/2009.05381},
  urldate = {2022-05-14},
  abstract = {This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described exclusively in the form of a natural-language sentence, with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is crucial. To that end, the two modalities need to be first encoded into real-valued vectors and then projected into a common space. In this paper we achieve this by proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Our novelty is two-fold. First, different from prior art that resorts to a specific single-level encoder, the proposed network performs multi-level encoding that represents the rich content of both modalities in a coarse-to-fine fashion. Second, different from a conventional common space learning algorithm which is either concept based or latent space based, we introduce hybrid space learning which combines the high performance of the latent space and the good interpretability of the concept space. Dual encoding is conceptually simple, practically effective and end-to-end trained with hybrid space learning. Extensive experiments on four challenging video datasets show the viability of the new method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\CL8T7RSM\\Dong et al_2021_Dual Encoding for Video Retrieval by Text.pdf;D\:\\Zotero\\storage\\HKPC6LU3\\2009.html}
}

@article{dongDualEncodingVideo2021a,
  title = {Dual Encoding for Video Retrieval by Text},
  author = {Dong, Jianfeng and Li, Xirong and Xu, Chaoxi and Yang, Xun and Yang, Gang and Wang, Xun and Wang, Meng},
  date = {2021},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {{IEEE}}
}

@unpublished{dongDualGraphConvolutional2021,
  title = {Dual {{Graph Convolutional Networks}} with {{Transformer}} and {{Curriculum Learning}} for {{Image Captioning}}},
  author = {Dong, Xinzhi and Long, Chengjiang and Xu, Wenju and Xiao, Chunxia},
  date = {2021-08-05},
  eprint = {2108.02366},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.02366},
  urldate = {2022-03-26},
  abstract = {Existing image captioning methods just focus on understanding the relationship between objects or instances in a single image, without exploring the contextual correlation existed among contextual image. In this paper, we propose Dual Graph Convolutional Networks (Dual-GCN) with transformer and curriculum learning for image captioning. In particular, we not only use an object-level GCN to capture the object to object spatial relation within a single image, but also adopt an image-level GCN to capture the feature information provided by similar images. With the well-designed Dual-GCN, we can make the linguistic transformer better understand the relationship between different objects in a single image and make full use of similar images as auxiliary information to generate a reasonable caption description for a single image. Meanwhile, with a cross-review strategy introduced to determine difficulty levels, we adopt curriculum learning as the training strategy to increase the robustness and generalization of our proposed model. We conduct extensive experiments on the large-scale MS COCO dataset, and the experimental results powerfully demonstrate that our proposed method outperforms recent state-of-the-art approaches. It achieves a BLEU-1 score of 82.2 and a BLEU-2 score of 67.6. Our source code is available at \{\textbackslash em \textbackslash color\{magenta\}\{\textbackslash url\{https://github.com/Unbear430/DGCN-for-image-captioning\}\}\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\2HKBMRY7\\Dong et al_2021_Dual Graph Convolutional Networks with Transformer and Curriculum Learning for.pdf;D\:\\Zotero\\storage\\S7VAD7TH\\2108.html}
}

@inproceedings{dongPremisebasedMultimodalReasoning2022,
  title = {Premise-Based {{Multimodal Reasoning}}: {{Conditional Inference}} on {{Joint Textual}} and {{Visual Clues}}},
  shorttitle = {Premise-Based {{Multimodal Reasoning}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Dong, Qingxiu and Qin, Ziwei and Xia, Heming and Feng, Tian and Tong, Shoujie and Meng, Haoran and Xu, Lin and Wei, Zhongyu and Zhan, Weidong and Chang, Baobao and Li, Sujian and Liu, Tianyu and Sui, Zhifang},
  date = {2022},
  pages = {932--946},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.66},
  url = {https://aclanthology.org/2022.acl-long.66},
  urldate = {2022-07-11},
  abstract = {It is a common practice for recent works in vision language cross-modal reasoning to adopt a binary or multi-choice classification formulation taking as input a set of source image(s) and textual query. In this work, we take a sober look at such an “unconditional” formulation in the sense that no prior knowledge is specified with respect to the source image(s). Inspired by the designs of both visual commonsense reasoning and natural language inference tasks, we propose a new task termed “Premise-based Multi-modal Reasoning” (PMR) where a textual premise is the background presumption on each source image.The PMR dataset contains 15,360 manually annotated samples which are created by a multi-phase crowd-sourcing process. With selected high-quality movie screenshots and human-curated premise templates from 6 pre-defined categories, we ask crowd-source workers to write one true hypothesis and three distractors (4 choices) given the premise and image through a cross-check procedure.},
  eventtitle = {{{ACL}} 2022},
  file = {D\:\\Zotero\\storage\\BMEZY4HV\\Dong et al_2022_Premise-based Multimodal Reasoning.pdf}
}

@article{Doose2005,
  title = {Comparison of {{Photophysical}} and {{Colloidal Properties}} of {{Biocompatible Semiconductor Nanocrystals Using Fluorescence Correlation Spectroscopy}}},
  author = {Doose, Sören and Tsay, James M. and Pinaud, Fabien and Weiss, Shimon},
  date = {2005-04-01},
  journaltitle = {Analytical Chemistry},
  shortjournal = {Anal. Chem.},
  volume = {77},
  number = {7},
  pages = {2235--2242},
  issn = {0003-2700, 1520-6882},
  doi = {10.1021/ac050035n},
  url = {https://pubs.acs.org/doi/10.1021/ac050035n},
  urldate = {2022-10-18},
  langid = {english},
  file = {D\:\\Notes\\08-Assets\\pdfs\\2005-Comparison of Photophysical and Colloidal Properties of Biocompatible.pdf}
}

@unpublished{duanCapsuleTransformerNeuralMachine2020,
  title = {Capsule-{{Transformer}} for {{Neural Machine Translation}}},
  author = {Duan, Sufeng and Cao, Juncheng and Zhao, Hai},
  date = {2020-04-30},
  eprint = {2004.14649},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.14649},
  urldate = {2021-10-09},
  abstract = {Transformer hugely benefits from its key design of the multi-head self-attention network (SAN), which extracts information from various perspectives through transforming the given input into different subspaces. However, its simple linear transformation aggregation strategy may still potentially fail to fully capture deeper contextualized information. In this paper, we thus propose the capsule-Transformer, which extends the linear transformation into a more general capsule routing algorithm by taking SAN as a special case of capsule network. So that the resulted capsule-Transformer is capable of obtaining a better attention distribution representation of the input sequence via information aggregation among different heads and words. Specifically, we see groups of attention weights in SAN as low layer capsules. By applying the iterative capsule routing algorithm they can be further aggregated into high layer capsules which contain deeper contextualized information. Experimental results on the widely-used machine translation datasets show our proposed capsule-Transformer outperforms strong Transformer baseline significantly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\UKCDL55Q\\Duan et al_2020_Capsule-Transformer for Neural Machine Translation.pdf;D\:\\Zotero\\storage\\B789LBUC\\2004.html}
}

@inproceedings{duaVQAGeneratingMultiword2021,
  title = {Beyond {{VQA}}: {{Generating Multi-word Answers}} and {{Rationales}} to {{Visual Questions}}},
  shorttitle = {Beyond {{VQA}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Dua, Radhika and Kancheti, Sai Srinivas and Balasubramanian, Vineeth N},
  date = {2021-06},
  pages = {1623--1632},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPRW53098.2021.00178},
  url = {https://ieeexplore.ieee.org/document/9523143/},
  urldate = {2021-09-13},
  abstract = {Visual Question Answering is a multi-modal task that aims to measure high-level visual understanding. Contemporary VQA models are restrictive in the sense that answers are obtained via classification over a limited vocabulary (in the case of open-ended VQA), or via classification over a set of multiple-choice-type answers. In this work, we present a completely generative formulation where a multi-word answer is generated for a visual query. To take this a step forward, we introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer. We propose an end-to-end architecture to solve this task and describe how to evaluate it. We show that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as through a human Turing Test.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-66544-899-4},
  langid = {english}
}

@inproceedings{duaVQAGeneratingMultiword2021a,
  title = {Beyond {{VQA}}: {{Generating Multi-word Answers}} and {{Rationales}} to {{Visual Questions}}},
  shorttitle = {Beyond {{VQA}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Dua, Radhika and Kancheti, Sai Srinivas and Balasubramanian, Vineeth N},
  date = {2021-06},
  pages = {1623--1632},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPRW53098.2021.00178},
  url = {https://ieeexplore.ieee.org/document/9523143/},
  urldate = {2021-09-13},
  abstract = {Visual Question Answering is a multi-modal task that aims to measure high-level visual understanding. Contemporary VQA models are restrictive in the sense that answers are obtained via classification over a limited vocabulary (in the case of open-ended VQA), or via classification over a set of multiple-choice-type answers. In this work, we present a completely generative formulation where a multi-word answer is generated for a visual query. To take this a step forward, we introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer. We propose an end-to-end architecture to solve this task and describe how to evaluate it. We show that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as through a human Turing Test.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-66544-899-4},
  langid = {english}
}

@unpublished{duImprovingMultiModalLearning2021,
  title = {Improving {{Multi-Modal Learning}} with {{Uni-Modal Teachers}}},
  author = {Du, Chenzhuang and Li, Tingle and Liu, Yichen and Wen, Zixin and Hua, Tianyu and Wang, Yue and Zhao, Hang},
  date = {2021-06-21},
  number = {arXiv:2106.11059},
  eprint = {2106.11059},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.11059},
  urldate = {2022-05-26},
  abstract = {Learning multi-modal representations is an essential step towards real-world robotic applications, and various multi-modal fusion models have been developed for this purpose. However, we observe that existing models, whose objectives are mostly based on joint training, often suffer from learning inferior representations of each modality. We name this problem Modality Failure, and hypothesize that the imbalance of modalities and the implicit bias of common objectives in fusion method prevent encoders of each modality from sufficient feature learning. To this end, we propose a new multi-modal learning method, Uni-Modal Teacher, which combines the fusion objective and uni-modal distillation to tackle the modality failure problem. We show that our method not only drastically improves the representation of each modality, but also improves the overall multi-modal task performance. Our method can be effectively generalized to most multi-modal fusion approaches. We achieve more than 3\% improvement on the VGGSound audio-visual classification task, as well as improving performance on the NYU depth V2 RGB-D image segmentation task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\EHLBDD2A\\Du et al_2021_Improving Multi-Modal Learning with Uni-Modal Teachers.pdf;D\:\\Zotero\\storage\\3ZYFNXVB\\2106.html}
}

@article{duInvariantRepresentationLearning2022,
  title = {Invariant {{Representation Learning}} for {{Multimedia Recommendation}}},
  author = {Du, Xiaoyu and Wu, Zike and Feng, Fuli},
  date = {2022},
  pages = {10},
  abstract = {Multimedia recommendation forms a personalized ranking task with multimedia content representations which are mostly extracted via generic encoders. However, the generic representations introduce spurious correlations — the meaningless correlation from the recommendation perspective. For example, suppose a user bought two dresses on the same model, this co-occurrence would produce a correlation between the model and purchases, but the correlation is spurious from the view of fashion recommendation. Existing work alleviates this issue by customizing preference-aware representations, requiring high-cost analysis and design. In this paper, we propose an Invariant Representation Learning Framework (InvRL) to alleviate the impact of the spurious correlations. We utilize environments to reflect the spurious correlations and determine each environment with a set of interactions. We then learn invariant representations — the inherent factors attracting user attention — to make a consistent prediction of user-item interaction across various environments. In this light, InvRL proposes two iteratively executed modules to cluster user-item interactions and learn invariant representations. According to the learned invariant representations, InvRL trains a final recommender model thus mitigating the spurious correlations. We demonstrate InvRL on a cutting-edge recommender model UltraGCN and conduct extensive experiments on three public multimedia recommendation datasets, Movielens, Tiktok, and Kwai. The experimental results validate the rationality and effectiveness of InvRL.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\5A2PCQ4L\\Du 等。 - 2022 - Invariant Representation Learning for Multimedia R.pdf}
}

@inproceedings{duShiDianNaoShiftingVision2015,
  title = {{{ShiDianNao}}: Shifting Vision Processing Closer to the Sensor},
  shorttitle = {{{ShiDianNao}}},
  booktitle = {Proceedings of the 42nd {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Du, Zidong and Fasthuber, Robert and Chen, Tianshi and Ienne, Paolo and Li, Ling and Luo, Tao and Feng, Xiaobing and Chen, Yunji and Temam, Olivier},
  date = {2015-06-13},
  pages = {92--104},
  publisher = {{ACM}},
  location = {{Portland Oregon}},
  doi = {10.1145/2749469.2750389},
  url = {https://dl.acm.org/doi/10.1145/2749469.2750389},
  urldate = {2021-11-24},
  abstract = {In recent years, neural network accelerators have been shown to achieve both high energy efficiency and high performance for a broad application scope within the important category of recognition and mining applications. Still, both the energy efficiency and performance of such accelerators remain limited by memory accesses. In this paper, we focus on image applications, arguably the most important category among recognition and mining applications. The neural networks which are state-of-the-art for these applications are Convolutional Neural Networks (CNN), and they have an important property: weights are shared among many neurons, considerably reducing the neural network memory footprint. This property allows to entirely map a CNN within an SRAM, eliminating all DRAM accesses for weights. By further hoisting this accelerator next to the image sensor, it is possible to eliminate all remaining DRAM accesses, i.e., for inputs and outputs.},
  eventtitle = {{{ISCA}} '15: {{The}} 42nd {{Annual International Symposium}} on {{Computer Architecture}}},
  isbn = {978-1-4503-3402-0},
  langid = {english}
}

@unpublished{dwivediGeneralizationTransformerNetworks2021,
  title = {A {{Generalization}} of {{Transformer Networks}} to {{Graphs}}},
  author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
  date = {2021-01-24},
  eprint = {2012.09699},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.09699},
  urldate = {2022-03-26},
  abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\N7Q4FM3L\\Dwivedi_Bresson_2021_A Generalization of Transformer Networks to Graphs.pdf;D\:\\Zotero\\storage\\XQJSPXHP\\2012.html}
}

@unpublished{dwivediGraphNeuralNetworks2022,
  title = {Graph {{Neural Networks}} with {{Learnable Structural}} and {{Positional Representations}}},
  author = {Dwivedi, Vijay Prakash and Luu, Anh Tuan and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  date = {2022-02-10},
  eprint = {2110.07875},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.07875},
  urldate = {2022-03-24},
  abstract = {Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79\% up to 64.14\% when considering learnable PE for both GNN classes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\2J3IXTI4\\Dwivedi 等。 - 2022 - Graph Neural Networks with Learnable Structural an.pdf}
}

@online{DynamicCapsuleAttention,
  title = {Dynamic {{Capsule Attention}} for {{Visual Question Answering}} - {{Google}} 搜索},
  url = {https://www.google.com/search?q=Dynamic+Capsule+Attention+for+Visual+Question+Answering&oq=Dynamic+Capsule+Attention+for+Visual+Question+Answering&aqs=chrome..69i57.852j0j9&sourceid=chrome&ie=UTF-8},
  urldate = {2021-10-17},
  file = {D\:\\Zotero\\storage\\7MWJK2KR\\search.html}
}

@unpublished{esserTamingTransformersHighResolution2021,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
  date = {2021-06-23},
  number = {arXiv:2012.09841},
  eprint = {2012.09841},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2012.09841},
  urldate = {2022-06-14},
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\EV5GQWPR\\Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf;D\:\\Zotero\\storage\\66W5WGZJ\\2012.html}
}

@inproceedings{eyzaguirreDifferentiableAdaptiveComputation2020,
  title = {Differentiable {{Adaptive Computation Time}} for {{Visual Reasoning}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Eyzaguirre, Cristobal and Soto, Alvaro},
  date = {2020-06},
  pages = {12814--12822},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01283},
  url = {https://ieeexplore.ieee.org/document/9157369/},
  urldate = {2021-09-09},
  abstract = {This paper presents a novel attention-based algorithm for achieving adaptive computation called DACT, which, unlike existing ones, is end-to-end differentiable. Our method can be used in conjunction with many networks; in particular, we study its application to the widely know MAC architecture, obtaining a significant reduction in the number of recurrent steps needed to achieve similar accuracies, therefore improving its performance to computation ratio. Furthermore, we show that by increasing the maximum number of steps used, we surpass the accuracy of even our best non-adaptive MAC in the CLEVR dataset, demonstrating that our approach is able to control the number of steps without significant loss of performance. Additional advantages provided by our approach include considerably improving interpretability by discarding useless steps and providing more insights into the underlying reasoning process. Finally, we present adaptive computation as an equivalent to an ensemble of models, similar to a mixture of expert formulation. Both the code and the configuration files for our experiments are made available to support further research in this area.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {1 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\TF3H76Q2\\Eyzaguirre 和 Soto - 2020 - Differentiable Adaptive Computation Time for Visua.pdf}
}

@unpublished{fanConstructingPhraselevelSemantic2021,
  title = {Constructing {{Phrase-level Semantic Labels}} to {{Form Multi-Grained Supervision}} for {{Image-Text Retrieval}}},
  author = {Fan, Zhihao and Wei, Zhongyu and Li, Zejun and Wang, Siyuan and Shan, Haijun and Huang, Xuanjing and Fan, Jianqing},
  date = {2021-09-12},
  eprint = {2109.05523},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.05523},
  urldate = {2022-03-23},
  abstract = {Existing research for image text retrieval mainly relies on sentence-level supervision to distinguish matched and mismatched sentences for a query image. However, semantic mismatch between an image and sentences usually happens in finer grain, i.e., phrase level. In this paper, we explore to introduce additional phrase-level supervision for the better identification of mismatched units in the text. In practice, multi-grained semantic labels are automatically constructed for a query image in both sentence-level and phrase-level. We construct text scene graphs for the matched sentences and extract entities and triples as the phrase-level labels. In order to integrate both supervision of sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT, we utilize different kinds of attention mechanisms to enforce interactions of multi-grain semantic units in both sides of vision and language. For the training, we propose multi-scale matching losses from both global and local perspectives, and penalize mismatched phrases. Experimental results on MS-COCO and Flickr30K show the effectiveness of our approach compared to some state-of-the-art models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\M67GIQ9G\\Fan et al_2021_Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for.pdf;D\:\\Zotero\\storage\\2YZ6F96D\\2109.html}
}

@unpublished{fangEscapingLanguageBias2022,
  title = {Towards {{Escaping}} from {{Language Bias}} and {{OCR Error}}: {{Semantics-Centered Text Visual Question Answering}}},
  shorttitle = {Towards {{Escaping}} from {{Language Bias}} and {{OCR Error}}},
  author = {Fang, Chengyang and Zeng, Gangyan and Zhou, Yu and Wu, Daiqing and Ma, Can and Hu, Dayong and Wang, Weiping},
  date = {2022-03-24},
  eprint = {2203.12929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2203.12929},
  urldate = {2022-03-27},
  abstract = {Texts in scene images convey critical information for scene understanding and reasoning. The abilities of reading and reasoning matter for the model in the text-based visual question answering (TextVQA) process. However, current TextVQA models do not center on the text and suffer from several limitations. The model is easily dominated by language biases and optical character recognition (OCR) errors due to the absence of semantic guidance in the answer prediction process. In this paper, we propose a novel Semantics-Centered Network (SC-Net) that consists of an instance-level contrastive semantic prediction module (ICSP) and a semantics-centered transformer module (SCT). Equipped with the two modules, the semantics-centered model can resist the language biases and the accumulated errors from OCR. Extensive experiments on TextVQA and ST-VQA datasets show the effectiveness of our model. SC-Net surpasses previous works with a noticeable margin and is more reasonable for the TextVQA task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\5BRNRSR6\\Fang et al_2022_Towards Escaping from Language Bias and OCR Error.pdf;D\:\\Zotero\\storage\\RZ64C2I8\\2203.html}
}

@unpublished{fangHierarchicalGraphNetwork2020,
  title = {Hierarchical {{Graph Network}} for {{Multi-hop Question Answering}}},
  author = {Fang, Yuwei and Sun, Siqi and Gan, Zhe and Pillai, Rohit and Wang, Shuohang and Liu, Jingjing},
  date = {2020-10-06},
  eprint = {1911.03631},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.03631},
  urldate = {2021-10-10},
  abstract = {In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\QKZISE25\\Fang et al_2020_Hierarchical Graph Network for Multi-hop Question Answering.pdf;D\:\\Zotero\\storage\\GBLY3SHE\\1911.html}
}

@inproceedings{fangReadHumansAutonomous2021,
  title = {Read {{Like Humans}}: {{Autonomous}}, {{Bidirectional}} and {{Iterative Language Modeling}} for {{Scene Text Recognition}}},
  shorttitle = {Read {{Like Humans}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Fang, Shancheng and Xie, Hongtao and Wang, Yuxin and Mao, Zhendong and Zhang, Yongdong},
  date = {2021-06},
  pages = {7094--7103},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00702},
  url = {https://ieeexplore.ieee.org/document/9578001/},
  urldate = {2022-05-05},
  abstract = {Linguistic knowledge is of great benefit to scene text recognition. However, how to effectively model linguistic rules in end-to-end deep networks remains a research challenge. In this paper, we argue that the limited capacity of language models comes from: 1) implicitly language modeling; 2) unidirectional feature representation; and 3) language model with noise input. Correspondingly, we propose an autonomous, bidirectional and iterative ABINet for scene text recognition. Firstly, the autonomous suggests to block gradient flow between vision and language models to enforce explicitly language modeling. Secondly, a novel bidirectional cloze network (BCN) as the language model is proposed based on bidirectional feature representation. Thirdly, we propose an execution manner of iterative correction for language model which can effectively alleviate the impact of noise input. Additionally, based on the ensemble of iterative predictions, we propose a self-training method which can learn from unlabeled images effectively. Extensive experiments indicate that ABINet has superiority on lowquality images and achieves state-of-the-art results on several mainstream benchmarks. Besides, the ABINet trained with ensemble self-training shows promising improvement in realizing human-level recognition. Code is available at https://github.com/FangShancheng/ABINet.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ESSFTPAR\\Fang 等。 - 2021 - Read Like Humans Autonomous, Bidirectional and It.pdf}
}

@inproceedings{fangReadHumansAutonomous2021a,
  title = {Read {{Like Humans}}: {{Autonomous}}, {{Bidirectional}} and {{Iterative Language Modeling}} for {{Scene Text Recognition}}},
  shorttitle = {Read {{Like Humans}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Fang, Shancheng and Xie, Hongtao and Wang, Yuxin and Mao, Zhendong and Zhang, Yongdong},
  date = {2021-06},
  pages = {7094--7103},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00702},
  url = {https://ieeexplore.ieee.org/document/9578001/},
  urldate = {2022-04-02},
  abstract = {Linguistic knowledge is of great benefit to scene text recognition. However, how to effectively model linguistic rules in end-to-end deep networks remains a research challenge. In this paper, we argue that the limited capacity of language models comes from: 1) implicitly language modeling; 2) unidirectional feature representation; and 3) language model with noise input. Correspondingly, we propose an autonomous, bidirectional and iterative ABINet for scene text recognition. Firstly, the autonomous suggests to block gradient flow between vision and language models to enforce explicitly language modeling. Secondly, a novel bidirectional cloze network (BCN) as the language model is proposed based on bidirectional feature representation. Thirdly, we propose an execution manner of iterative correction for language model which can effectively alleviate the impact of noise input. Additionally, based on the ensemble of iterative predictions, we propose a self-training method which can learn from unlabeled images effectively. Extensive experiments indicate that ABINet has superiority on lowquality images and achieves state-of-the-art results on several mainstream benchmarks. Besides, the ABINet trained with ensemble self-training shows promising improvement in realizing human-level recognition. Code is available at https://github.com/FangShancheng/ABINet.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\MJAWFTK4\\Fang 等。 - 2021 - Read Like Humans Autonomous, Bidirectional and It.pdf}
}

@inproceedings{fanQuestionTypeDriven2018,
  title = {A {{Question Type Driven Framework}} to {{Diversify Visual Question Generation}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Fan, Zhihao and Wei, Zhongyu and Li, Piji and Lan, Yanyan and Huang, Xuanjing},
  date = {2018-07},
  pages = {4048--4054},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/563},
  url = {https://www.ijcai.org/proceedings/2018/563},
  urldate = {2021-09-10},
  abstract = {Visual question generation aims at asking questions about an image automatically. Existing research works on this topic usually generate a single question for each given image without considering the issue of diversity. In this paper, we propose a question type driven framework to produce multiple questions for a given image with different focuses. In our framework, each question is constructed following the guidance of a sampled question type in a sequence-to-sequence fashion. To diversify the generated questions, a novel conditional variational auto-encoder is introduced to generate multiple questions with a specific question type. Moreover, we design a strategy to conduct the question type distribution learning for each image to select the final questions. Experimental results on three benchmark datasets show that our framework outperforms the state-of-the-art approaches in terms of both relevance and diversity.},
  eventtitle = {Twenty-{{Seventh International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-18}}\vphantom\{\}},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\TBM2VHF6\\Fan 等。 - 2018 - A Question Type Driven Framework to Diversify Visu.pdf}
}

@misc{fanUnifiedContinuousLearning2022,
  title = {A {{Unified Continuous Learning Framework}} for {{Multi-modal Knowledge Discovery}} and {{Pre-training}}},
  author = {Fan, Zhihao and Wei, Zhongyu and Chen, Jingjing and Wang, Siyuan and Li, Zejun and Xu, Jiarong and Huang, Xuanjing},
  date = {2022-06-11},
  number = {arXiv:2206.05555},
  eprint = {2206.05555},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.05555},
  urldate = {2022-07-11},
  abstract = {Multi-modal pre-training and knowledge discovery are two important research topics in multi-modal machine learning. Nevertheless, none of existing works make attempts to link knowledge discovery with knowledge guided multi-modal pre-training. In this paper, we propose to unify them into a continuous learning framework for mutual improvement. Taking the open-domain uni-modal datasets of images and texts as input, we maintain a knowledge graph as the foundation to support these two tasks. For knowledge discovery, a pre-trained model is used to identify cross-modal links on the graph. For model pre-training, the knowledge graph is used as the external knowledge to guide the model updating. These two steps are iteratively performed in our framework for continuous learning. The experimental results on MS-COCO and Flickr30K with respect to both knowledge discovery and the pre-trained model validate the effectiveness of our framework.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\AAPED9YX\\Fan et al_2022_A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and.pdf;D\:\\Zotero\\storage\\NHJZIH5G\\2206.html}
}

@article{feiArtificialGeneralIntelligence2022,
  title = {Towards Artificial General Intelligence via a Multimodal Foundation Model},
  author = {Fei, Nanyi and Lu, Zhiwu and Gao, Yizhao and Yang, Guoxing and Huo, Yuqi and Wen, Jingyuan and Lu, Haoyu and Song, Ruihua and Gao, Xin and Xiang, Tao and Sun, Hao and Wen, Ji-Rong},
  date = {2022-12},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {3094},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-30761-2},
  url = {https://www.nature.com/articles/s41467-022-30761-2},
  urldate = {2022-06-07},
  abstract = {Abstract             The fundamental goal of artificial intelligence (AI) is to mimic the core cognitive activities of human. Despite tremendous success in the AI research, most of existing methods have only single-cognitive ability. To overcome this limitation and take a solid step towards artificial general intelligence (AGI), we develop a foundation model pre-trained with huge multimodal data, which can be quickly adapted for various downstream cognitive tasks. To achieve this goal, we propose to pre-train our foundation model by self-supervised learning with weak semantic correlation data crawled from the Internet and show that promising results can be obtained on a wide range of downstream tasks. Particularly, with the developed model-interpretability tools, we demonstrate that strong imagination ability is now possessed by our foundation model. We believe that our work makes a transformative stride towards AGI, from our common practice of “weak or narrow AI” to that of “strong or generalized AI”.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4LVHEF35\\Fei 等。 - 2022 - Towards artificial general intelligence via a mult.pdf}
}

@unpublished{fengVideoRelocalization2018,
  title = {Video {{Re-localization}}},
  author = {Feng, Yang and Ma, Lin and Liu, Wei and Zhang, Tong and Luo, Jiebo},
  date = {2018-08-05},
  number = {arXiv:1808.01575},
  eprint = {1808.01575},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1808.01575},
  urldate = {2022-05-25},
  abstract = {Many methods have been developed to help people find the video contents they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely \textbackslash textbf\{video re-localization\}, to address this scenario. Video re-localization is an important emerging technology implicating many applications, such as fast seeking in videos, video copy detection, video surveillance, etc. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the competing methods. Our code is available at: https://github.com/fengyang0317/video\_reloc.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\UJQCAELE\\Feng et al_2018_Video Re-localization.pdf;D\:\\Zotero\\storage\\IDPTG7PR\\1808.html}
}

@unpublished{frankVisionandLanguageVisionforLanguageCrossModal2021,
  title = {Vision-and-{{Language}} or {{Vision-for-Language}}? {{On Cross-Modal Influence}} in {{Multimodal Transformers}}},
  shorttitle = {Vision-and-{{Language}} or {{Vision-for-Language}}?},
  author = {Frank, Stella and Bugliarello, Emanuele and Elliott, Desmond},
  date = {2021-09-09},
  eprint = {2109.04448},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.04448},
  urldate = {2021-09-10},
  abstract = {Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\ZRZPUCE8\\Frank et al_2021_Vision-and-Language or Vision-for-Language.pdf;D\:\\Zotero\\storage\\APDBT5B4\\2109.html}
}

@unpublished{fransCLIPDrawExploringTexttoDrawing2021,
  title = {{{CLIPDraw}}: {{Exploring Text-to-Drawing Synthesis}} through {{Language-Image Encoders}}},
  shorttitle = {{{CLIPDraw}}},
  author = {Frans, Kevin and Soros, L. B. and Witkowski, Olaf},
  date = {2021-06-28},
  number = {arXiv:2106.14843},
  eprint = {2106.14843},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.14843},
  urldate = {2022-06-12},
  abstract = {This work presents CLIPDraw, an algorithm that synthesizes novel drawings based on natural language input. CLIPDraw does not require any training; rather a pre-trained CLIP language-image encoder is used as a metric for maximizing similarity between the given description and a generated drawing. Crucially, CLIPDraw operates over vector strokes rather than pixel images, a constraint that biases drawings towards simpler human-recognizable shapes. Results compare between CLIPDraw and other synthesis-through-optimization methods, as well as highlight various interesting behaviors of CLIPDraw, such as satisfying ambiguous text in multiple ways, reliably producing drawings in diverse artistic styles, and scaling from simple to complex visual representations as stroke count is increased. Code for experimenting with the method is available at: https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\UMP8NG87\\Frans et al_2021_CLIPDraw.pdf;D\:\\Zotero\\storage\\JD4VNS6R\\2106.html}
}

@inproceedings{fukuiMultimodalCompactBilinear2016,
  title = {Multimodal {{Compact Bilinear Pooling}} for {{Visual Question Answering}} and {{Visual Grounding}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  date = {2016},
  pages = {457--468},
  publisher = {{Association for Computational Linguistics}},
  location = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1044},
  url = {http://aclweb.org/anthology/D16-1044},
  urldate = {2021-10-25},
  abstract = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
  eventtitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  langid = {english},
  file = {D\:\\Zotero\\storage\\DEPZXXFR\\Fukui 等。 - 2016 - Multimodal Compact Bilinear Pooling for Visual Que.pdf}
}

@unpublished{gamageImprovedRAMENDomain2021,
  title = {Improved {{RAMEN}}: {{Towards Domain Generalization}} for {{Visual Question Answering}}},
  shorttitle = {Improved {{RAMEN}}},
  author = {Gamage, Bhanuka Manesha Samarasekara Vitharana and Hong, Lim Chern},
  date = {2021-09-06},
  eprint = {2109.02370},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.02370},
  urldate = {2021-09-07},
  abstract = {Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence. Established as a multi-disciplinary field in machine learning, both computer vision and natural language processing communities are working together to achieve state-of-the-art (SOTA) performance. However, there is a gap between the SOTA results and real world applications. This is due to the lack of model generalisation. The RAMEN model \textbackslash cite\{Shrestha2019\} aimed to achieve domain generalization by obtaining the highest score across two main types of VQA datasets. This study provides two major improvements to the early/late fusion module and aggregation module of the RAMEN architecture, with the objective of further strengthening domain generalization. Vector operations based fusion strategies are introduced for the fusion module and the transformer architecture is introduced for the aggregation module. Improvements of up to five VQA datasets from the experiments conducted are evident. Following the results, this study analyses the effects of both the improvements on the domain generalization problem. The code is available on GitHub though the following link \textbackslash url\{https://github.com/bhanukaManesha/ramen\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\6YB3NAMX\\Gamage_Hong_2021_Improved RAMEN.pdf;D\:\\Zotero\\storage\\LII8E73K\\2109.html}
}

@unpublished{ganMultistepReasoningRecurrent2019,
  title = {Multi-Step {{Reasoning}} via {{Recurrent Dual Attention}} for {{Visual Dialog}}},
  author = {Gan, Zhe and Cheng, Yu and Kholy, Ahmed El and Li, Linjie and Liu, Jingjing and Gao, Jianfeng},
  date = {2019-06-04},
  eprint = {1902.00579},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.00579},
  urldate = {2021-12-08},
  abstract = {This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47\% NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\NRRLMS4D\\Gan et al_2019_Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.pdf;D\:\\Zotero\\storage\\2WE8MRQF\\1902.html}
}

@inproceedings{ganVQSLinkingSegmentations2017,
  title = {{{VQS}}: {{Linking Segmentations}} to {{Questions}} and {{Answers}} for {{Supervised Attention}} in {{VQA}} and {{Question-Focused Semantic Segmentation}}},
  shorttitle = {{{VQS}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Gan, Chuang and Li, Yandong and Li, Haoxiang and Sun, Chen and Gong, Boqing},
  date = {2017-10},
  pages = {1829--1838},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.201},
  url = {http://ieeexplore.ieee.org/document/8237463/},
  urldate = {2021-09-10},
  abstract = {Rich and dense human labeled datasets are among the main enabling factors for the recent advance on visionlanguage understanding. Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal different levels and perspectives of human understandings about the same visual scenes — and even the same set of images (e.g., of COCO). The popularity of COCO correlates those annotations and tasks. Explicitly linking them up may significantly benefit both individual tasks and the unified vision and language modeling.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\J67KSCD4\\Gan 等。 - 2017 - VQS Linking Segmentations to Questions and Answer.pdf}
}

@unpublished{gaoAreYouTalking2015,
  title = {Are {{You Talking}} to a {{Machine}}? {{Dataset}} and {{Methods}} for {{Multilingual Image Question Answering}}},
  shorttitle = {Are {{You Talking}} to a {{Machine}}?},
  author = {Gao, Haoyuan and Mao, Junhua and Zhou, Jie and Huang, Zhiheng and Wang, Lei and Xu, Wei},
  date = {2015-11-02},
  eprint = {1505.05612},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1505.05612},
  urldate = {2021-03-09},
  abstract = {In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7\% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: http://idl.baidu.com/FM-IQA.html},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.10,I.2.6,I.2.7},
  file = {D\:\\Zotero\\storage\\DFEAIBVI\\1505.html}
}

@unpublished{gaoAreYouTalking2015a,
  title = {Are {{You Talking}} to a {{Machine}}? {{Dataset}} and {{Methods}} for {{Multilingual Image Question Answering}}},
  shorttitle = {Are {{You Talking}} to a {{Machine}}?},
  author = {Gao, Haoyuan and Mao, Junhua and Zhou, Jie and Huang, Zhiheng and Wang, Lei and Xu, Wei},
  date = {2015-11-02},
  eprint = {1505.05612},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1505.05612},
  urldate = {2021-03-09},
  abstract = {In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7\% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: http://idl.baidu.com/FM-IQA.html},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.10,I.2.6,I.2.7},
  file = {D\:\\Zotero\\storage\\ZDX5VWZZ\\Gao 等。 - 2015 - Are You Talking to a Machine Dataset and Methods .pdf;D\:\\Zotero\\storage\\LVYT9IFU\\1505.html}
}

@unpublished{gaoChopChopBERT2021,
  title = {Chop {{Chop BERT}}: {{Visual Question Answering}} by {{Chopping VisualBERT}}'s {{Heads}}},
  shorttitle = {Chop {{Chop BERT}}},
  author = {Gao, Chenyu and Zhu, Qi and Wang, Peng and Wu, Qi},
  date = {2021-04-29},
  eprint = {2104.14741},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.14741},
  urldate = {2021-10-11},
  abstract = {Vision-and-Language (VL) pre-training has shown great potential on many related downstream tasks, such as Visual Question Answering (VQA), one of the most popular problems in the VL field. All of these pre-trained models (such as VisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which extends the classical attention mechanism to multiple layers and heads. To investigate why and how these models work on VQA so well, in this paper we explore the roles of individual heads and layers in Transformer models when handling 12 different types of questions. Specifically, we manually remove (chop) heads (or layers) from a pre-trained VisualBERT model at a time, and test it on different levels of questions to record its performance. As shown in the interesting echelon shape of the result matrices, experiments reveal different heads and layers are responsible for different question types, with higher-level layers activated by higher-level visual reasoning questions. Based on this observation, we design a dynamic chopping module that can automatically remove heads and layers of the VisualBERT at an instance level when dealing with different questions. Our dynamic chopping module can effectively reduce the parameters of the original model by 50\%, while only damaging the accuracy by less than 1\% on the VQA task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T45,Computer Science - Computer Vision and Pattern Recognition,I.4.8},
  file = {D\:\\Zotero\\storage\\N2AUYLWG\\Gao 等。 - 2021 - Chop Chop BERT Visual Question Answering by Chopp.pdf}
}

@inproceedings{gaoDynamicFusionIntra2019,
  title = {Dynamic {{Fusion With Intra-}} and {{Inter-Modality Attention Flow}} for {{Visual Question Answering}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gao, Peng and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven C. H. and Wang, Xiaogang and Li, Hongsheng},
  date = {2019-06},
  pages = {6632--6641},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00680},
  url = {https://ieeexplore.ieee.org/document/8953537/},
  urldate = {2021-09-09},
  abstract = {Learning effective fusion of multi-modality features is at the heart of visual question answering. We propose a novel method of dynamically fusing multi-modal features with intra- and inter-modality information flow, which alternatively pass dynamic information between and across the visual and language modalities. It can robustly capture the high-level interactions between language and vision domains, thus significantly improves the performance of visual question answering. We also show that the proposed dynamic intra-modality attention flow conditioned on the other modality can dynamically modulate the intramodality attention of the target modality, which is vital for multimodality feature fusion. Experimental evaluations on the VQA 2.0 dataset show that the proposed method achieves state-of-the-art VQA performance. Extensive ablation studies are carried out for the comprehensive analysis of the proposed method.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  keywords = {Attention},
  annotation = {51 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\66RQN2M2\\Gao 等。 - 2019 - Dynamic Fusion With Intra- and Inter-Modality Atte.pdf}
}

@inproceedings{gaoDynamicFusionIntra2019a,
  title = {Dynamic {{Fusion With Intra-}} and {{Inter-Modality Attention Flow}} for {{Visual Question Answering}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gao, Peng and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven C. H. and Wang, Xiaogang and Li, Hongsheng},
  date = {2019-06},
  pages = {6632--6641},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00680},
  url = {https://ieeexplore.ieee.org/document/8953537/},
  urldate = {2021-03-11},
  abstract = {Learning effective fusion of multi-modality features is at the heart of visual question answering. We propose a novel method of dynamically fusing multi-modal features with intra- and inter-modality information flow, which alternatively pass dynamic information between and across the visual and language modalities. It can robustly capture the high-level interactions between language and vision domains, thus significantly improves the performance of visual question answering. We also show that the proposed dynamic intra-modality attention flow conditioned on the other modality can dynamically modulate the intramodality attention of the target modality, which is vital for multimodality feature fusion. Experimental evaluations on the VQA 2.0 dataset show that the proposed method achieves state-of-the-art VQA performance. Extensive ablation studies are carried out for the comprehensive analysis of the proposed method.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SW8B4B23\\Gao 等。 - 2019 - Dynamic Fusion With Intra- and Inter-Modality Atte.pdf}
}

@article{gaoMotionAppearanceCoMemoryNetworks,
  title = {Motion-{{Appearance Co-Memory Networks}} for {{Video Question Answering}}},
  author = {Gao, Jiyang and Ge, Runzhou and Chen, Kan and Nevatia, Ram},
  pages = {10},
  abstract = {Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based on these observations, we propose a motion-appearance comemory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-ofthe-art significantly on all four tasks of TGIF-QA.},
  langid = {english},
  annotation = {00000},
  file = {D\:\\Zotero\\storage\\8FTE5MHI\\Gao 等。 - Motion-Appearance Co-Memory Networks for Video Que.pdf}
}

@inproceedings{gaoMotionAppearanceComemoryNetworks2018,
  title = {Motion-{{Appearance Co-memory Networks}} for {{Video Question Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Gao, Jiyang and Ge, Runzhou and Chen, Kan and Nevatia, Ram},
  date = {2018-06},
  pages = {6576--6585},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00688},
  url = {https://ieeexplore.ieee.org/document/8578786/},
  urldate = {2021-03-11},
  abstract = {Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based on these observations, we propose a motion-appearance comemory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-ofthe-art significantly on all four tasks of TGIF-QA.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\YHCYV5L3\\Gao 等。 - 2018 - Motion-Appearance Co-memory Networks for Video Que.pdf}
}

@unpublished{gaoMultiModalGraphNeural2020,
  title = {Multi-{{Modal Graph Neural Network}} for {{Joint Reasoning}} on {{Vision}} and {{Scene Text}}},
  author = {Gao, Difei and Li, Ke and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
  date = {2020-03-31},
  eprint = {2003.13962},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.13962},
  urldate = {2022-02-16},
  abstract = {Answering questions that require reading texts in an image is challenging for current models. One key difficulty of this task is that rare, polysemous, and ambiguous words frequently appear in images, e.g., names of places, products, and sports teams. To overcome this difficulty, only resorting to pre-trained word embedding models is far from enough. A desired model should utilize the rich information in multiple modalities of the image to help understand the meaning of scene texts, e.g., the prominent text on a bottle is most likely to be the brand. Following this idea, we propose a novel VQA approach, Multi-Modal Graph Neural Network (MM-GNN). It first represents an image as a graph consisting of three sub-graphs, depicting visual, semantic, and numeric modalities respectively. Then, we introduce three aggregators which guide the message passing from one graph to another to utilize the contexts in various modalities, so as to refine the features of nodes. The updated nodes have better features for the downstream question answering module. Experimental evaluations show that our MM-GNN represents the scene texts better and obviously facilitates the performances on two VQA tasks that require reading scene texts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\Z7UJHCJD\\Gao et al_2020_Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text.pdf;D\:\\Zotero\\storage\\GJNH933G\\2003.html}
}

@unpublished{gaoMultiModalGraphNeural2020a,
  title = {Multi-{{Modal Graph Neural Network}} for {{Joint Reasoning}} on {{Vision}} and {{Scene Text}}},
  author = {Gao, Difei and Li, Ke and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
  date = {2020-03-31},
  eprint = {2003.13962},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.13962},
  urldate = {2021-09-09},
  abstract = {Answering questions that require reading texts in an image is challenging for current models. One key difficulty of this task is that rare, polysemous, and ambiguous words frequently appear in images, e.g., names of places, products, and sports teams. To overcome this difficulty, only resorting to pre-trained word embedding models is far from enough. A desired model should utilize the rich information in multiple modalities of the image to help understand the meaning of scene texts, e.g., the prominent text on a bottle is most likely to be the brand. Following this idea, we propose a novel VQA approach, Multi-Modal Graph Neural Network (MM-GNN). It first represents an image as a graph consisting of three sub-graphs, depicting visual, semantic, and numeric modalities respectively. Then, we introduce three aggregators which guide the message passing from one graph to another to utilize the contexts in various modalities, so as to refine the features of nodes. The updated nodes have better features for the downstream question answering module. Experimental evaluations show that our MM-GNN represents the scene texts better and obviously facilitates the performances on two VQA tasks that require reading scene texts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Graph，Reasoning},
  file = {D\:\\Zotero\\storage\\IA2QK42B\\Gao et al_2020_Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text.pdf;D\:\\Zotero\\storage\\2LXRRINN\\2003.html}
}

@incollection{gaoQuestionGuidedHybridConvolution2018,
  title = {Question-{{Guided Hybrid Convolution}} for {{Visual Question Answering}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Gao, Peng and Li, Hongsheng and Li, Shuang and Lu, Pan and Li, Yikang and Hoi, Steven C. H. and Wang, Xiaogang},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11205},
  pages = {485--501},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01246-5_29},
  url = {http://link.springer.com/10.1007/978-3-030-01246-5_29},
  urldate = {2021-09-10},
  abstract = {In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features. To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage. The question-guided convolution can tightly couple the textual and visual information but also introduce more parameters when learning kernels. We apply the group convolution, which consists of question-independent kernels and question-dependent kernels, to reduce the parameter size and alleviate over-fitting. The hybrid convolution can generate discriminative multi-modal features with fewer parameters. The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods. By integrating with them, our method could further boost the performance. Experiments on VQA datasets validate the effectiveness of QGHC.},
  isbn = {978-3-030-01245-8 978-3-030-01246-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\E7GIJPQ6\\Gao 等。 - 2018 - Question-Guided Hybrid Convolution for Visual Ques.pdf}
}

@unpublished{gaoStructuredMultimodalAttentions2021,
  title = {Structured {{Multimodal Attentions}} for {{TextVQA}}},
  author = {Gao, Chenyu and Zhu, Qi and Wang, Peng and Li, Hui and Liu, Yuliang and van den Hengel, Anton and Wu, Qi},
  date = {2021-11-25},
  eprint = {2006.00753},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.00753},
  urldate = {2021-12-08},
  abstract = {In this paper, we propose an end-to-end structured multimodal attention (SMA) neural network to mainly solve the first two issues above. SMA first uses a structural graph representation to encode the object-object, object-text and text-text relationships appearing in the image, and then designs a multimodal graph attention network to reason over it. Finally, the outputs from the above modules are processed by a global-local attentional answering module to produce an answer splicing together tokens from both OCR and general vocabulary iteratively by following M4C. Our proposed model outperforms the SoTA models on TextVQA dataset and two tasks of ST-VQA dataset among all models except pre-training based TAP. Demonstrating strong reasoning ability, it also won first place in TextVQA Challenge 2020. We extensively test different OCR methods on several reasoning models and investigate the impact of gradually increased OCR performance on TextVQA benchmark. With better OCR results, different models share dramatic improvement over the VQA accuracy, but our model benefits most blessed by strong textual-visual reasoning ability. To grant our method an upper bound and make a fair testing base available for further works, we also provide human-annotated ground-truth OCR annotations for the TextVQA dataset, which were not given in the original release. The code and ground-truth OCR annotations for the TextVQA dataset are available at https://github.com/ChenyuGAO-CS/SMA},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\67T3KPS6\\Gao et al_2021_Structured Multimodal Attentions for TextVQA.pdf;D\:\\Zotero\\storage\\RXBUYZ9Z\\2006.html}
}

@article{gaoStructuredTwoStreamAttention2019,
  title = {Structured {{Two-Stream Attention Network}} for {{Video Question Answering}}},
  author = {Gao, Lianli and Zeng, Pengpeng and Song, Jingkuan and Li, Yuan-Fang and Liu, Wu and Mei, Tao and Shen, Heng Tao},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {6391--6398},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33016391},
  url = {http://www.aaai.org/ojs/index.php/AAAI/article/view/4602},
  urldate = {2021-09-10},
  abstract = {To date, visual question answering (VQA) (i.e., image QA and video QA) is still a holy grail in vision and language understanding, especially for video QA. Compared with image QA that focuses primarily on understanding the associations between image region-level details and corresponding questions, video QA requires a model to jointly reason across both spatial and long-range temporal structures of a video as well as text to provide an accurate answer. In this paper, we specifically tackle the problem of video QA by proposing a Structured Two-stream Attention network, namely STA, to answer a free-form or open-ended natural language question about the content of a given video. First, we infer rich longrange temporal structures in videos using our structured segment component and encode text features. Then, our structured two-stream attention component simultaneously localizes important visual instance, reduces the influence of background video and focuses on the relevant text. Finally, the structured two-stream fusion component incorporates different segments of query and video aware context representation and infers the answers. Experiments on the large-scale video QA dataset TGIF-QA show that our proposed method significantly surpasses the best counterpart (i.e., with one representation for the video input) by 13.0\%, 13.5\%, 11.0\% and 0.3 for Action, Trans., TrameQA and Count tasks. It also outperforms the best competitor (i.e., with two representations) on the Action, Trans., TrameQA tasks by 4.1\%, 4.7\%, and 5.1\%.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\U7F6KHBC\\Gao 等。 - 2019 - Structured Two-Stream Attention Network for Video .pdf}
}

@article{garciaKnowITVQAAnswering2020,
  title = {{{KnowIT VQA}}: {{Answering Knowledge-Based Questions}} about {{Videos}}},
  shorttitle = {{{KnowIT VQA}}},
  author = {Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {07},
  pages = {10826--10834},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i07.6713},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6713},
  urldate = {2021-09-10},
  abstract = {We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 humangenerated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BFTRC6HG\\Garcia 等。 - 2020 - KnowIT VQA Answering Knowledge-Based Questions ab.pdf}
}

@incollection{garciaKnowledgeBasedVideoQuestion2020,
  title = {Knowledge-{{Based Video Question Answering}} with {{Unsupervised Scene Descriptions}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Garcia, Noa and Nakashima, Yuta},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12363},
  pages = {581--598},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58523-5_34},
  url = {https://link.springer.com/10.1007/978-3-030-58523-5_34},
  urldate = {2021-09-10},
  abstract = {To understand movies, humans constantly reason over the dialogues and actions shown in specific scenes and relate them to the overall storyline already seen. Inspired by this behaviour, we design ROLL, a model for knowledge-based video story question answering that leverages three crucial aspects of movie understanding: dialog comprehension, scene reasoning, and storyline recalling. In ROLL, each of these tasks is in charge of extracting rich and diverse information by 1) processing scene dialogues, 2) generating unsupervised video scene descriptions, and 3) obtaining external knowledge in a weakly supervised fashion. To answer a given question correctly, the information generated by each inspiredcognitive task is encoded via Transformers and fused through a modality weighting mechanism, which balances the information from the different sources. Exhaustive evaluation demonstrates the effectiveness of our approach, which yields a new state-of-the-art on two challenging video question answering datasets: KnowIT VQA and TVQA+.},
  isbn = {978-3-030-58522-8 978-3-030-58523-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\B5BV2HBS\\Garcia 和 Nakashima - 2020 - Knowledge-Based Video Question Answering with Unsu.pdf}
}

@inproceedings{garderesConceptBertConceptAwareRepresentation2020,
  title = {{{ConceptBert}}: {{Concept-Aware Representation}} for {{Visual Question Answering}}},
  shorttitle = {{{ConceptBert}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Gardères, François and Ziaeefard, Maryam and Abeloos, Baptiste and Lecue, Freddy},
  date = {2020},
  pages = {489--498},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.44},
  url = {https://www.aclweb.org/anthology/2020.findings-emnlp.44},
  urldate = {2021-09-10},
  eventtitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2ES57QK3\\Gardères 等。 - 2020 - ConceptBert Concept-Aware Representation for Visu.pdf}
}

@unpublished{gargUnconditionalSceneGraph2021,
  title = {Unconditional {{Scene Graph Generation}}},
  author = {Garg, Sarthak and Dhamo, Helisa and Farshad, Azade and Musatian, Sabrina and Navab, Nassir and Tombari, Federico},
  date = {2021-08-12},
  eprint = {2108.05884},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.05884},
  urldate = {2021-10-09},
  abstract = {Despite recent advancements in single-domain or single-object image generation, it is still challenging to generate complex scenes containing diverse, multiple objects and their interactions. Scene graphs, composed of nodes as objects and directed-edges as relationships among objects, offer an alternative representation of a scene that is more semantically grounded than images. We hypothesize that a generative model for scene graphs might be able to learn the underlying semantic structure of real-world scenes more effectively than images, and hence, generate realistic novel scenes in the form of scene graphs. In this work, we explore a new task for the unconditional generation of semantic scene graphs. We develop a deep auto-regressive model called SceneGraphGen which can directly learn the probability distribution over labelled and directed graphs using a hierarchical recurrent architecture. The model takes a seed object as input and generates a scene graph in a sequence of steps, each step generating an object node, followed by a sequence of relationship edges connecting to the previous nodes. We show that the scene graphs generated by SceneGraphGen are diverse and follow the semantic patterns of real-world scenes. Additionally, we demonstrate the application of the generated graphs in image synthesis, anomaly detection and scene graph completion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\3XISU9NM\\Garg et al_2021_Unconditional Scene Graph Generation.pdf;D\:\\Zotero\\storage\\C6SYCD7Q\\2108.html}
}

@inproceedings{gatRemovingBiasMultimodal2020,
  title = {Removing {{Bias}} in {{Multi-modal Classifiers}}: {{Regularization}} by {{Maximizing Functional Entropies}}},
  shorttitle = {Removing {{Bias}} in {{Multi-modal Classifiers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gat, Itai and Schwartz, Idan and Schwing, Alexander and Hazan, Tamir},
  date = {2020},
  volume = {33},
  pages = {3197--3208},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/20d749bc05f47d2bd3026ce457dcfd8e-Abstract.html},
  urldate = {2021-09-10},
  keywords = {Bias},
  file = {D\:\\Zotero\\storage\\9T6BKVT6\\Gat et al_2020_Removing Bias in Multi-modal Classifiers.pdf}
}

@unpublished{geBridgingVideotextRetrieval2022,
  title = {Bridging {{Video-text Retrieval}} with {{Multiple Choice Questions}}},
  author = {Ge, Yuying and Ge, Yixiao and Liu, Xihui and Li, Dian and Shan, Ying and Qie, Xiaohu and Luo, Ping},
  date = {2022-03-17},
  number = {arXiv:2201.04850},
  eprint = {2201.04850},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.04850},
  urldate = {2022-05-26},
  abstract = {Pre-training a model to learn transferable video-text representation for retrieval has attracted a lot of attention in recent years. Previous dominant works mainly adopt two separate encoders for efficient retrieval, but ignore local associations between videos and texts. Another line of research uses a joint encoder to interact video with texts, but results in low efficiency since each text-video pair needs to be fed into the model. In this work, we enable fine-grained video-text interactions while maintaining high efficiency for retrieval via a novel pretext task, dubbed as Multiple Choice Questions (MCQ), where a parametric module BridgeFormer is trained to answer the "questions" constructed by the text features via resorting to the video features. Specifically, we exploit the rich semantics of text (i.e., nouns and verbs) to build questions, with which the video encoder can be trained to capture more regional content and temporal dynamics. In the form of questions and answers, the semantic associations between local video-text features can be properly established. BridgeFormer is able to be removed for downstream retrieval, rendering an efficient and flexible model with only two encoders. Our method outperforms state-of-the-art methods on the popular text-to-video retrieval task in five datasets with different experimental setups (i.e., zero-shot and fine-tune), including HowTo100M (one million videos). We further conduct zero-shot action recognition, which can be cast as video-to-text retrieval, and our approach also significantly surpasses its counterparts. As an additional benefit, our method achieves competitive results with much shorter pre-training videos on single-modality downstream tasks, e.g., action recognition with linear evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\WSHIDLBT\\Ge et al_2022_Bridging Video-text Retrieval with Multiple Choice Questions.pdf;D\:\\Zotero\\storage\\N4RJGKIL\\2201.html}
}

@unpublished{geBridgingVideotextRetrieval2022a,
  title = {Bridging {{Video-text Retrieval}} with {{Multiple Choice Questions}}},
  author = {Ge, Yuying and Ge, Yixiao and Liu, Xihui and Li, Dian and Shan, Ying and Qie, Xiaohu and Luo, Ping},
  date = {2022-03-17},
  eprint = {2201.04850},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.04850},
  urldate = {2022-04-14},
  abstract = {Pre-training a model to learn transferable video-text representation for retrieval has attracted a lot of attention in recent years. Previous dominant works mainly adopt two separate encoders for efficient retrieval, but ignore local associations between videos and texts. Another line of research uses a joint encoder to interact video with texts, but results in low efficiency since each text-video pair needs to be fed into the model. In this work, we enable fine-grained video-text interactions while maintaining high efficiency for retrieval via a novel pretext task, dubbed as Multiple Choice Questions (MCQ), where a parametric module BridgeFormer is trained to answer the "questions" constructed by the text features via resorting to the video features. Specifically, we exploit the rich semantics of text (i.e., nouns and verbs) to build questions, with which the video encoder can be trained to capture more regional content and temporal dynamics. In the form of questions and answers, the semantic associations between local video-text features can be properly established. BridgeFormer is able to be removed for downstream retrieval, rendering an efficient and flexible model with only two encoders. Our method outperforms state-of-the-art methods on the popular text-to-video retrieval task in five datasets with different experimental setups (i.e., zero-shot and fine-tune), including HowTo100M (one million videos). We further conduct zero-shot action recognition, which can be cast as video-to-text retrieval, and our approach also significantly surpasses its counterparts. As an additional benefit, our method achieves competitive results with much shorter pre-training videos on single-modality downstream tasks, e.g., action recognition with linear evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\MCBSTTEZ\\Ge et al_2022_Bridging Video-text Retrieval with Multiple Choice Questions.pdf;D\:\\Zotero\\storage\\EIMJKSH6\\2201.html}
}

@inproceedings{gokhaleMUTANTTrainingParadigm2020,
  title = {{{MUTANT}}: {{A Training Paradigm}} for {{Out-of-Distribution Generalization}} in {{Visual Question Answering}}},
  shorttitle = {{{MUTANT}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Gokhale, Tejas and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
  date = {2020-11},
  pages = {878--892},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.63},
  url = {https://aclanthology.org/2020.emnlp-main.63},
  urldate = {2021-09-10},
  abstract = {While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57\% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.},
  eventtitle = {{{EMNLP}} 2020},
  keywords = {Distribution},
  file = {D\:\\Zotero\\storage\\DWKQ8ZUN\\Gokhale et al_2020_MUTANT.pdf}
}

@incollection{gokhaleVQALOLVisualQuestion2020,
  title = {{{VQA-LOL}}: {{Visual Question Answering Under}} the {{Lens}} of {{Logic}}},
  shorttitle = {{{VQA-LOL}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Gokhale, Tejas and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12366},
  pages = {379--396},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58589-1_23},
  url = {https://link.springer.com/10.1007/978-3-030-58589-1_23},
  urldate = {2021-09-10},
  abstract = {Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate whether visual question answering (VQA) systems trained to answer a question about an image, are able to answer the logical composition of multiple such questions. When put under this Lens of Logic, state-of-the-art VQA models have difficulty in correctly answering these logically composed questions. We construct an augmentation of the VQA dataset as a benchmark, with questions containing logical compositions and linguistic transformations (negation, disjunction, conjunction, and antonyms). We propose our Lens of Logic (LOL) model which uses question-attention and logic-attention to understand logical connectives in the question, and a novel Fr´echet-Compatibility Loss, which ensures that the answers of the component questions and the composed question are consistent with the inferred logical operation. Our model shows substantial improvement in learning logical compositions while retaining performance on VQA. We suggest this work as a move towards robustness by embedding logical connectives in visual understanding.},
  isbn = {978-3-030-58588-4 978-3-030-58589-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\K34UNI98\\Gokhale 等。 - 2020 - VQA-LOL Visual Question Answering Under the Lens .pdf}
}

@article{gomezMultimodalGridFeatures2021,
  title = {Multimodal Grid Features and Cell Pointers for Scene Text Visual Question Answering},
  author = {Gómez, Lluís and Biten, Ali Furkan and Tito, Rubén and Mafla, Andrés and Rusiñol, Marçal and Valveny, Ernest and Karatzas, Dimosthenis},
  date = {2021-10},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {150},
  pages = {242--249},
  issn = {01678655},
  doi = {10.1016/j.patrec.2021.06.026},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865521002336},
  urldate = {2022-02-10},
  langid = {english}
}

@article{gordonIQAVisualQuestion,
  title = {{{IQA}}: {{Visual Question Answering}} in {{Interactive Environments}}},
  author = {Gordon, Daniel and Kembhavi, Aniruddha and Rastegari, Mohammad and Redmon, Joseph and Fox, Dieter and Farhadi, Ali},
  pages = {10},
  abstract = {We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: “Are there any apples in the fridge?” The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2THOR [35], a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98.},
  langid = {english},
  annotation = {00000},
  file = {D\:\\Zotero\\storage\\D8AA5NKM\\Gordon 等。 - IQA Visual Question Answering in Interactive Envi.pdf}
}

@article{goyalMakingVQAMatter,
  title = {Making the v in {{VQA Matter}}: {{Elevating}} the {{Role}} of {{Image Understanding}} in {{Visual Question Answering}}},
  author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  pages = {10},
  abstract = {Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {D\:\\Zotero\\storage\\H5ELWT8D\\Goyal 等。 - Making the v in VQA Matter Elevating the Role of .pdf}
}

@article{goyalMakingVQAMattera,
  title = {Making the v in {{VQA Matter}}: {{Elevating}} the {{Role}} of {{Image Understanding}} in {{Visual Question Answering}}},
  author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  pages = {10},
  abstract = {Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\WNTCPRWD\\Goyal 等。 - Making the v in VQA Matter Elevating the Role of .pdf}
}

@article{goyalMakingVQAMatterb,
  title = {Making the v in {{VQA Matter}}: {{Elevating}} the {{Role}} of {{Image Understanding}} in {{Visual Question Answering}}},
  author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  pages = {10},
  abstract = {Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.},
  langid = {english}
}

@article{GraphConvolutionalMatrix2018,
  title = {Graph {{Convolutional Matrix Completion}}},
  date = {2018},
  pages = {7},
  abstract = {We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. This representation is especially useful in the setting where additional graph-based side information is present. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods. Furthermore, to validate the proposed message passing scheme, we test our model on standard collaborative filtering tasks and show competitive results.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\LDRR8H3I\\2018 - Graph Convolutional Matrix Completion.pdf}
}

@inproceedings{grecoPsycholinguisticsMeetsContinual2019,
  title = {Psycholinguistics {{Meets Continual Learning}}: {{Measuring Catastrophic Forgetting}} in {{Visual Question Answering}}},
  shorttitle = {Psycholinguistics {{Meets Continual Learning}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Greco, Claudio and Plank, Barbara and Fernández, Raquel and Bernardi, Raffaella},
  date = {2019-07},
  pages = {3601--3605},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1350},
  url = {https://aclanthology.org/P19-1350},
  urldate = {2021-09-10},
  abstract = {We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.},
  eventtitle = {{{ACL}} 2019},
  file = {D\:\\Zotero\\storage\\L5HUF8WY\\Greco et al_2019_Psycholinguistics Meets Continual Learning.pdf}
}

@unpublished{grunde-mclaughlinAGQABenchmarkCompositional2021,
  title = {{{AGQA}}: {{A Benchmark}} for {{Compositional Spatio-Temporal Reasoning}}},
  shorttitle = {{{AGQA}}},
  author = {Grunde-McLaughlin, Madeleine and Krishna, Ranjay and Agrawala, Maneesh},
  date = {2021-03-29},
  eprint = {2103.16002},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.16002},
  urldate = {2021-09-23},
  abstract = {Visual events are a composition of temporal actions involving actors spatially interacting with objects. When developing computer vision models that can reason about compositional spatio-temporal events, we need benchmarks that can analyze progress and uncover shortcomings. Existing video question answering benchmarks are useful, but they often conflate multiple sources of error into one accuracy metric and have strong biases that models can exploit, making it difficult to pinpoint model weaknesses. We present Action Genome Question Answering (AGQA), a new benchmark for compositional spatio-temporal reasoning. AGQA contains \$192M\$ unbalanced question answer pairs for \$9.6K\$ videos. We also provide a balanced subset of \$3.9M\$ question answer pairs, \$3\$ orders of magnitude larger than existing benchmarks, that minimizes bias by balancing the answer distributions and types of question structures. Although human evaluators marked \$86.02\textbackslash\%\$ of our question-answer pairs as correct, the best model achieves only \$47.74\textbackslash\%\$ accuracy. In addition, AGQA introduces multiple training/test splits to test for various reasoning abilities, including generalization to novel compositions, to indirect references, and to more compositional steps. Using AGQA, we evaluate modern visual reasoning systems, demonstrating that the best models barely perform better than non-visual baselines exploiting linguistic biases and that none of the existing models generalize to novel compositions unseen during training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\A5M3IWE8\\Grunde-McLaughlin et al_2021_AGQA.pdf;D\:\\Zotero\\storage\\HM6W22KV\\2103.html}
}

@article{guGraphBasedMultiInteractionNetwork2021,
  title = {Graph-{{Based Multi-Interaction Network}} for {{Video Question Answering}}},
  author = {Gu, Mao and Zhao, Zhou and Jin, Weike and Hong, Richang and Wu, Fei},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {30},
  pages = {2758--2770},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2021.3051756},
  url = {https://ieeexplore.ieee.org/document/9332277/},
  urldate = {2022-03-21}
}

@article{guGraphBasedMultiInteractionNetwork2021a,
  title = {Graph-{{Based Multi-Interaction Network}} for {{Video Question Answering}}},
  author = {Gu, Mao and Zhao, Zhou and Jin, Weike and Hong, Richang and Wu, Fei},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {30},
  pages = {2758--2770},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2021.3051756},
  url = {https://ieeexplore.ieee.org/document/9332277/},
  urldate = {2022-02-05},
  abstract = {Video question answering is an important task combining both Natural Language Processing and Computer Vision, which requires a machine to obtain a thorough understanding of the video. Most existing approaches simply capture spatio-temporal information in videos by using a combination of recurrent and convolutional neural networks. Nonetheless, most previous work focus on only salient frames or regions, which normally lacks some significant details, such as potential location and action relations. In this paper, we propose a new method called Graph-based Multi-interaction Network for video question answering. In our model, a new attention mechanism named multi-interaction is designed to capture both element-wise and segment-wise sequence interactions simultaneously, which can be found between and inside the multi-modal inputs. Moreover, we propose a graph-based relation-aware neural network to explore a more fine-grained visual representation, which could explore the relationships and dependencies between objects spatially and temporally. We evaluate our method on TGIF-QA and other two video QA datasets. The qualitative and quantitative experimental results show the effectiveness of our model, which achieves state-of-the-art performance.},
  langid = {english}
}

@unpublished{guInterpretableGraphCapsule2021,
  title = {Interpretable {{Graph Capsule Networks}} for {{Object Recognition}}},
  author = {Gu, Jindong and Tresp, Volker},
  date = {2021-03-07},
  eprint = {2012.01674},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.01674},
  urldate = {2021-10-04},
  abstract = {Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed to recognize objects from images. The current literature demonstrates many advantages of CapsNets over CNNs. However, how to create explanations for individual classifications of CapsNets has not been well explored. The widely used saliency methods are mainly proposed for explaining CNN-based classifications; they create saliency map explanations by combining activation values and the corresponding gradients, e.g., Grad-CAM. These saliency methods require a specific architecture of the underlying classifiers and cannot be trivially applied to CapsNets due to the iterative routing mechanism therein. To overcome the lack of interpretability, we can either propose new post-hoc interpretation methods for CapsNets or modifying the model to have build-in explanations. In this work, we explore the latter. Specifically, we propose interpretable Graph Capsule Networks (GraCapsNets), where we replace the routing part with a multi-head attention-based Graph Pooling approach. In the proposed model, individual classification explanations can be created effectively and efficiently. Our model also demonstrates some unexpected benefits, even though it replaces the fundamental part of CapsNets. Our GraCapsNets achieve better classification performance with fewer parameters and better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets also keep other advantages of CapsNets, namely, disentangled representations and affine transformation robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\8JNNRI9R\\2012.html}
}

@inproceedings{guoDualVisualAttention2019,
  title = {Dual {{Visual Attention Network}} for {{Visual Dialog}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Guo, Dan and Wang, Hui and Wang, Meng},
  date = {2019-08},
  pages = {4989--4995},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Macao, China}},
  doi = {10.24963/ijcai.2019/693},
  url = {https://www.ijcai.org/proceedings/2019/693},
  urldate = {2021-12-08},
  abstract = {Visual dialog is a challenging task, which involves multi-round semantic transformations between vision and language. This paper aims to address cross-modal semantic correlation for visual dialog. Motivated by that Vg (global vision), Vl (local vision), Q (question) and H (history) have inseparable relevances, the paper proposes a novel Dual Visual Attention Network (DVAN) to realize (Vg, Vl, Q, H) ⇒ A. DVAN is a three-stage queryadaptive attention model. In order to acquire accurate A (answer), it first explores the textual attention, which imposes the question on history to pick out related context H . Then, based on Q and H , it implements respective visual attentions to discover related global image visual hints Vg and local object-based visual hints Vl . Next, a dual crossing visual attention is proposed. Vg and Vl are mutually embedded to learn the complementary of visual semantics. Finally, the attended textual and visual features are combined to infer the answer. Experimental results on the VisDial v0.9 and v1.0 datasets validate the effectiveness of the proposed approach.},
  eventtitle = {Twenty-{{Eighth International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-19}}\vphantom\{\}},
  isbn = {978-0-9992411-4-1},
  langid = {english}
}

@unpublished{guoImageQuestionAnswerSynergisticNetwork2019,
  title = {Image-{{Question-Answer Synergistic Network}} for {{Visual Dialog}}},
  author = {Guo, Dalu and Xu, Chang and Tao, Dacheng},
  date = {2019-02-26},
  eprint = {1902.09774},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.09774},
  urldate = {2021-12-08},
  abstract = {The image, question (combined with the history for de-referencing), and the corresponding answer are three vital components of visual dialog. Classical visual dialog systems integrate the image, question, and history to search for or generate the best matched answer, and so, this approach significantly ignores the role of the answer. In this paper, we devise a novel image-question-answer synergistic network to value the role of the answer for precise visual dialog. We extend the traditional one-stage solution to a two-stage solution. In the first stage, candidate answers are coarsely scored according to their relevance to the image and question pair. Afterward, in the second stage, answers with high probability of being correct are re-ranked by synergizing with image and question. On the Visual Dialog v1.0 dataset, the proposed synergistic network boosts the discriminative visual dialog model to achieve a new state-of-the-art of 57.88\textbackslash\% normalized discounted cumulative gain. A generative visual dialog model equipped with the proposed technique also shows promising improvements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\NIEVW3YT\\Guo et al_2019_Image-Question-Answer Synergistic Network for Visual Dialog.pdf;D\:\\Zotero\\storage\\7FN8PUYG\\1902.html}
}

@unpublished{guoImageQuestionAnswerSynergisticNetwork2019a,
  title = {Image-{{Question-Answer Synergistic Network}} for {{Visual Dialog}}},
  author = {Guo, Dalu and Xu, Chang and Tao, Dacheng},
  date = {2019-02-26},
  eprint = {1902.09774},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.09774},
  urldate = {2021-09-09},
  abstract = {The image, question (combined with the history for de-referencing), and the corresponding answer are three vital components of visual dialog. Classical visual dialog systems integrate the image, question, and history to search for or generate the best matched answer, and so, this approach significantly ignores the role of the answer. In this paper, we devise a novel image-question-answer synergistic network to value the role of the answer for precise visual dialog. We extend the traditional one-stage solution to a two-stage solution. In the first stage, candidate answers are coarsely scored according to their relevance to the image and question pair. Afterward, in the second stage, answers with high probability of being correct are re-ranked by synergizing with image and question. On the Visual Dialog v1.0 dataset, the proposed synergistic network boosts the discriminative visual dialog model to achieve a new state-of-the-art of 57.88\textbackslash\% normalized discounted cumulative gain. A generative visual dialog model equipped with the proposed technique also shows promising improvements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Visual Dialog},
  file = {D\:\\Zotero\\storage\\RC9E9CN8\\Guo et al_2019_Image-Question-Answer Synergistic Network for Visual Dialog.pdf;D\:\\Zotero\\storage\\HG7JUW5U\\1902.html}
}

@unpublished{guoIterativeContextAwareGraph2020,
  title = {Iterative {{Context-Aware Graph Inference}} for {{Visual Dialog}}},
  author = {Guo, Dan and Wang, Hui and Zhang, Hanwang and Zha, Zheng-Jun and Wang, Meng},
  date = {2020-04-05},
  eprint = {2004.02194},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.02194},
  urldate = {2022-03-06},
  abstract = {Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-\$K\$ message passing mechanism. Specifically, in every message passing step, each node selects the most \$K\$ relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related \$K\$ neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\4989N6RF\\Guo et al_2020_Iterative Context-Aware Graph Inference for Visual Dialog.pdf;D\:\\Zotero\\storage\\29L6TDLJ\\2004.html}
}

@inproceedings{guoIterativeContextAwareGraph2020a,
  title = {Iterative {{Context-Aware Graph Inference}} for {{Visual Dialog}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Guo, Dan and Wang, Hui and Zhang, Hanwang and Zha, Zheng-Jun and Wang, Meng},
  date = {2020-06},
  pages = {10052--10061},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01007},
  url = {https://ieeexplore.ieee.org/document/9157701/},
  urldate = {2021-11-09},
  abstract = {Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both objectbased (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@inproceedings{guoQuantifyingAlleviatingLanguage2019,
  title = {Quantifying and {{Alleviating}} the {{Language Prior Problem}} in {{Visual Question Answering}}},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and Liu, Yibing and Wang, Yinglong and Kankanhalli, Mohan},
  date = {2019-07-18},
  pages = {75--84},
  publisher = {{ACM}},
  location = {{Paris France}},
  doi = {10.1145/3331184.3331186},
  url = {https://dl.acm.org/doi/10.1145/3331184.3331186},
  urldate = {2021-09-09},
  abstract = {Benefiting from the advancement of computer vision, natural language processing and information retrieval techniques, visual question answering (VQA), which aims to answer questions about an image or a video, has received lots of attentions over the past few years. Although some progress has been achieved so far, several studies have pointed out that current VQA models are heavily affected by the language prior problem, which means they tend to answer questions based on the co-occurrence patterns of question keywords (e.g., how many) and answers (e.g., 2) instead of understanding images and questions. Existing methods attempt to solve this problem by either balancing the biased datasets or forcing models to better understand images. However, only marginal effects and even performance deterioration are observed for the first and second solution, respectively. In addition, another important issue is the lack of measurement to quantitatively measure the extent of the language prior effect, which severely hinders the advancement of related techniques.},
  eventtitle = {{{SIGIR}} '19: {{The}} 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-6172-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4HCZL5C9\\Guo 等。 - 2019 - Quantifying and Alleviating the Language Prior Pro.pdf}
}

@article{guoReAttentionVisualQuestion,
  title = {Re-{{Attention}} for {{Visual Question Answering}}},
  author = {Guo, Wenya and Zhang, Ying and Wu, Xiaoping and Yang, Jufeng and Cai, Xiangrui and Yuan, Xiaojie},
  pages = {8},
  abstract = {Visual Question Answering (VQA) requires a simultaneous understanding of images and questions. Existing methods achieve well performance by focusing on both key objects in images and key words in questions. However, the answer also contains rich information which can help to better describe the image and generate more accurate attention maps. In this paper, to utilize the information in answer, we propose a reattention framework for the VQA task. We first associate image and question by calculating the similarity of each objectword pairs in the feature space. Then, based on the answer, the learned model re-attends the corresponding visual objects in images and reconstructs the initial attention map to produce consistent results. Benefiting from the re-attention procedure, the question can be better understood, and the satisfactory answer is generated. Extensive experiments on the benchmark dataset demonstrate the proposed method performs favorably against the state-of-the-art approaches.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZT6VXTBR\\Guo 等。 - Re-Attention for Visual Question Answering.pdf}
}

@article{guoTextualVisualReferenceAwareAttention2020,
  title = {Textual-{{Visual Reference-Aware Attention Network}} for {{Visual Dialog}}},
  author = {Guo, Dan and Wang, Hui and Wang, Shuhui and Wang, Meng},
  date = {2020},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {29},
  pages = {6655--6666},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2020.2992888},
  url = {https://ieeexplore.ieee.org/document/9095243/},
  urldate = {2021-11-22},
  abstract = {Visual dialog is a challenging task in multimedia understanding, which requires the dialog agent to answer a series of questions that are based on an input image. The critical issue to produce an exact answer is how to model the mutual semantic interaction among feature representations of the image, questionanswer history, and current question. In this study, we propose a textual-visual Reference-Aware Attention Network (RAA-Net), which aims to effectively fuse Q (question), H (history), Vl (local vision), and Vg (global vision) to infer the exact answer. In the multimodal feature flows, RAA-Net first learns the textual context through multi-head attention between Q and H and then guides the textual reference semantics to the image to capture visual reference semantics by self- and cross- reference-aware attention in and between Vl and Vg. In the proposed RAANet, we exploit the two-stage (intra- and inter-) visual reasoning mechanism on Vl and Vg. Extensive experiments on the VisDial v0.9 and v1.0 datasets show that RAA-Net achieves state-of-theart performance. Visualization results on both visual and textual attention maps further validate the remarkable interpretability achieved by our solution.},
  langid = {english}
}

@article{guoTextualVisualReferenceAwareAttention2020a,
  title = {Textual-{{Visual Reference-Aware Attention Network}} for {{Visual Dialog}}},
  author = {Guo, Dan and Wang, Hui and Wang, Shuhui and Wang, Meng},
  date = {2020},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {29},
  pages = {6655--6666},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2020.2992888},
  url = {https://ieeexplore.ieee.org/document/9095243/},
  urldate = {2021-11-17},
  abstract = {Visual dialog is a challenging task in multimedia understanding, which requires the dialog agent to answer a series of questions that are based on an input image. The critical issue to produce an exact answer is how to model the mutual semantic interaction among feature representations of the image, questionanswer history, and current question. In this study, we propose a textual-visual Reference-Aware Attention Network (RAA-Net), which aims to effectively fuse Q (question), H (history), Vl (local vision), and Vg (global vision) to infer the exact answer. In the multimodal feature flows, RAA-Net first learns the textual context through multi-head attention between Q and H and then guides the textual reference semantics to the image to capture visual reference semantics by self- and cross- reference-aware attention in and between Vl and Vg. In the proposed RAANet, we exploit the two-stage (intra- and inter-) visual reasoning mechanism on Vl and Vg. Extensive experiments on the VisDial v0.9 and v1.0 datasets show that RAA-Net achieves state-of-theart performance. Visualization results on both visual and textual attention maps further validate the remarkable interpretability achieved by our solution.},
  langid = {english}
}

@article{guptaHierarchicalDeepMultimodal2021,
  title = {Hierarchical Deep Multi-Modal Network for Medical Visual Question Answering},
  author = {Gupta, Deepak and Suman, Swati and Ekbal, Asif},
  date = {2021-02},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {164},
  pages = {113993},
  issn = {09574174},
  doi = {10.1016/j.eswa.2020.113993},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420307697},
  urldate = {2021-12-24},
  abstract = {Visual Question Answering in Medical domain (VQA-Med) plays an important role in providing medical assis­ tance to the end-users. These users are expected to raise either a straightforward question with a Yes/No answer or a challenging question that requires a detailed and descriptive answer. The existing techniques in VQA-Med fail to distinguish between the different question types sometimes complicates the simpler problems, or oversimplifies the complicated ones. It is certainly true that for different question types, several distinct systems can lead to confusion and discomfort for the end-users. To address this issue, we propose a hierarchical deep multi-modal network that analyzes and classifies end-user questions/queries and then incorporates a queryspecific approach for answer prediction. We refer our proposed approach as Hierarchical Question Segrega­ tion based Visual Question Answering, in short HQS-VQA. Our contributions are three-fold, viz. firstly, we propose a question segregation (QS) technique for VQA-Med; secondly, we integrate the QS model to the hier­ archical deep multi-modal neural network to generate proper answers to the queries related to medical images; and thirdly, we study the impact of QS in Medical-VQA by comparing the performance of the proposed model with QS and a model without QS. We evaluate the performance of our proposed model on two benchmark datasets, viz. RAD and CLEF18. Experimental results show that our proposed HQS-VQA technique outperforms the baseline models with significant margins. We also conduct a detailed quantitative and qualitative analysis of the obtained results and discover potential causes of errors and their solutions.},
  langid = {english}
}

@unpublished{guptaSwapMixDiagnosingRegularizing2022,
  title = {{{SwapMix}}: {{Diagnosing}} and {{Regularizing}} the {{Over-Reliance}} on {{Visual Context}} in {{Visual Question Answering}}},
  shorttitle = {{{SwapMix}}},
  author = {Gupta, Vipul and Li, Zhuowan and Kortylewski, Adam and Zhang, Chenyu and Li, Yingwei and Yuille, Alan},
  date = {2022-04-05},
  number = {arXiv:2204.02285},
  eprint = {2204.02285},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.02285},
  urldate = {2022-05-26},
  abstract = {While Visual Question Answering (VQA) has progressed rapidly, previous works raise concerns about robustness of current VQA models. In this work, we study the robustness of VQA models from a novel perspective: visual context. We suggest that the models over-rely on the visual context, i.e., irrelevant objects in the image, to make predictions. To diagnose the model's reliance on visual context and measure their robustness, we propose a simple yet effective perturbation technique, SwapMix. SwapMix perturbs the visual context by swapping features of irrelevant context objects with features from other objects in the dataset. Using SwapMix we are able to change answers to more than 45 \% of the questions for a representative VQA model. Additionally, we train the models with perfect sight and find that the context over-reliance highly depends on the quality of visual representations. In addition to diagnosing, SwapMix can also be applied as a data augmentation strategy during training in order to regularize the context over-reliance. By swapping the context object features, the model reliance on context can be suppressed effectively. Two representative VQA models are studied using SwapMix: a co-attention model MCAN and a large-scale pretrained model LXMERT. Our experiments on the popular GQA dataset show the effectiveness of SwapMix for both diagnosing model robustness and regularizing the over-reliance on visual context. The code for our method is available at https://github.com/vipulgupta1011/swapmix},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\FF35GYB5\\Gupta et al_2022_SwapMix.pdf;D\:\\Zotero\\storage\\8BC972C5\\2204.html}
}

@unpublished{guptaSyntheticDataText2016,
  title = {Synthetic {{Data}} for {{Text Localisation}} in {{Natural Images}}},
  author = {Gupta, Ankush and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2016-04-22},
  eprint = {1604.06646},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1604.06646},
  urldate = {2022-02-10},
  abstract = {In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter. This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2\% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\243ZDP5P\\Gupta et al_2016_Synthetic Data for Text Localisation in Natural Images.pdf;D\:\\Zotero\\storage\\GF6LZET9\\1604.html}
}

@unpublished{guptaZeroShotSketchBased2022,
  title = {Zero-{{Shot Sketch Based Image Retrieval}} Using {{Graph Transformer}}},
  author = {Gupta, Sumrit and Chaudhuri, Ushasi and Banerjee, Biplab},
  date = {2022-01-25},
  eprint = {2201.10185},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.10185},
  urldate = {2022-03-26},
  abstract = {The performance of a zero-shot sketch-based image retrieval (ZS-SBIR) task is primarily affected by two challenges. The substantial domain gap between image and sketch features needs to be bridged, while at the same time the side information has to be chosen tactfully. Existing literature has shown that varying the semantic side information greatly affects the performance of ZS-SBIR. To this end, we propose a novel graph transformer based zero-shot sketch-based image retrieval (GTZSR) framework for solving ZS-SBIR tasks which uses a novel graph transformer to preserve the topology of the classes in the semantic space and propagates the context-graph of the classes within the embedding features of the visual space. To bridge the domain gap between the visual features, we propose minimizing the Wasserstein distance between images and sketches in a learned domain-shared space. We also propose a novel compatibility loss that further aligns the two visual domains by bridging the domain gap of one class with respect to the domain gap of all other classes in the training set. Experimental results obtained on the extended Sketchy, TU-Berlin, and QuickDraw datasets exhibit sharp improvements over the existing state-of-the-art methods in both ZS-SBIR and generalized ZS-SBIR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\9PUUVBAH\\Gupta et al_2022_Zero-Shot Sketch Based Image Retrieval using Graph Transformer.pdf;D\:\\Zotero\\storage\\43CIN3SN\\2201.html}
}

@inproceedings{gurariVizWizGrandChallenge2018,
  title = {{{VizWiz Grand Challenge}}: {{Answering Visual Questions}} from {{Blind People}}},
  shorttitle = {{{VizWiz Grand Challenge}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Gurari, Danna and Li, Qing and Stangl, Abigale J. and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P.},
  date = {2018-06},
  pages = {3608--3617},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00380},
  url = {https://ieeexplore.ieee.org/document/8578478/},
  urldate = {2021-09-09},
  abstract = {The study of algorithms to automatically answer visual questions currently is motivated by visual question answering (VQA) datasets constructed in artificial VQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising from a natural VQA setting. VizWiz consists of over 31,000 visual questions originating from blind people who each took a picture using a mobile phone and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. VizWiz differs from the many existing VQA datasets because (1) images are captured by blind photographers and so are often poor quality, (2) questions are spoken and so are more conversational, and (3) often visual questions cannot be answered. Evaluation of modern algorithms for answering visual questions and deciding if a visual question is answerable reveals that VizWiz is a challenging dataset. We introduce this dataset to encourage a larger community to develop more generalized algorithms that can assist blind people.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {64 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\PEAG4PWB\\Gurari 等。 - 2018 - VizWiz Grand Challenge Answering Visual Questions.pdf}
}

@article{haiderActiveDataRepresentation2020,
  title = {An {{Active Data Representation}} of {{Videos}} for {{Automatic Scoring}} of {{Oral Presentation Delivery Skills}} and {{Feedback Generation}}},
  author = {Haider, Fasih and Koutsombogera, Maria and Conlan, Owen and Vogel, Carl and Campbell, Nick and Luz, Saturnino},
  date = {2020-01-28},
  journaltitle = {Frontiers in Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {2},
  pages = {1},
  issn = {2624-9898},
  doi = {10.3389/fcomp.2020.00001},
  url = {https://www.frontiersin.org/article/10.3389/fcomp.2020.00001/full},
  urldate = {2022-05-23},
  abstract = {Public speaking is an important skill, the acquisition of which requires dedicated and time consuming training. In recent years, researchers have started to investigate automatic methods to support public speaking skills training. These methods include assessment of the trainee’s oral presentation delivery skills which may be accomplished through automatic understanding and processing of social and behavioral cues displayed by the presenter. In this study, we propose an automatic scoring system for presentation delivery skills using a novel active data representation method to automatically rate segments of a full video presentation. While most approaches have employed a two step strategy consisting of detecting multiple events followed by classification, which involve the annotation of data for building the different event detectors and generating a data representation based on their output for classification, our method does not require event detectors. The proposed data representation is generated unsupervised using low-level audiovisual descriptors and self-organizing mapping and used for video classification. This representation is also used to analyse video segments within a full video presentation in terms of several characteristics of the presenter’s performance. The audio representation provides the best prediction results for self-confidence and enthusiasm, posture and body language, structure and connection of ideas, and overall presentation delivery. The video data representation provides the best results for presentation of relevant information with good pronunciation, usage of language according to audience, and maintenance of adequate voice volume for the audience. The fusion of audio and video data provides the best results for eye contact. Applications of the method to provision of feedback to teachers and trainees are discussed.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\EJULFYLR\\Haider 等。 - 2020 - An Active Data Representation of Videos for Automa.pdf}
}

@article{hallProfessionalDifferencesComparative2022,
  title = {Professional {{Differences}}: {{A Comparative Study}} of {{Visualization Task Performance}} and {{Spatial Ability Across Disciplines}}},
  shorttitle = {Professional {{Differences}}},
  author = {Hall, Kyle Wm. and Kouroupis, Anthony and Bezerianos, Anastasia and Szafir, Danielle Albers and Collins, Christopher},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  volume = {28},
  number = {1},
  pages = {654--664},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2021.3114805},
  url = {https://ieeexplore.ieee.org/document/9572234/},
  urldate = {2022-05-17},
  abstract = {Problem-driven visualization work is rooted in deeply understanding the data, actors, processes, and workflows of a target domain. However, an individual’s personality traits and cognitive abilities may also influence visualization use. Diverse user needs and abilities raise natural questions for specificity in visualization design: Could individuals from different domains exhibit performance differences when using visualizations? Are any systematic variations related to their cognitive abilities? This study bridges domain-specific perspectives on visualization design with those provided by cognition and perception. We measure variations in visualization task performance across chemistry, computer science, and education, and relate these differences to variations in spatial ability. We conducted an online study with over 60 domain experts consisting of tasks related to pie charts, isocontour plots, and 3D scatterplots, and grounded by a well-documented spatial ability test. Task performance (correctness) varied with profession across more complex visualizations (isocontour plots and scatterplots), but not pie charts, a comparatively common visualization. We found that correctness correlates with spatial ability, and the professions differ in terms of spatial ability. These results indicate that domains differ not only in the specifics of their data and tasks, but also in terms of how effectively their constituent members engage with visualizations and their cognitive traits. Analyzing participants’ confidence and strategy comments suggests that focusing on performance neglects important nuances, such as differing approaches to engage with even common visualizations and potential skill transference. Our findings offer a fresh perspective on discipline-specific visualization with specific recommendations to help guide visualization design that celebrates the uniqueness of the disciplines and individuals we seek to serve.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\XYUNUD63\\Hall 等。 - 2022 - Professional Differences A Comparative Study of V.pdf}
}

@inproceedings{hanExploreMultiStepReasoning2018,
  title = {Explore {{Multi-Step Reasoning}} in {{Video Question Answering}}},
  booktitle = {Proceedings of the 1st {{Workshop}} and {{Challenge}} on {{Comprehensive Video Understanding}} in the {{Wild}}},
  author = {Han, Yahong},
  date = {2018-10-15},
  pages = {5--5},
  publisher = {{ACM}},
  location = {{Seoul Republic of Korea}},
  doi = {10.1145/3265987.3265996},
  url = {https://dl.acm.org/doi/10.1145/3265987.3265996},
  urldate = {2021-03-11},
  abstract = {This invited talk is a repeated but more detailed talk about the paper which is accepted by ACM-MM 2018: Video question answering (VideoQA) always involves visual reasoning. When answering questions composing of multiple logic correlations, models need to perform multi-step reasoning. In this paper, we formulate multistep reasoning in VideoQA as a new task to answer compositional and logical structured questions based on video content. Existing VideoQA datasets are inadequate as benchmarks for the multi-step reasoning due to limitations as lacking logical structure and having language biases. Thus we design a system to automatically generate a large-scale dataset, namely SVQA (Synthetic Video Question Answering). Compared with other VideoQA datasets, SVQA contains exclusively long and structured questions with various spatial and temporal relations between objects. More importantly, questions in SVQA can be decomposed into human readable logical tree or chain layouts, each node of which represents a sub-task requiring a reasoning operation such as comparison or arithmetic. Towards automatic question answering in SVQA, we develop a new VideoQA model. Particularly, we construct a new attention module, which contains spatial attention mechanism to address crucial and multiple logical sub-tasks embedded in questions, as well as a refined GRU called ta-GRU (temporal-attention GRU) to capture the long-term temporal dependency and gather complete visual cues. Experimental results show the capability of multi-step reasoning of SVQA and the effectiveness of our model when compared with other existing models.},
  eventtitle = {{{MM}} '18: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-5976-4},
  langid = {english},
  file = {D\:\\Zotero\\storage\\93G4AEBQ\\Han - 2018 - Explore Multi-Step Reasoning in Video Question Ans.pdf}
}

@unpublished{hanFindingEvidenceLocalizationaware2020,
  title = {Finding the {{Evidence}}: {{Localization-aware Answer Prediction}} for {{Text Visual Question Answering}}},
  shorttitle = {Finding the {{Evidence}}},
  author = {Han, Wei and Huang, Hantao and Han, Tao},
  date = {2020-10-06},
  eprint = {2010.02582},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.02582},
  urldate = {2021-12-08},
  abstract = {Image text carries essential information to understand the scene and perform reasoning. Text-based visual question answering (text VQA) task focuses on visual questions that require reading text in images. Existing text VQA systems generate an answer by selecting from optical character recognition (OCR) texts or a fixed vocabulary. Positional information of text is underused and there is a lack of evidence for the generated answer. As such, this paper proposes a localization-aware answer prediction network (LaAP-Net) to address this challenge. Our LaAP-Net not only generates the answer to the question but also predicts a bounding box as evidence of the generated answer. Moreover, a context-enriched OCR representation (COR) for multimodal fusion is proposed to facilitate the localization task. Our proposed LaAP-Net outperforms existing approaches on three benchmark datasets for the text VQA task by a noticeable margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\8DNKQPLX\\Han et al_2020_Finding the Evidence.pdf;D\:\\Zotero\\storage\\YJ227LWM\\2010.html}
}

@inproceedings{hanGreedyGradientEnsemble2021,
  title = {Greedy {{Gradient Ensemble}} for {{Robust Visual Question Answering}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Han, Xinzhe and Wang, Shuhui and Su, Chi and Huang, Qingming and Tian, Qi},
  date = {2021-10},
  pages = {1564--1573},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00161},
  url = {https://ieeexplore.ieee.org/document/9711436/},
  urldate = {2022-04-20},
  abstract = {Language bias is a critical issue in Visual Question Answering (VQA), where models often exploit dataset biases for the final decision without considering the image information. As a result, they suffer from performance drop on out-of-distribution data and inadequate visual explanation. Based on experimental analysis for existing robust VQA methods, we stress the language bias in VQA that comes from two aspects, i.e., distribution bias and shortcut bias. We further propose a new de-bias framework, Greedy Gradient Ensemble (GGE), which combines multiple biased models for unbiased base model learning. With the greedy strategy, GGE forces the biased models to over-fit the biased data distribution in priority, thus makes the base model pay more attention to examples that are hard to solve by biased models. The experiments demonstrate that our method makes better use of visual information and achieves state-of-the-art performance on diagnosing dataset VQACP without using extra annotations.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\Q9PNV8NG\\Han 等。 - 2021 - Greedy Gradient Ensemble for Robust Visual Questio.pdf}
}

@incollection{hanInterpretableVisualReasoning2020,
  title = {Interpretable {{Visual Reasoning}} via {{Probabilistic Formulation Under Natural Supervision}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Han, Xinzhe and Wang, Shuhui and Su, Chi and Zhang, Weigang and Huang, Qingming and Tian, Qi},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12354},
  pages = {553--570},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58545-7_32},
  url = {https://link.springer.com/10.1007/978-3-030-58545-7_32},
  urldate = {2021-09-17},
  abstract = {Visual reasoning is crucial for visual question answering (VQA). However, without labelled programs, implicit reasoning under natural supervision is still quite challenging and previous models are hard to interpret. In this paper, we rethink implicit reasoning process in VQA, and propose a new formulation which maximizes the log-likelihood of joint distribution for the observed question and predicted answer. Accordingly, we derive a Temporal Reasoning Network (TRN) framework which models the implicit reasoning process as sequential planning in latent space. Our model is interpretable on both model design in probabilist and reasoning process via visualization. We experimentally demonstrate that TRN can support implicit reasoning across various datasets. The experimental results of our model are competitive to existing implicit reasoning models and surpass baseline by a large margin on complicated reasoning tasks without extra computation cost in forward stage.},
  isbn = {978-3-030-58544-0 978-3-030-58545-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4KS2NERJ\\Han 等。 - 2020 - Interpretable Visual Reasoning via Probabilistic F.pdf}
}

@misc{hanSemanticawareModularCapsule2022,
  title = {Semantic-Aware {{Modular Capsule Routing}} for {{Visual Question Answering}}},
  author = {Han, Yudong and Yin, Jianhua and Wu, Jianlong and Wei, Yinwei and Nie, Liqiang},
  date = {2022-07-21},
  number = {arXiv:2207.10404},
  eprint = {2207.10404},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.10404},
  urldate = {2022-07-30},
  abstract = {Visual Question Answering (VQA) is fundamentally compositional in nature, and many questions are simply answered by decomposing them into modular sub-problems. The recent proposed Neural Module Network (NMN) employ this strategy to question answering, whereas heavily rest with off-the-shelf layout parser or additional expert policy regarding the network architecture design instead of learning from the data. These strategies result in the unsatisfactory adaptability to the semantically-complicated variance of the inputs, thereby hindering the representational capacity and generalizability of the model. To tackle this problem, we propose a Semantic-aware modUlar caPsulE Routing framework, termed as SUPER, to better capture the instance-specific vision-semantic characteristics and refine the discriminative representations for prediction. Particularly, five powerful specialized modules as well as dynamic routers are tailored in each layer of the SUPER network, and the compact routing spaces are constructed such that a variety of customizable routes can be sufficiently exploited and the vision-semantic representations can be explicitly calibrated. We comparatively justify the effectiveness and generalization ability of our proposed SUPER scheme over five benchmark datasets, as well as the parametric-efficient advantage. It is worth emphasizing that this work is not to pursue the state-of-the-art results in VQA. Instead, we expect that our model is responsible to provide a novel perspective towards architecture learning and representation calibration for VQA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.2.10},
  file = {D\:\\Zotero\\storage\\VQNB7JR3\\Han et al_2022_Semantic-aware Modular Capsule Routing for Visual Question Answering.pdf;D\:\\Zotero\\storage\\JIVBNHKN\\2207.html}
}

@inproceedings{hanShapeCaptionerGenerativeCaption2020,
  title = {{{ShapeCaptioner}}: {{Generative Caption Network}} for {{3D Shapes}} by {{Learning}} a {{Mapping}} from {{Parts Detected}} in {{Multiple Views}} to {{Sentences}}},
  shorttitle = {{{ShapeCaptioner}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Han, Zhizhong and Chen, Chao and Liu, Yu-Shen and Zwicker, Matthias},
  date = {2020-10-12},
  pages = {1018--1027},
  publisher = {{ACM}},
  location = {{Seattle WA USA}},
  doi = {10.1145/3394171.3413889},
  url = {https://dl.acm.org/doi/10.1145/3394171.3413889},
  urldate = {2022-06-02},
  eventtitle = {{{MM}} '20: {{The}} 28th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-7988-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BSZGSK6L\\Han 等。 - 2020 - ShapeCaptioner Generative Caption Network for 3D .pdf}
}

@inproceedings{hauriletItNotJourney2019,
  title = {It's {{Not About}} the {{Journey}}; {{It}}'s {{About}} the {{Destination}}: {{Following Soft Paths Under Question-Guidance}} for {{Visual Reasoning}}},
  shorttitle = {It's {{Not About}} the {{Journey}}; {{It}}'s {{About}} the {{Destination}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Haurilet, Monica and Roitberg, Alina and Stiefelhagen, Rainer},
  date = {2019-06},
  pages = {1930--1939},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00203},
  url = {https://ieeexplore.ieee.org/document/8953472/},
  urldate = {2021-09-09},
  abstract = {Visual Reasoning remains a challenging task, as it has to deal with long-range and multi-step object relationships in the scene. We present a new model for Visual Reasoning, aimed at capturing the interplay among individual objects in the image represented as a scene graph. As not all graph components are relevant for the query, we introduce the concept of a question-based visual guide, which constrains the potential solution space by learning an optimal traversal scheme. The final destination nodes alone are then used to produce the answer. We show, that finding relevant semantic structures facilitates generalization to new tasks by introducing a novel problem of knowledge transfer: training on one question type and answering questions from a different domain without any training data. Furthermore, we achieve state-of-the-art results for Visual Reasoning on multiple query types and diverse image and video datasets.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  keywords = {Reasoning，Path},
  annotation = {5 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\XK32GI3M\\Haurilet 等。 - 2019 - It's Not About the Journey\; It's About the Destina.pdf}
}

@unpublished{heFedCVFederatedLearning2021,
  title = {{{FedCV}}: {{A Federated Learning Framework}} for {{Diverse Computer Vision Tasks}}},
  shorttitle = {{{FedCV}}},
  author = {He, Chaoyang and Shah, Alay Dilipbhai and Tang, Zhenheng and Sivashunmugam, Di Fan1Adarshan Naiynar and Bhogaraju, Keerti and Shimpi, Mita and Shen, Li and Chu, Xiaowen and Soltanolkotabi, Mahdi and Avestimehr, Salman},
  date = {2021-11-22},
  eprint = {2111.11066},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.11066},
  urldate = {2022-01-18},
  abstract = {Federated Learning (FL) is a distributed learning paradigm that can learn a global or personalized model from decentralized datasets on edge devices. However, in the computer vision domain, model performance in FL is far behind centralized training due to the lack of exploration in diverse tasks with a unified FL framework. FL has rarely been demonstrated effectively in advanced computer vision tasks such as object detection and image segmentation. To bridge the gap and facilitate the development of FL for computer vision tasks, in this work, we propose a federated learning library and benchmarking framework, named FedCV, to evaluate FL on the three most representative computer vision tasks: image classification, image segmentation, and object detection. We provide non-I.I.D. benchmarking datasets, models, and various reference FL algorithms. Our benchmark study suggests that there are multiple challenges that deserve future exploration: centralized training tricks may not be directly applied to FL; the non-I.I.D. dataset actually downgrades the model accuracy to some degree in different tasks; improving the system efficiency of federated training is challenging given the huge number of parameters and the per-client memory cost. We believe that such a library and benchmark, along with comparable evaluation settings, is necessary to make meaningful progress in FL on computer vision tasks. FedCV is publicly available: https://github.com/FedML-AI/FedCV .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\A7ET3ZSD\\He 等。 - 2021 - FedCV A Federated Learning Framework for Diverse .pdf}
}

@article{heInterpretableVisualReasoning2021,
  title = {Interpretable Visual Reasoning: {{A}} Survey},
  shorttitle = {Interpretable Visual Reasoning},
  author = {He, Feijuan and Wang, Yaxian and Miao, Xianglin and Sun, Xia},
  date = {2021-08},
  journaltitle = {Image and Vision Computing},
  shortjournal = {Image and Vision Computing},
  volume = {112},
  pages = {104194},
  issn = {02628856},
  doi = {10.1016/j.imavis.2021.104194},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885621000998},
  urldate = {2021-09-26},
  abstract = {Visual reasoning refers to the process of solving questions about visual information. At present, most visual reasoning models are mainly based on deep learning and end-to-end architecture. Although these models have achieved good performance, they are usually black boxes for users, and it is difficult to understand the basic rationales of the reasoning process. In recent years, the academic community has realized the importance of interpretability in visual reasoning and has developed a series of Interpretable Visual Reasoning (IVR) models. In this paper, we review these models. First, we have established a taxonomy based on four explanation forms of vision, text, graph and symbol used in current visual reasoning. Secondly, we explore the typical IVR models of each category and analyze their pros and cons. Thirdly, we elaborate on the current mainstream datasets about visual reasoning and VQA, and analyze how these datasets promote IVR research from different perspectives. Finally, we summarize the challenges for IVR and point out potential research directions.},
  langid = {english}
}

@article{heInterpretableVisualReasoning2021a,
  title = {Interpretable Visual Reasoning: {{A}} Survey},
  shorttitle = {Interpretable Visual Reasoning},
  author = {He, Feijuan and Wang, Yaxian and Miao, Xianglin and Sun, Xia},
  date = {2021-08-01},
  journaltitle = {Image and Vision Computing},
  shortjournal = {Image and Vision Computing},
  volume = {112},
  pages = {104194},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2021.104194},
  url = {https://www.sciencedirect.com/science/article/pii/S0262885621000998},
  urldate = {2021-09-26},
  abstract = {Visual reasoning refers to the process of solving questions about visual information. At present, most visual reasoning models are mainly based on deep learning and end-to-end architecture. Although these models have achieved good performance, they are usually black boxes for users, and it is difficult to understand the basic rationales of the reasoning process. In recent years, the academic community has realized the importance of interpretability in visual reasoning and has developed a series of Interpretable Visual Reasoning (IVR) models. In this paper, we review these models. First, we have established a taxonomy based on four explanation forms of vision, text, graph and symbol used in current visual reasoning. Secondly, we explore the typical IVR models of each category and analyze their pros and cons. Thirdly, we elaborate on the current mainstream datasets about visual reasoning and VQA, and analyze how these datasets promote IVR research from different perspectives. Finally, we summarize the challenges for IVR and point out potential research directions.},
  langid = {english},
  keywords = {Datasets,Interpretability,Survey,Visual question answering,Visual reasoning}
}

@unpublished{heMOSTMultiOrientedScene2021,
  title = {{{MOST}}: {{A Multi-Oriented Scene Text Detector}} with {{Localization Refinement}}},
  shorttitle = {{{MOST}}},
  author = {He, Minghang and Liao, Minghui and Yang, Zhibo and Zhong, Humen and Tang, Jun and Cheng, Wenqing and Yao, Cong and Wang, Yongpan and Bai, Xiang},
  date = {2021-04-05},
  eprint = {2104.01070},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.01070},
  urldate = {2022-04-02},
  abstract = {Over the past few years, the field of scene text detection has progressed rapidly that modern text detectors are able to hunt text in various challenging scenarios. However, they might still fall short when handling text instances of extreme aspect ratios and varying scales. To tackle such difficulties, we propose in this paper a new algorithm for scene text detection, which puts forward a set of strategies to significantly improve the quality of text localization. Specifically, a Text Feature Alignment Module (TFAM) is proposed to dynamically adjust the receptive fields of features based on initial raw detections; a Position-Aware Non-Maximum Suppression (PA-NMS) module is devised to selectively concentrate on reliable raw detections and exclude unreliable ones; besides, we propose an Instance-wise IoU loss for balanced training to deal with text instances of different scales. An extensive ablation study demonstrates the effectiveness and superiority of the proposed strategies. The resulting text detection system, which integrates the proposed strategies with a leading scene text detector EAST, achieves state-of-the-art or competitive performance on various standard benchmarks for text detection while keeping a fast running speed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\2F3HL4T4\\He et al_2021_MOST.pdf;D\:\\Zotero\\storage\\ZUN5XUJW\\2104.html}
}

@article{hennessyNewGoldenAge2019,
  title = {A New Golden Age for Computer Architecture},
  author = {Hennessy, John L. and Patterson, David A.},
  date = {2019-01-28},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {62},
  number = {2},
  pages = {48--60},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3282307},
  url = {https://dl.acm.org/doi/10.1145/3282307},
  urldate = {2021-11-23},
  abstract = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\T5U3GNBJ\\Hennessy 和 Patterson - 2019 - A new golden age for computer architecture.pdf}
}

@unpublished{hePathVQA30000Questions2020,
  title = {{{PathVQA}}: 30000+ {{Questions}} for {{Medical Visual Question Answering}}},
  shorttitle = {{{PathVQA}}},
  author = {He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
  date = {2020-03-07},
  eprint = {2003.10286},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.10286},
  urldate = {2021-03-09},
  abstract = {Is it possible to develop an “AI Pathologist" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Our work makes the first attempt to build such a dataset. Different from creating general-domain VQA datasets where the images are widely accessible and there are many crowdsourcing workers available and capable of generating question-answer pairs, developing a medical VQA dataset is much more challenging. First, due to privacy concerns, pathology images are usually not publicly available. Second, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. To address these challenges, we resort to pathology textbooks and online digital libraries. We develop a semi-automated pipeline to extract pathology images and captions from textbooks and generate question-answer pairs from captions using natural language processing. We collect 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness. To our best knowledge, this is the first dataset for pathology VQA. Our dataset will be released publicly to promote research in medical VQA.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\BLR2L4P7\\He 等。 - 2020 - PathVQA 30000+ Questions for Medical Visual Quest.pdf}
}

@inproceedings{heVisualQuestionAnswering2021,
  title = {Towards {{Visual Question Answering}} on {{Pathology Images}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {He, Xuehai and Cai, Zhuo and Wei, Wenlan and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
  date = {2021-08},
  pages = {708--718},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-short.90},
  url = {https://aclanthology.org/2021.acl-short.90},
  urldate = {2021-09-09},
  abstract = {Pathology imaging is broadly used for identifying the causes and effects of diseases or injuries. Given a pathology image, being able to answer questions about the clinical findings contained in the image is very important for medical decision making. In this paper, we aim to develop a pathological visual question answering framework to analyze pathology images and answer medical questions related to these images. To build such a framework, we create PathVQA, a VQA dataset with 32,795 questions asked from 4,998 pathology images. We also propose a three-level optimization framework which performs self-supervised pretraining and VQA finetuning end-to-end to learn powerful visual and textual representations jointly and automatically identifies and excludes noisy self-supervised examples from pretraining. We perform experiments on our created PathVQA dataset and the results demonstrate the effectiveness of our proposed methods. The datasets and code are available at https://github.com/UCSD-AI4H/PathVQA},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  keywords = {Path},
  file = {D\:\\Zotero\\storage\\TSF2LVJF\\He et al_2021_Towards Visual Question Answering on Pathology Images.pdf}
}

@unpublished{hildebrandtSceneGraphReasoning2020,
  title = {Scene {{Graph Reasoning}} for {{Visual Question Answering}}},
  author = {Hildebrandt, Marcel and Li, Hang and Koner, Rajat and Tresp, Volker and Günnemann, Stephan},
  date = {2020-07-02},
  eprint = {2007.01072},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.01072},
  urldate = {2021-10-09},
  abstract = {Visual question answering is concerned with answering free-form questions about an image. Since it requires a deep linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires techniques from both computer vision and natural language processing. We propose a novel method that approaches the task by performing context-driven, sequential reasoning based on the objects and their semantic and spatial relationships present in the scene. As a first step, we derive a scene graph which describes the objects in the image, as well as their attributes and their mutual relationships. A reinforcement agent then learns to autonomously navigate over the extracted scene graph to generate paths, which are then the basis for deriving answers. We conduct a first experimental study on the challenging GQA dataset with manually curated scene graphs, where our method almost reaches the level of human performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\LFYAN29U\\Hildebrandt et al_2020_Scene Graph Reasoning for Visual Question Answering.pdf;D\:\\Zotero\\storage\\P4EIZ58H\\2007.html}
}

@article{hongExploitingHierarchicalVisual2019,
  title = {Exploiting Hierarchical Visual Features for Visual Question Answering},
  author = {Hong, Jongkwang and Fu, Jianlong and Uh, Youngjung and Mei, Tao and Byun, Hyeran},
  date = {2019-07},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {351},
  pages = {187--195},
  issn = {09252312},
  doi = {10.1016/j.neucom.2019.03.035},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219303753},
  urldate = {2022-02-07},
  langid = {english},
  file = {D\:\\Zotero\\storage\\JG7D4MS3\\Hong 等。 - 2019 - Exploiting hierarchical visual features for visual.pdf}
}

@article{hongLearningComposeReason2022,
  title = {Learning to {{Compose}} and {{Reason}} with {{Language Tree Structures}} for {{Visual Grounding}}},
  author = {Hong, Richang and Liu, Daqing and Mo, Xiaoyu and He, Xiangnan and Zhang, Hanwang},
  date = {2022-02-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {44},
  number = {2},
  eprint = {1906.01784},
  eprinttype = {arxiv},
  pages = {684--696},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2019.2911066},
  url = {http://arxiv.org/abs/1906.01784},
  urldate = {2022-03-23},
  abstract = {Grounding natural language in images, such as localizing "the black dog on the left of the tree", is one of the core problems in artificial intelligence, as it needs to comprehend the fine-grained and compositional language space. However, existing solutions rely on the association between the holistic language features and visual features, while neglect the nature of compositional reasoning implied in the language. In this paper, we propose a natural language grounding model that can automatically compose a binary tree structure for parsing the language and then perform visual reasoning along the tree in a bottom-up fashion. We call our model RVG-TREE: Recursive Grounding Tree, which is inspired by the intuition that any language expression can be recursively decomposed into two constituent parts, and the grounding confidence score can be recursively accumulated by calculating their grounding scores returned by sub-trees. RVG-TREE can be trained end-to-end by using the Straight-Through Gumbel-Softmax estimator that allows the gradients from the continuous score functions passing through the discrete tree construction. Experiments on several benchmarks show that our model achieves the state-of-the-art performance with more explainable reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\FIY9WG45\\Hong et al_2022_Learning to Compose and Reason with Language Tree Structures for Visual.pdf;D\:\\Zotero\\storage\\GDBFDBPT\\1906.html}
}

@article{hongSelectiveResidualLearning2020,
  title = {Selective Residual Learning for {{Visual Question Answering}}},
  author = {Hong, Jongkwang and Park, Sungho and Byun, Hyeran},
  date = {2020-08},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {402},
  pages = {366--374},
  issn = {09252312},
  doi = {10.1016/j.neucom.2020.03.098},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220304859},
  urldate = {2022-02-07},
  langid = {english},
  file = {D\:\\Zotero\\storage\\FDKY4LJF\\Hong 等。 - 2020 - Selective residual learning for Visual Question An.pdf}
}

@misc{huangArbitraryStyleTransfer2017,
  title = {Arbitrary {{Style Transfer}} in {{Real-time}} with {{Adaptive Instance Normalization}}},
  author = {Huang, Xun and Belongie, Serge},
  date = {2017-07-30},
  number = {arXiv:1703.06868},
  eprint = {1703.06868},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1703.06868},
  urldate = {2022-06-07},
  abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\GHD39WXR\\Huang 和 Belongie - 2017 - Arbitrary Style Transfer in Real-time with Adaptiv.pdf}
}

@article{huangLocationAwareGraphConvolutional2020,
  title = {Location-{{Aware Graph Convolutional Networks}} for {{Video Question Answering}}},
  author = {Huang, Deng and Chen, Peihao and Zeng, Runhao and Du, Qing and Tan, Mingkui and Gan, Chuang},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {07},
  pages = {11021--11028},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i07.6737},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6737},
  urldate = {2021-09-10},
  abstract = {We addressed the challenging task of video question answering, which requires machines to answer questions about videos in a natural language form. Previous state-of-the-art methods attempt to apply spatio-temporal attention mechanism on video frame features without explicitly modeling the location and relations among object interaction occurred in videos. However, the relations between object interaction and their location information are very critical for both action recognition and question reasoning. In this work, we propose to represent the contents in the video as a locationaware graph by incorporating the location information of an object into the graph construction. Here, each node is associated with an object represented by its appearance and location features. Based on the constructed graph, we propose to use graph convolution to infer both the category and temporal locations of an action. As the graph is built on objects, our method is able to focus on the foreground action contents for better video question answering. Lastly, we leverage an attention mechanism to combine the output of graph convolution and encoded question features for final answer reasoning. Extensive experiments demonstrate the effectiveness of the proposed methods. Specifically, our method significantly outperforms state-of-the-art methods on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\HU4A6QH9\\Huang 等。 - 2020 - Location-Aware Graph Convolutional Networks for Vi.pdf}
}

@inproceedings{huangMultigrainedAttentionObjectlevel2019,
  title = {Multi-Grained {{Attention}} with {{Object-level Grounding}} for {{Visual Question Answering}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Huang, Pingping and Huang, Jianhui and Guo, Yuqing and Qiao, Min and Zhu, Yong},
  date = {2019-07},
  pages = {3595--3600},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1349},
  url = {https://aclanthology.org/P19-1349},
  urldate = {2021-09-10},
  abstract = {Attention mechanisms are widely used in Visual Question Answering (VQA) to search for visual clues related to the question. Most approaches train attention models from a coarse-grained association between sentences and images, which tends to fail on small objects or uncommon concepts. To address this problem, this paper proposes a multi-grained attention method. It learns explicit word-object correspondence by two types of word-level attention complementary to the sentence-image association. Evaluated on the VQA benchmark, the multi-grained attention model achieves competitive performance with state-of-the-art models. And the visualized attention maps demonstrate that addition of object-level groundings leads to a better understanding of the images and locates the attended objects more precisely.},
  eventtitle = {{{ACL}} 2019},
  keywords = {Grounding},
  file = {D\:\\Zotero\\storage\\CKU8UTBQ\\Huang et al_2019_Multi-grained Attention with Object-level Grounding for Visual Question.pdf}
}

@inproceedings{huangScallopProbabilisticDeductive2021,
  title = {Scallop: {{From Probabilistic Deductive Databases}} to {{Scalable Differentiable Reasoning}}},
  shorttitle = {Scallop},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Huang, Jiani and Li, Ziyang and Chen, Binghong and Samel, Karan and Naik, Mayur and Song, Le and Si, Xujie},
  date = {2021},
  volume = {34},
  pages = {25134--25145},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html},
  urldate = {2022-06-17},
  abstract = {Deep learning and symbolic reasoning are complementary techniques for an intelligent system. However, principled combinations of these techniques have limited scalability, rendering them ill-suited for real-world applications. We propose Scallop, a system that builds upon probabilistic deductive databases, to bridge this gap. The key insight underlying Scallop is a provenance framework that introduces a tunable parameter to specify the level of reasoning granularity. Scallop thereby i) generalizes exact probabilistic reasoning, ii) asymptotically reduces computational cost, and iii) provides relative accuracy guarantees. On a suite of tasks that involve mathematical and logical reasoning, Scallop scales significantly better without sacrificing accuracy compared to DeepProbLog, a principled neural logic programming approach. We also create and evaluate on a real-world Visual Question Answering (VQA) benchmark that requires multi-hop reasoning. Scallop outperforms two VQA-tailored models, a Neural Module Networks based and a transformer based model, by 12.42\% and 21.66\% respectively.},
  file = {D\:\\Zotero\\storage\\CW9V7HHR\\Huang et al_2021_Scallop.pdf}
}

@misc{huDistillingCausalEffect2021,
  title = {Distilling {{Causal Effect}} of {{Data}} in {{Class-Incremental Learning}}},
  author = {Hu, Xinting and Tang, Kaihua and Miao, Chunyan and Hua, Xian-Sheng and Zhang, Hanwang},
  date = {2021-03-07},
  number = {arXiv:2103.01737},
  eprint = {2103.01737},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2103.01737},
  urldate = {2022-09-02},
  abstract = {We propose a causal framework to explain the catastrophic forgetting in Class-Incremental Learning (CIL) and then derive a novel distillation method that is orthogonal to the existing anti-forgetting techniques, such as data replay and feature/label distillation. We first 1) place CIL into the framework, 2) answer why the forgetting happens: the causal effect of the old data is lost in new training, and then 3) explain how the existing techniques mitigate it: they bring the causal effect back. Based on the framework, we find that although the feature/label distillation is storage-efficient, its causal effect is not coherent with the end-to-end feature learning merit, which is however preserved by data replay. To this end, we propose to distill the Colliding Effect between the old and the new data, which is fundamentally equivalent to the causal effect of data replay, but without any cost of replay storage. Thanks to the causal effect analysis, we can further capture the Incremental Momentum Effect of the data stream, removing which can help to retain the old effect overwhelmed by the new data effect, and thus alleviate the forgetting of the old class in testing. Extensive experiments on three CIL benchmarks: CIFAR-100, ImageNet-Sub\&Full, show that the proposed causal effect distillation can improve various state-of-the-art CIL methods by a large margin (0.72\%--9.06\%).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {D\:\\Zotero\\storage\\PPMAVV6X\\Hu 等。 - 2021 - Distilling Causal Effect of Data in Class-Incremen.pdf;D\:\\Zotero\\storage\\XBWL6VNH\\2103.html}
}

@unpublished{hudsonCompositionalAttentionNetworks2018,
  title = {Compositional {{Attention Networks}} for {{Machine Reasoning}}},
  author = {Hudson, Drew A. and Manning, Christopher D.},
  date = {2018-04-24},
  eprint = {1803.03067},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.03067},
  urldate = {2022-01-30},
  abstract = {We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9\% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {D\:\\Zotero\\storage\\ZCSD3FQ5\\Hudson_Manning_2018_Compositional Attention Networks for Machine Reasoning.pdf;D\:\\Zotero\\storage\\A6AEB5WU\\1803.html}
}

@unpublished{huExplainableNeuralComputation2019,
  title = {Explainable {{Neural Computation}} via {{Stack Neural Module Networks}}},
  author = {Hu, Ronghang and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
  date = {2019-03-06},
  eprint = {1807.08556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1807.08556},
  urldate = {2022-01-30},
  abstract = {In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\I6TLQCI5\\Hu et al_2019_Explainable Neural Computation via Stack Neural Module Networks.pdf;D\:\\Zotero\\storage\\J6ZF5T5C\\1807.html}
}

@article{huGraphbasedVisualSemanticEntanglement2021,
  title = {Graph-Based {{Visual-Semantic Entanglement Network}} for {{Zero-shot Image Recognition}}},
  author = {Hu, Yang and Wen, Guihua and Chapman, Adriane and Yang, Pei and Luo, Mingnan and Xu, Yingxue and Dai, Dan and Hall, Wendy},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  shortjournal = {IEEE Trans. Multimedia},
  eprint = {2006.04648},
  eprinttype = {arxiv},
  pages = {1--1},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2021.3082292},
  url = {http://arxiv.org/abs/2006.04648},
  urldate = {2022-03-09},
  abstract = {Zero-shot learning uses semantic attributes to connect the search space of unseen objects. In recent years, although the deep convolutional network brings powerful visual modeling capabilities to the ZSL task, its visual features have severe pattern inertia and lack of representation of semantic relationships, which leads to severe bias and ambiguity. In response to this, we propose the Graph-based Visual-Semantic Entanglement Network to conduct graph modeling of visual features, which is mapped to semantic attributes by using a knowledge graph, it contains several novel designs: 1. it establishes a multi-path entangled network with the convolutional neural network (CNN) and the graph convolutional network (GCN), which input the visual features from CNN to GCN to model the implicit semantic relations, then GCN feedback the graph modeled information to CNN features; 2. it uses attribute word vectors as the target for the graph semantic modeling of GCN, which forms a self-consistent regression for graph modeling and supervise GCN to learn more personalized attribute relations; 3. it fuses and supplements the hierarchical visual-semantic features refined by graph modeling into visual embedding. Our method outperforms state-of-the-art approaches on multiple representative ZSL datasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of visual features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {D\:\\Zotero\\storage\\VVUFB9Y9\\Hu et al_2021_Graph-based Visual-Semantic Entanglement Network for Zero-shot Image Recognition.pdf;D\:\\Zotero\\storage\\JC5CWB98\\2006.html}
}

@inproceedings{huHeterogeneousGraphTransformer2020,
  title = {Heterogeneous {{Graph Transformer}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Sun, Yizhou},
  date = {2020-04-20},
  pages = {2704--2710},
  publisher = {{ACM}},
  location = {{Taipei Taiwan}},
  doi = {10.1145/3366423.3380027},
  url = {https://dl.acm.org/doi/10.1145/3366423.3380027},
  urldate = {2022-03-26},
  abstract = {Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9\%–21\% on various downstream tasks. The dataset and source code of HGT are publicly available at https://github.com/acbull/pyHGT.},
  eventtitle = {{{WWW}} '20: {{The Web Conference}} 2020},
  isbn = {978-1-4503-7023-3},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZRH2P5SS\\Hu 等。 - 2020 - Heterogeneous Graph Transformer.pdf}
}

@unpublished{huIterativeAnswerPrediction2020,
  title = {Iterative {{Answer Prediction}} with {{Pointer-Augmented Multimodal Transformers}} for {{TextVQA}}},
  author = {Hu, Ronghang and Singh, Amanpreet and Darrell, Trevor and Rohrbach, Marcus},
  date = {2020-03-24},
  eprint = {1911.06258},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.06258},
  urldate = {2021-12-08},
  abstract = {Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks. For example, a deep water label on a warning sign warns people about the danger in the scene. Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question. However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task. In this work, we propose a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images. Our model naturally fuses different modalities homogeneously by embedding them into a common semantic space where self-attention is applied to model inter- and intra- modality context. Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification. Our model outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\8WYAWM8V\\Hu et al_2020_Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for.pdf;D\:\\Zotero\\storage\\K3DCTVMZ\\1911.html}
}

@inproceedings{huIterativeAnswerPrediction2020a,
  title = {Iterative {{Answer Prediction With Pointer-Augmented Multimodal Transformers}} for {{TextVQA}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hu, Ronghang and Singh, Amanpreet and Darrell, Trevor and Rohrbach, Marcus},
  date = {2020-06},
  pages = {9989--9999},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01001},
  url = {https://ieeexplore.ieee.org/document/9156750/},
  urldate = {2021-09-09},
  abstract = {Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks. For example, a deep water label on a warning sign warns people about the danger in the scene. Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question. However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task. In this work, we propose a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images. Our model naturally fuses different modalities homogeneously by embedding them into a common semantic space where self-attention is applied to model inter- and intra- modality context. Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification. Our model outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {8 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\DWCYK3EP\\Hu 等。 - 2020 - Iterative Answer Prediction With Pointer-Augmented.pdf}
}

@inproceedings{huLearningReasonEndtoEnd2017,
  title = {Learning to {{Reason}}: {{End-to-End Module Networks}} for {{Visual Question Answering}}},
  shorttitle = {Learning to {{Reason}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
  date = {2017-10},
  pages = {804--813},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.93},
  url = {http://ieeexplore.ieee.org/document/8237355/},
  urldate = {2021-09-10},
  abstract = {Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer “is there an equal number of balls and boxes?” we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture [3, 2] implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50\% relative to state-of-theart attentional approaches, while discovering interpretable network architectures specialized for each question.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\WEI4MEXH\\Hu 等。 - 2017 - Learning to Reason End-to-End Module Networks for.pdf}
}

@unpublished{huoHeterogeneousContrastiveLearning2020,
  title = {Heterogeneous {{Contrastive Learning}}: {{Encoding Spatial Information}} for {{Compact Visual Representations}}},
  shorttitle = {Heterogeneous {{Contrastive Learning}}},
  author = {Huo, Xinyue and Xie, Lingxi and Wei, Longhui and Zhang, Xiaopeng and Li, Hao and Yang, Zijie and Zhou, Wengang and Li, Houqiang and Tian, Qi},
  date = {2020-11-19},
  eprint = {2011.09941},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.09941},
  urldate = {2022-02-13},
  abstract = {Contrastive learning has achieved great success in self-supervised visual representation learning, but existing approaches mostly ignored spatial information which is often crucial for visual representation. This paper presents heterogeneous contrastive learning (HCL), an effective approach that adds spatial information to the encoding stage to alleviate the learning inconsistency between the contrastive objective and strong data augmentation operations. We demonstrate the effectiveness of HCL by showing that (i) it achieves higher accuracy in instance discrimination and (ii) it surpasses existing pre-training methods in a series of downstream tasks while shrinking the pre-training costs by half. More importantly, we show that our approach achieves higher efficiency in visual representations, and thus delivers a key message to inspire the future research of self-supervised visual representation learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\UWSYNLHU\\Huo et al_2020_Heterogeneous Contrastive Learning.pdf;D\:\\Zotero\\storage\\3BGN5F6X\\2011.html}
}

@unpublished{huoWenLanBridgingVision2021,
  title = {{{WenLan}}: {{Bridging Vision}} and {{Language}} by {{Large-Scale Multi-Modal Pre-Training}}},
  shorttitle = {{{WenLan}}},
  author = {Huo, Yuqi and Zhang, Manli and Liu, Guangzhen and Lu, Haoyu and Gao, Yizhao and Yang, Guoxing and Wen, Jingyuan and Zhang, Heng and Xu, Baogui and Zheng, Weihao and Xi, Zongzheng and Yang, Yueqian and Hu, Anwen and Zhao, Jinming and Li, Ruichen and Zhao, Yida and Zhang, Liang and Song, Yuqing and Hong, Xin and Cui, Wanqing and Hou, Danyang and Li, Yingyan and Li, Junyi and Liu, Peiyu and Gong, Zheng and Jin, Chuhao and Sun, Yuchong and Chen, Shizhe and Lu, Zhiwu and Dou, Zhicheng and Jin, Qin and Lan, Yanyan and Zhao, Wayne Xin and Song, Ruihua and Wen, Ji-Rong},
  date = {2021-07-08},
  number = {arXiv:2103.06561},
  eprint = {2103.06561},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2103.06561},
  urldate = {2022-06-09},
  abstract = {Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. Unlike OpenAI CLIP that adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in limited GPU resources. We further construct a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER and OpenAI CLIP on various downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\ZPVLX9P7\\Huo et al_2021_WenLan.pdf;D\:\\Zotero\\storage\\29UCXDDX\\2103.html}
}

@unpublished{huQuestioncontrolledTextawareImage2021,
  title = {Question-Controlled {{Text-aware Image Captioning}}},
  author = {Hu, Anwen and Chen, Shizhe and Jin, Qin},
  date = {2021-08-04},
  eprint = {2108.02059},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.02059},
  urldate = {2022-03-12},
  abstract = {For an image with multiple scene texts, different people may be interested in different text information. Current text-aware image captioning models are not able to generate distinctive captions according to various information needs. To explore how to generate personalized text-aware captions, we define a new challenging task, namely Question-controlled Text-aware Image Captioning (Qc-TextCap). With questions as control signals, this task requires models to understand questions, find related scene texts and describe them together with objects fluently in human language. Based on two existing text-aware captioning datasets, we automatically construct two datasets, ControlTextCaps and ControlVizWiz to support the task. We propose a novel Geometry and Question Aware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to fuse region-level object features and region-level scene text features with considering spatial relationships. Then, we design a Question-guided Encoder to select the most relevant visual features for each question. Finally, GQAM generates a personalized text-aware caption with a Multimodal Decoder. Our model achieves better captioning performance and question answering ability than carefully designed baselines on both two datasets. With questions as control signals, our model generates more informative and diverse captions than the state-of-the-art text-aware captioning model. Our code and datasets are publicly available at https://github.com/HAWLYQ/Qc-TextCap.},
  archiveprefix = {arXiv},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\NM5PFUEJ\\Hu et al_2021_Question-controlled Text-aware Image Captioning.pdf;D\:\\Zotero\\storage\\8FQLRZ5Y\\2108.html}
}

@unpublished{hussainEdgeaugmentedGraphTransformers2021,
  title = {Edge-Augmented {{Graph Transformers}}: {{Global Self-attention}} Is {{Enough}} for {{Graphs}}},
  shorttitle = {Edge-Augmented {{Graph Transformers}}},
  author = {Hussain, Md Shamim and Zaki, Mohammed J. and Subramanian, Dharmashankar},
  date = {2021-09-13},
  eprint = {2108.03348},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.03348},
  urldate = {2022-03-26},
  abstract = {Transformer neural networks have achieved state-of-the-art results for unstructured data such as text and images but their adoption for graph-structured data has been limited. This is partly due to the difficulty of incorporating complex structural information in the basic transformer framework. We propose a simple yet powerful extension to the transformer - residual edge channels. The resultant framework, which we call Edge-augmented Graph Transformer (EGT), can directly accept, process and output structural information as well as node information. It allows us to use global self-attention, the key element of transformers, directly for graphs and comes with the benefit of long-range interaction among nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. In addition, we introduce a generalized positional encoding scheme for graphs based on Singular Value Decomposition which can improve the performance of EGT. Our framework, which relies on global node feature aggregation, achieves better performance compared to Convolutional/Message-Passing Graph Neural Networks, which rely on local feature aggregation within a neighborhood. We verify the performance of EGT in a supervised learning setting on a wide range of experiments on benchmark datasets. Our findings indicate that convolutional aggregation is not an essential inductive bias for graphs and global self-attention can serve as a flexible and adaptive alternative.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\N497Y8QQ\\Hussain et al_2021_Edge-augmented Graph Transformers.pdf;D\:\\Zotero\\storage\\ERSFXXAI\\2108.html}
}

@article{huStructuredModelsVisionandLanguage,
  title = {Structured {{Models}} for {{Vision-and-Language Reasoning}}},
  author = {Hu, Ronghang},
  pages = {124},
  langid = {english},
  file = {D\:\\Zotero\\storage\\HJDG27F5\\Hu - Structured Models for Vision-and-Language Reasonin.pdf}
}

@article{ilievskiFocusedDynamicAttention,
  title = {A {{Focused Dynamic Attention Model}} for {{Visual Question Answering}}},
  author = {Ilievski, Ilija and Yan, Shuicheng and Feng, Jiashi},
  pages = {15},
  abstract = {Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects. Extracting features from auto-generated regions – as some region-based image recognition methods do – cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit. Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over wellestablished baselines.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\JJKVTZPL\\Ilievski 等。 - A Focused Dynamic Attention Model for Visual Quest.pdf}
}

@online{InterpretableVisualReasoning,
  title = {Interpretable Visual Reasoning: {{A}} Survey | {{Elsevier Enhanced Reader}}},
  shorttitle = {Interpretable Visual Reasoning},
  doi = {10.1016/j.imavis.2021.104194},
  url = {https://reader.elsevier.com/reader/sd/pii/S0262885621000998?token=0B6856C3CD056E71256DACA5D0FBB7D63D887F52F19C5582D25C0705406BBB12DAB895F2842F32B9F28691B116A3F690&originRegion=us-east-1&originCreation=20210926044131},
  urldate = {2021-09-26},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZMEUY6I6\\1-s2.0-S0262885621000998-main.pdf}
}

@inproceedings{jaderbergSpatialTransformerNetworks2015,
  title = {Spatial {{Transformer Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and {kavukcuoglu}, koray},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2015/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html},
  urldate = {2022-02-10},
  file = {D\:\\Zotero\\storage\\53TDRH6R\\Jaderberg et al_2015_Spatial Transformer Networks.pdf}
}

@inproceedings{jainSelectSubstituteSearch2021,
  title = {Select, {{Substitute}}, {{Search}}: {{A New Benchmark}} for {{Knowledge-Augmented Visual Question Answering}}},
  shorttitle = {Select, {{Substitute}}, {{Search}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Jain, Aman and Kothyari, Mayank and Kumar, Vishwajeet and Jyothi, Preethi and Ramakrishnan, Ganesh and Chakrabarti, Soumen},
  date = {2021-07-11},
  pages = {2491--2498},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3463259},
  url = {https://dl.acm.org/doi/10.1145/3404835.3463259},
  urldate = {2021-09-10},
  eventtitle = {{{SIGIR}} '21: {{The}} 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\URS4CTJT\\Jain 等。 - 2021 - Select, Substitute, Search A New Benchmark for Kn.pdf}
}

@inproceedings{jainTwoCanPlay2018,
  title = {Two {{Can Play This Game}}: {{Visual Dialog}} with {{Discriminative Question Generation}} and {{Answering}}},
  shorttitle = {Two {{Can Play This Game}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Jain, Unnat and Schwing, Alexander and Lazebnik, Svetlana},
  date = {2018-06},
  pages = {5754--5763},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00603},
  url = {https://ieeexplore.ieee.org/document/8578701/},
  urldate = {2021-09-09},
  abstract = {Human conversation is a complex mechanism with subtle nuances. It is hence an ambitious goal to develop artificial intelligence agents that can participate fluently in a conversation. While we are still far from achieving this goal, recent progress in visual question answering, image captioning, and visual question generation shows that dialog systems may be realizable in the not too distant future. To this end, a novel dataset was introduced recently and encouraging results were demonstrated, particularly for question answering. In this paper, we demonstrate a simple symmetric discriminative baseline, that can be applied to both predicting an answer as well as predicting a question. We show that this method performs on par with the state of the art, even memory net based methods. In addition, for the first time on the visual dialog dataset, we assess the performance of a system asking questions, and demonstrate how visual dialog can be generated from discriminative question generation and question answering.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {27 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\MR6PYPUE\\Jain 等。 - 2018 - Two Can Play This Game Visual Dialog with Discrim.pdf}
}

@inproceedings{jangTGIFQASpatioTemporalReasoning2017,
  title = {{{TGIF-QA}}: {{Toward Spatio-Temporal Reasoning}} in {{Visual Question Answering}}},
  shorttitle = {{{TGIF-QA}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  date = {2017-07},
  pages = {1359--1367},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.149},
  url = {http://ieeexplore.ieee.org/document/8099632/},
  urldate = {2021-03-11},
  abstract = {Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\Y7M7J4EX\\Jang 等。 - 2017 - TGIF-QA Toward Spatio-Temporal Reasoning in Visua.pdf}
}

@unpublished{jiangDAMDeliberationAbandon2020,
  title = {{{DAM}}: {{Deliberation}}, {{Abandon}} and {{Memory Networks}} for {{Generating Detailed}} and {{Non-repetitive Responses}} in {{Visual Dialogue}}},
  shorttitle = {{{DAM}}},
  author = {Jiang, Xiaoze and Yu, Jing and Sun, Yajing and Qin, Zengchang and Zhu, Zihao and Hu, Yue and Wu, Qi},
  date = {2020-07-07},
  eprint = {2007.03310},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2007.03310},
  urldate = {2021-12-08},
  abstract = {Visual Dialogue task requires an agent to be engaged in a conversation with human about an image. The ability of generating detailed and non-repetitive responses is crucial for the agent to achieve human-like conversation. In this paper, we propose a novel generative decoding architecture to generate high-quality responses, which moves away from decoding the whole encoded semantics towards the design that advocates both transparency and flexibility. In this architecture, word generation is decomposed into a series of attention-based information selection steps, performed by the novel recurrent Deliberation, Abandon and Memory (DAM) module. Each DAM module performs an adaptive combination of the response-level semantics captured from the encoder and the word-level semantics specifically selected for generating each word. Therefore, the responses contain more detailed and non-repetitive descriptions while maintaining the semantic accuracy. Furthermore, DAM is flexible to cooperate with existing visual dialogue encoders and adaptive to the encoder structures by constraining the information selection mode in DAM. We apply DAM to three typical encoders and verify the performance on the VisDial v1.0 dataset. Experimental results show that the proposed models achieve new state-of-the-art performance with high-quality responses. The code is available at https://github.com/JXZe/DAM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\PXF5ES4Y\\Jiang et al_2020_DAM.pdf;D\:\\Zotero\\storage\\HHVLTWQQ\\2007.html}
}

@inproceedings{jiangDefenseGridFeatures2020,
  title = {In {{Defense}} of {{Grid Features}} for {{Visual Question Answering}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  date = {2020-06},
  pages = {10264--10273},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01028},
  url = {https://ieeexplore.ieee.org/document/9157669/},
  urldate = {2021-09-09},
  abstract = {Popularized as ‘bottom-up’ attention [2], bounding box (or region) based visual features have recently surpassed vanilla grid-based convolutional features as the de facto standard for vision and language tasks like visual question answering (VQA). However, it is not clear whether the advantages of regions (e.g. better localization) are the key reasons for the success of bottom-up attention. In this paper, we revisit grid features for VQA, and find they can work surprisingly well – running more than an order of magnitude faster with the same accuracy (e.g. if pre-trained in a similar fashion). Through extensive experiments, we verify that this observation holds true across different VQA models (reporting a state-of-the-art accuracy on VQA 2.0 test-std, 72.71), datasets, and generalizes well to other tasks like image captioning. As grid features make the model design and training process much simpler, this enables us to train them end-to-end and also use a more flexible network design. We learn VQA models end-to-end, from pixels directly to answers, and show that strong performance is achievable without using any region annotations in pre-training. We hope our findings help further improve the scientific understanding and the practical application of VQA. Code and features will be made available.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {7 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\S82GAU3S\\Jiang 等。 - 2020 - In Defense of Grid Features for Visual Question An.pdf}
}

@unpublished{jiangDefenseGridFeatures2020a,
  title = {In {{Defense}} of {{Grid Features}} for {{Visual Question Answering}}},
  author = {Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  date = {2020-04-02},
  eprint = {2001.03615},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2001.03615},
  urldate = {2021-08-25},
  abstract = {Popularized as 'bottom-up' attention, bounding box (or region) based visual features have recently surpassed vanilla grid-based convolutional features as the de facto standard for vision and language tasks like visual question answering (VQA). However, it is not clear whether the advantages of regions (e.g. better localization) are the key reasons for the success of bottom-up attention. In this paper, we revisit grid features for VQA, and find they can work surprisingly well - running more than an order of magnitude faster with the same accuracy (e.g. if pre-trained in a similar fashion). Through extensive experiments, we verify that this observation holds true across different VQA models (reporting a state-of-the-art accuracy on VQA 2.0 test-std, 72.71), datasets, and generalizes well to other tasks like image captioning. As grid features make the model design and training process much simpler, this enables us to train them end-to-end and also use a more flexible network design. We learn VQA models end-to-end, from pixels directly to answers, and show that strong performance is achievable without using any region annotations in pre-training. We hope our findings help further improve the scientific understanding and the practical application of VQA. Code and features will be made available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,grid},
  file = {D\:\\Zotero\\storage\\TBV53H97\\Jiang et al_2020_In Defense of Grid Features for Visual Question Answering.pdf;D\:\\Zotero\\storage\\UXGIWJTX\\2001.html}
}

@article{jiangDivideConquerQuestionGuided2020,
  title = {Divide and {{Conquer}}: {{Question-Guided Spatio-Temporal Contextual Attention}} for {{Video Question Answering}}},
  shorttitle = {Divide and {{Conquer}}},
  author = {Jiang, Jianwen and Chen, Ziqiang and Lin, Haojie and Zhao, Xibin and Gao, Yue},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {07},
  pages = {11101--11108},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i07.6766},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6766},
  urldate = {2021-09-10},
  abstract = {Understanding questions and finding clues for answers are the key for video question answering. Compared with image question answering, video question answering (Video QA) requires to find the clues accurately on both spatial and temporal dimension simultaneously, and thus is more challenging. However, the relationship between spatio-temporal information and question still has not been well utilized in most existing methods for Video QA. To tackle this problem, we propose a Question-Guided Spatio-Temporal Contextual Attention Network (QueST) method. In QueST, we divide the semantic features generated from question into two separate parts: the spatial part and the temporal part, respectively guiding the process of constructing the contextual attention on spatial and temporal dimension. Under the guidance of the corresponding contextual attention, visual features can be better exploited on both spatial and temporal dimensions. To evaluate the effectiveness of the proposed method, experiments are conducted on TGIF-QA dataset, MSRVTTQA dataset and MSVD-QA dataset. Experimental results and comparisons with the state-of-the-art methods have shown that our method can achieve superior performance.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\9R4W6F3Y\\Jiang 等。 - 2020 - Divide and Conquer Question-Guided Spatio-Tempora.pdf}
}

@unpublished{jiangDualVDAdaptiveDual2019,
  title = {{{DualVD}}: {{An Adaptive Dual Encoding Model}} for {{Deep Visual Understanding}} in {{Visual Dialogue}}},
  shorttitle = {{{DualVD}}},
  author = {Jiang, Xiaoze and Yu, Jing and Qin, Zengchang and Zhuang, Yingying and Zhang, Xingxing and Hu, Yue and Wu, Qi},
  date = {2019-11-17},
  eprint = {1911.07251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.07251},
  urldate = {2021-12-08},
  abstract = {Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue involves multiple questions which cover a broad range of visual content that could be related to any objects, relationships or semantics. The key challenge in Visual Dialogue task is thus to learn a more comprehensive and semantic-rich image representation which may have adaptive attentions on the image for variant questions. In this research, we propose a novel model to depict an image from both visual and semantic perspectives. Specifically, the visual view helps capture the appearance-level information, including objects and their relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Futhermore, on top of such multi-view image features, we propose a feature selection framework which is able to adaptively capture question-relevant information hierarchically in fine-grained level. The proposed method achieved state-of-the-art results on benchmark Visual Dialogue datasets. More importantly, we can tell which modality (visual or semantic) has more contribution in answering the current question by visualizing the gate values. It gives us insights in understanding of human cognition in Visual Dialogue.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\CGZUKSSE\\Jiang et al_2019_DualVD.pdf;D\:\\Zotero\\storage\\Z956EQAP\\1911.html}
}

@inproceedings{jiangFantasticAnswersWhere2020,
  title = {Fantastic {{Answers}} and {{Where}} to {{Find Them}}: {{Immersive Question-Directed Visual Attention}}},
  shorttitle = {Fantastic {{Answers}} and {{Where}} to {{Find Them}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Jiang, Ming and Chen, Shi and Yang, Jinhui and Zhao, Qi},
  date = {2020-06},
  pages = {2977--2986},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00305},
  url = {https://ieeexplore.ieee.org/document/9157348/},
  urldate = {2021-09-26},
  abstract = {While most visual attention studies focus on bottom-up attention with restricted field-of-view, real-life situations are filled with embodied vision tasks. The role of attention is more significant in the latter due to the information overload, and attention to the most important regions is critical to the success of tasks. The effects of visual attention on task performance in this context have also been widely ignored. This research addresses a number of challenges to bridge this research gap, on both the data and model aspects.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@unpublished{jiangKBGNKnowledgeBridgeGraph2020,
  title = {{{KBGN}}: {{Knowledge-Bridge Graph Network}} for {{Adaptive Vision-Text Reasoning}} in {{Visual Dialogue}}},
  shorttitle = {{{KBGN}}},
  author = {Jiang, Xiaoze and Du, Siyi and Qin, Zengchang and Sun, Yajing and Yu, Jing},
  date = {2020-08-28},
  eprint = {2008.04858},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.04858},
  urldate = {2021-12-08},
  abstract = {Visual dialogue is a challenging task that needs to extract implicit information from both visual (image) and textual (dialogue history) contexts. Classical approaches pay more attention to the integration of the current question, vision knowledge and text knowledge, despising the heterogeneous semantic gaps between the cross-modal information. In the meantime, the concatenation operation has become de-facto standard to the cross-modal information fusion, which has a limited ability in information retrieval. In this paper, we propose a novel Knowledge-Bridge Graph Network (KBGN) model by using graph to bridge the cross-modal semantic relations between vision and text knowledge in fine granularity, as well as retrieving required knowledge via an adaptive information selection mode. Moreover, the reasoning clues for visual dialogue can be clearly drawn from intra-modal entities and inter-modal bridges. Experimental results on VisDial v1.0 and VisDial-Q datasets demonstrate that our model outperforms existing models with state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\BF5E98I9\\Jiang et al_2020_KBGN.pdf;D\:\\Zotero\\storage\\42VKYYXD\\2008.html}
}

@article{jiangReasoningHeterogeneousGraph2020,
  title = {Reasoning with {{Heterogeneous Graph Alignment}} for {{Video Question Answering}}},
  author = {Jiang, Pin and Han, Yahong},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {07},
  pages = {11109--11116},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i07.6767},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6767},
  urldate = {2021-09-10},
  abstract = {The dominant video question answering methods are based on fine-grained representation or model-specific attention mechanism. They usually process video and question separately, then feed the representations of different modalities into following late fusion networks. Although these methods use information of one modality to boost the other, they neglect to integrate correlations of both inter- and intra-modality in an uniform module. We propose a deep heterogeneous graph alignment network over the video shots and question words. Furthermore, we explore the network architecture from four steps: representation, fusion, alignment, and reasoning. Within our network, the inter- and intra-modality information can be aligned and interacted simultaneously over the heterogeneous graph and used for cross-modal reasoning. We evaluate our method on three benchmark datasets and conduct extensive ablation study to the effectiveness of the network architecture. Experiments show the network to be superior in quality.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4A9P69EA\\Jiang 和 Han - 2020 - Reasoning with Heterogeneous Graph Alignment for V.pdf}
}

@unpublished{jiangSelfAssemblingModularNetworks2019,
  title = {Self-{{Assembling Modular Networks}} for {{Interpretable Multi-Hop Reasoning}}},
  author = {Jiang, Yichen and Bansal, Mohit},
  date = {2019-10-30},
  eprint = {1909.05803},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1909.05803},
  urldate = {2021-10-10},
  abstract = {Multi-hop QA requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. The recently proposed HotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four different multi-hop reasoning paradigms (two bridge entity setups, checking multiple properties, and comparing two entities), making it challenging for a single neural network to handle all four. In this work, we present an interpretable, controller-based Self-Assembling Neural Modular Network (Hu et al., 2017, 2018) for multi-hop reasoning, where we design four novel modules (Find, Relocate, Compare, NoOp) to perform unique types of language reasoning. Based on a question, our layout controller RNN dynamically infers a series of reasoning modules to construct the entire network. Empirically, we show that our dynamic, multi-hop modular network achieves significant improvements over the static, single-hop baseline (on both regular and adversarial evaluation). We further demonstrate the interpretability of our model via three analyses. First, the controller can softly decompose the multi-hop question into multiple single-hop sub-questions to promote compositional reasoning behavior of the main network. Second, the controller can predict layouts that conform to the layouts designed by human experts. Finally, the intermediate module can infer the entity that connects two distantly-located supporting facts by addressing the sub-question from the controller.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\2GL4BPE9\\Jiang_Bansal_2019_Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.pdf;D\:\\Zotero\\storage\\CDL3JX7B\\1909.html}
}

@article{jianSemanticManifoldModularizationbased2021,
  title = {Semantic Manifold Modularization-Based Ranking for Image Recommendation},
  author = {Jian, Meng and Guo, Jingjing and Zhang, Chenlin and Jia, Ting and Wu, Lifang and Yang, Xun and Huo, Lina},
  date = {2021-12},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {120},
  pages = {108100},
  issn = {00313203},
  doi = {10.1016/j.patcog.2021.108100},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321002879},
  urldate = {2022-05-14},
  langid = {english},
  file = {D\:\\Zotero\\storage\\FJ4ZUNJI\\Jian 等。 - 2021 - Semantic manifold modularization-based ranking for.pdf}
}

@article{jinAdaptiveSpatioTemporalGraph2021,
  title = {Adaptive {{Spatio-Temporal Graph Enhanced Vision-Language Representation}} for {{Video QA}}},
  author = {Jin, Weike and Zhao, Zhou and Cao, Xiaochun and Zhu, Jieming and He, Xiuqiang and Zhuang, Yueting},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {30},
  pages = {5477--5489},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2021.3076556},
  url = {https://ieeexplore.ieee.org/document/9424429/},
  urldate = {2022-03-21}
}

@article{jinDiagNetBridgingText,
  title = {{{DiagNet}}: {{Bridging Text}} and {{Image}}},
  author = {Jin, Mark and Qian, Shengyi and Shen, Xiaoyang and Wen, Yi and Zheng, Xinyi},
  pages = {10},
  abstract = {Visual Question Answering (VQA) is to answer open-ended natural language questions related to images. Modern VQA tasks require reading and reasoning of both images and texts. We propose DiagNet, an attention-based neural network model that can effectively combine multiple evidence of texts, images and texts in images. Within DiagNet, a novel multi-task training strategy is used to combine answer type evidence in a hybrid fusion. We conduct comprehensive evaluation on multiple VQA tasks, and achieve competitive results. Our code is available at https://github.com/WYchelsy/DiagNet.},
  langid = {english}
}

@inproceedings{jinGoodPromptWorth2022,
  title = {A {{Good Prompt Is Worth Millions}} of {{Parameters}}: {{Low-resource Prompt-based Learning}} for {{Vision-Language Models}}},
  shorttitle = {A {{Good Prompt Is Worth Millions}} of {{Parameters}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jin, Woojeong and Cheng, Yu and Shen, Yelong and Chen, Weizhu and Ren, Xiang},
  date = {2022},
  pages = {2763--2775},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.197},
  url = {https://aclanthology.org/2022.acl-long.197},
  urldate = {2022-07-06},
  eventtitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {D\:\\Zotero\\storage\\PJ6WY8HU\\Jin 等。 - 2022 - A Good Prompt Is Worth Millions of Parameters Low.pdf}
}

@article{jingOvercomingLanguagePriors2020,
  title = {Overcoming {{Language Priors}} in {{VQA}} via {{Decomposed Linguistic Representations}}},
  author = {Jing, Chenchen and Wu, Yuwei and Zhang, Xiaoxun and Jia, Yunde and Wu, Qi},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {07},
  pages = {11181--11188},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i07.6776},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6776},
  urldate = {2021-09-10},
  abstract = {Most existing Visual Question Answering (VQA) models overly rely on language priors between questions and answers. In this paper, we present a novel method of language attention-based VQA that learns decomposed linguistic representations of questions and utilizes the representations to infer answers for overcoming language priors. We introduce a modular language attention mechanism to parse a question into three phrase representations: type representation, object representation, and concept representation. We use the type representation to identify the question type and the possible answer set (yes/no or specific concepts such as colors or numbers), and the object representation to focus on the relevant region of an image. The concept representation is verified with the attended region to infer the final answer. The proposed method decouples the language-based concept discovery and vision-based concept verification in the process of answer inference to prevent language priors from dominating the answering process. Experiments on the VQA-CP dataset demonstrate the effectiveness of our method.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2U6WLLB8\\Jing 等。 - 2020 - Overcoming Language Priors in VQA via Decomposed L.pdf}
}

@inproceedings{jingVisualSemanticGraphMatching2020,
  title = {Visual-{{Semantic Graph Matching}} for {{Visual Grounding}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Jing, Chenchen and Wu, Yuwei and Pei, Mingtao and Hu, Yao and Jia, Yunde and Wu, Qi},
  date = {2020-10-12},
  pages = {4041--4050},
  publisher = {{ACM}},
  location = {{Seattle WA USA}},
  doi = {10.1145/3394171.3413902},
  url = {https://dl.acm.org/doi/10.1145/3394171.3413902},
  urldate = {2022-03-09},
  abstract = {Visual Grounding is the task of associating entities in a natural language sentence with objects in an image. In this paper, we formulate visual grounding as a graph matching problem to find node correspondences between a visual scene graph and a language scene graph. These two graphs are heterogeneous, representing structure layouts of the sentence and image, respectively. We learn unified contextual node representations of the two graphs by using a crossmodal graph convolutional network to reduce their discrepancy. The graph matching is thus relaxed as a linear assignment problem because the learned node representations characterize both node information and structure information. A permutation loss and a semantic cycle-consistency loss are further introduced to solve the linear assignment problem with or without ground-truth correspondences. Experimental results on two visual grounding tasks, i.e., referring expression comprehension and phrase localization, demonstrate the effectiveness of our method.},
  eventtitle = {{{MM}} '20: {{The}} 28th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-7988-5},
  langid = {english}
}

@unpublished{jinRUArtNovelTextCentered2020,
  title = {{{RUArt}}: {{A Novel Text-Centered Solution}} for {{Text-Based Visual Question Answering}}},
  shorttitle = {{{RUArt}}},
  author = {Jin, Zan-Xia and Wu, Heran and Yang, Chun and Zhou, Fang and Qin, Jingyan and Xiao, Lei and Yin, Xu-Cheng},
  date = {2020-10-24},
  eprint = {2010.12917},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.12917},
  urldate = {2021-12-08},
  abstract = {Text-based visual question answering (VQA) requires to read and understand text in an image to correctly answer a given question. However, most current methods simply add optical character recognition (OCR) tokens extracted from the image into the VQA model without considering contextual information of OCR tokens and mining the relationships between OCR tokens and scene objects. In this paper, we propose a novel text-centered method called RUArt (Reading, Understanding and Answering the Related Text) for text-based VQA. Taking an image and a question as input, RUArt first reads the image and obtains text and scene objects. Then, it understands the question, OCRed text and objects in the context of the scene, and further mines the relationships among them. Finally, it answers the related text for the given question through text semantic matching and reasoning. We evaluate our RUArt on two text-based VQA benchmarks (ST-VQA and TextVQA) and conduct extensive ablation studies for exploring the reasons behind RUArt's effectiveness. Experimental results demonstrate that our method can effectively explore the contextual information of the text and mine the stable relationships between the text and objects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\2AQADTZU\\Jin et al_2020_RUArt.pdf;D\:\\Zotero\\storage\\NF8IA2D4\\2010.html}
}

@inproceedings{johnsonCLEVRDiagnosticDataset2017,
  title = {{{CLEVR}}: {{A Diagnostic Dataset}} for {{Compositional Language}} and {{Elementary Visual Reasoning}}},
  shorttitle = {{{CLEVR}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
  options = {useprefix=true},
  date = {2017-07},
  pages = {1988--1997},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.215},
  url = {https://ieeexplore.ieee.org/document/8099698/},
  urldate = {2021-09-11},
  abstract = {When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@inproceedings{johnsonCLEVRDiagnosticDataset2017a,
  title = {{{CLEVR}}: {{A Diagnostic Dataset}} for {{Compositional Language}} and {{Elementary Visual Reasoning}}},
  shorttitle = {{{CLEVR}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
  options = {useprefix=true},
  date = {2017-07},
  pages = {1988--1997},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.215},
  url = {https://ieeexplore.ieee.org/document/8099698/},
  urldate = {2021-09-11},
  abstract = {When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@inproceedings{johnsonCLEVRDiagnosticDataset2017b,
  title = {{{CLEVR}}: {{A Diagnostic Dataset}} for {{Compositional Language}} and {{Elementary Visual Reasoning}}},
  shorttitle = {{{CLEVR}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
  options = {useprefix=true},
  date = {2017-07},
  pages = {1988--1997},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.215},
  url = {https://ieeexplore.ieee.org/document/8099698/},
  urldate = {2021-09-11},
  abstract = {When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@inproceedings{jollyEaSeDiagnosticTool2021,
  title = {{{EaSe}}: {{A Diagnostic Tool}} for {{VQA}} Based on {{Answer Diversity}}},
  shorttitle = {{{EaSe}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Jolly, Shailza and Pezzelle, Sandro and Nabi, Moin},
  date = {2021-06},
  pages = {2407--2414},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.192},
  url = {https://aclanthology.org/2021.naacl-main.192},
  urldate = {2021-09-10},
  abstract = {We propose EASE, a simple diagnostic tool for Visual Question Answering (VQA) which quantifies the difficulty of an image, question sample. EASE is based on the pattern of answers provided by multiple annotators to a given question. In particular, it considers two aspects of the answers: (i) their Entropy; (ii) their Semantic content. First, we prove the validity of our diagnostic to identify samples that are easy/hard for state-of-art VQA models. Second, we show that EASE can be successfully used to select the most-informative samples for training/fine-tuning. Crucially, only information that is readily available in any VQA dataset is used to compute its scores.},
  eventtitle = {{{NAACL-HLT}} 2021},
  file = {D\:\\Zotero\\storage\\ZTYYLTEA\\Jolly et al_2021_EaSe.pdf}
}

@article{jouppiInDatacenterPerformanceAnalysis,
  title = {In-{{Datacenter Performance Analysis}} of a {{Tensor Processing UnitTM}}},
  author = {Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  pages = {17},
  langid = {english},
  file = {D\:\\Zotero\\storage\\39ISVTIL\\Jouppi 等。 - In-Datacenter Performance Analysis of a Tensor Pro.pdf}
}

@inproceedings{kafleAnalysisVisualQuestion2017,
  title = {An {{Analysis}} of {{Visual Question Answering Algorithms}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Kafle, Kushal and Kanan, Christopher},
  date = {2017-10},
  pages = {1983--1991},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.217},
  url = {http://ieeexplore.ieee.org/document/8237479/},
  urldate = {2021-09-10},
  abstract = {In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset called the Task Driven Image Understanding Challenge (TDIUC), which has over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BZJ4GTD9\\Kafle 和 Kanan - 2017 - An Analysis of Visual Question Answering Algorithm.pdf}
}

@article{kafleAnsweringQuestionsData,
  title = {Answering {{Questions}} about {{Data Visualizations}} Using {{Efficient Bimodal Fusion}}},
  author = {Kafle, Kushal and Shrestha, Robik and Cohen, Scott and Price, Brian and Kanan, Christopher},
  pages = {10},
  abstract = {Chart question answering (CQA) is a newly proposed visual question answering (VQA) task where an algorithm must answer questions about data visualizations, e.g. bar charts, pie charts, and line graphs. CQA requires capabilities that natural-image VQA algorithms lack: fine-grained measurements, optical character recognition, and handling out-of-vocabulary words in both questions and answers. Without modifications, state-of-the-art VQA algorithms perform poorly on this task. Here, we propose a novel CQA algorithm called parallel recurrent fusion of image and language (PReFIL). PReFIL first learns bimodal embeddings by fusing question and image features and then intelligently aggregates these learned embeddings to answer the given question. Despite its simplicity, PReFIL greatly surpasses state-of-the art systems and human baselines on both the FigureQA and DVQA datasets. Additionally, we demonstrate that PReFIL can be used to reconstruct tables by asking a series of questions about a chart.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\3HQTVEHN\\Kafle 等。 - Answering Questions about Data Visualizations usin.pdf}
}

@inproceedings{kafleDVQAUnderstandingData2018,
  title = {{{DVQA}}: {{Understanding Data Visualizations}} via {{Question Answering}}},
  shorttitle = {{{DVQA}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
  date = {2018-06},
  pages = {5648--5656},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00592},
  url = {https://ieeexplore.ieee.org/document/8578690/},
  urldate = {2021-09-09},
  abstract = {Bar charts are an effective way to convey numeric information, but today’s algorithms cannot parse them. Existing methods fail when faced with even minor variations in appearance. Here, we present DVQA, a dataset that tests many aspects of bar chart understanding in a question answering framework. Unlike visual question answering (VQA), DVQA requires processing words and answers that are unique to a particular bar chart. State-of-the-art VQA algorithms perform poorly on DVQA, and we propose two strong baselines that perform considerably better. Our work will enable algorithms to automatically extract numeric and semantic information from vast quantities of bar charts found in scientific publications, Internet articles, business reports, and many other areas.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {33 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\CBVPU4H2\\Kafle 等。 - 2018 - DVQA Understanding Data Visualizations via Questi.pdf}
}

@inproceedings{kaiLearningGenerateVisual2021,
  title = {Learning to {{Generate Visual Questions}} with {{Noisy Supervision}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kai, Shen and Wu, Lingfei and Tang, Siliang and Zhuang, Yueting and {he}, zhen and Ding, Zhuoye and Xiao, Yun and Long, Bo},
  date = {2021},
  volume = {34},
  pages = {11604--11617},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2021/hash/60792d855cd8a912a97711f91a1f155c-Abstract.html},
  urldate = {2022-04-25},
  abstract = {The task of visual question generation (VQG) aims to generate human-like neural questions from an image and potentially other side information (e.g., answer type or the answer itself). Existing works often suffer from the severe one image to many questions mapping problem, which generates uninformative and non-referential questions. Recent work has demonstrated that by leveraging double visual and answer hints, a model can faithfully generate much better quality questions. However, visual hints are not available naturally. Despite they proposed a simple rule-based similarity matching method to obtain candidate visual hints, they could be very noisy practically and thus restrict the quality of generated questions. In this paper, we present a novel learning approach for double-hints based VQG, which can be cast as a weakly supervised learning problem with noises. The key rationale is that the salient visual regions of interest can be viewed as a constraint to improve the generation procedure for producing high-quality questions. As a result, given the predicted salient visual regions of interest, we can focus on estimating the probability of being ground-truth questions, which in turn implicitly measures the quality of predicted visual hints. Experimental results on two benchmark datasets show that our proposed method outperforms the state-of-the-art approaches by a large margin on a variety of metrics, including both automatic machine metrics and human evaluation.},
  file = {D\:\\Zotero\\storage\\WDP52EFF\\Kai et al_2021_Learning to Generate Visual Questions with Noisy Supervision.pdf}
}

@article{kairouzAdvancesOpenProblems,
  title = {Advances and {{Open Problems}} in {{Federated Learning}}},
  author = {Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aurelien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D’Oliveira, Rafael G L and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascon, Adria and Ghazi, Badih and Gibbons, Phillip B and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konecˇny, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrede and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard},
  pages = {105},
  abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\MP294EKC\\1912.04977.pdf}
}

@unpublished{kamathMDETRModulatedDetection2021,
  title = {{{MDETR}} -- {{Modulated Detection}} for {{End-to-End Multi-Modal Understanding}}},
  author = {Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Misra, Ishan and Synnaeve, Gabriel and Carion, Nicolas},
  date = {2021-04-26},
  eprint = {2104.12763},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.12763},
  urldate = {2021-09-26},
  abstract = {Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\JZXWFT8H\\Kamath et al_2021_MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding.pdf;D\:\\Zotero\\storage\\3XCBCNUT\\2104.html}
}

@unpublished{kangDualAttentionNetworks2019,
  title = {Dual {{Attention Networks}} for {{Visual Reference Resolution}} in {{Visual Dialog}}},
  author = {Kang, Gi-Cheon and Lim, Jaeseo and Zhang, Byoung-Tak},
  date = {2019-08-28},
  eprint = {1902.09368},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.09368},
  urldate = {2021-12-08},
  abstract = {Visual dialog (VisDial) is a task which requires an AI agent to answer a series of questions grounded in an image. Unlike in visual question answering (VQA), the series of questions should be able to capture a temporal context from a dialog history and exploit visually-grounded information. A problem called visual reference resolution involves these challenges, requiring the agent to resolve ambiguous references in a given question and find the references in a given image. In this paper, we propose Dual Attention Networks (DAN) for visual reference resolution. DAN consists of two kinds of attention networks, REFER and FIND. Specifically, REFER module learns latent relationships between a given question and a dialog history by employing a self-attention mechanism. FIND module takes image features and reference-aware representations (i.e., the output of REFER module) as input, and performs visual grounding via bottom-up attention mechanism. We qualitatively and quantitatively evaluate our model on VisDial v1.0 and v0.9 datasets, showing that DAN outperforms the previous state-of-the-art model by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\BJVUREGP\\Kang et al_2019_Dual Attention Networks for Visual Reference Resolution in Visual Dialog.pdf;D\:\\Zotero\\storage\\WHFI38YC\\1902.html}
}

@inproceedings{kantContrastClassifyTraining2021,
  title = {Contrast and {{Classify}}: {{Training Robust VQA Models}}},
  shorttitle = {Contrast and {{Classify}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Kant, Yash and Moudgil, Abhinav and Batra, Dhruv and Parikh, Devi and Agrawal, Harsh},
  date = {2021-10},
  pages = {1584--1593},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00163},
  url = {https://ieeexplore.ieee.org/document/9711203/},
  urldate = {2022-03-10},
  abstract = {Recent Visual Question Answering (VQA) models have shown impressive performance on the VQA benchmark but remain sensitive to small linguistic variations in input questions. Existing approaches address this by augmenting the dataset with question paraphrases from visual question generation models or adversarial perturbations. These approaches use the combined data to learn an answer classifier by minimizing the standard cross-entropy loss. To more effectively leverage augmented data, we build on the recent success in contrastive learning. We propose a novel training paradigm (ConClaT) that optimizes both cross-entropy and contrastive losses. The contrastive loss encourages representations to be robust to linguistic variations in questions while the cross-entropy loss preserves the discriminative power of representations for answer prediction.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english}
}

@unpublished{kantContrastClassifyTraining2021a,
  title = {Contrast and {{Classify}}: {{Training Robust VQA Models}}},
  shorttitle = {Contrast and {{Classify}}},
  author = {Kant, Yash and Moudgil, Abhinav and Batra, Dhruv and Parikh, Devi and Agrawal, Harsh},
  date = {2021-04-18},
  eprint = {2010.06087},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.06087},
  urldate = {2021-10-16},
  abstract = {Recent Visual Question Answering (VQA) models have shown impressive performance on the VQA benchmark but remain sensitive to small linguistic variations in input questions. Existing approaches address this by augmenting the dataset with question paraphrases from visual question generation models or adversarial perturbations. These approaches use the combined data to learn an answer classifier by minimizing the standard cross-entropy loss. To more effectively leverage augmented data, we build on the recent success in contrastive learning. We propose a novel training paradigm (ConClaT) that optimizes both cross-entropy and contrastive losses. The contrastive loss encourages representations to be robust to linguistic variations in questions while the cross-entropy loss preserves the discriminative power of representations for answer prediction. We find that optimizing both losses -- either alternately or jointly -- is key to effective training. On the VQA-Rephrasings benchmark, which measures the VQA model's answer consistency across human paraphrases of a question, ConClaT improves Consensus Score by 1 .63\% over an improved baseline. In addition, on the standard VQA 2.0 benchmark, we improve the VQA accuracy by 0.78\% overall. We also show that ConClaT is agnostic to the type of data-augmentation strategy used.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\FPSJXMJP\\Kant et al_2021_Contrast and Classify.pdf;D\:\\Zotero\\storage\\4CKK244M\\2010.html}
}

@unpublished{kantSpatiallyAwareMultimodal2020,
  title = {Spatially {{Aware Multimodal Transformers}} for {{TextVQA}}},
  author = {Kant, Yash and Batra, Dhruv and Anderson, Peter and Schwing, Alex and Parikh, Devi and Lu, Jiasen and Agrawal, Harsh},
  date = {2020-12-22},
  eprint = {2007.12146},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2007.12146},
  urldate = {2022-02-08},
  abstract = {Textual cues are essential for everyday tasks like buying groceries and using public transport. To develop this assistive technology, we study the TextVQA task, i.e., reasoning about text in images to answer a question. Existing approaches are limited in their use of spatial relations and rely on fully-connected transformer-like architectures to implicitly learn the spatial structure of a scene. In contrast, we propose a novel spatially aware self-attention layer such that each visual entity only looks at neighboring entities defined by a spatial graph. Further, each head in our multi-head self-attention layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2\% overall over an improved baseline, and 4.62\% on questions that involve spatial reasoning and can be answered correctly using OCR tokens. Similarly on ST-VQA, we improve the absolute accuracy by 4.2\%. We further show that spatially aware self-attention improves visual grounding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\2G5KQ49I\\Kant et al_2020_Spatially Aware Multimodal Transformers for TextVQA.pdf;D\:\\Zotero\\storage\\HQNP92SN\\2007.html}
}

@unpublished{kantSpatiallyAwareMultimodal2020a,
  title = {Spatially {{Aware Multimodal Transformers}} for {{TextVQA}}},
  author = {Kant, Yash and Batra, Dhruv and Anderson, Peter and Schwing, Alex and Parikh, Devi and Lu, Jiasen and Agrawal, Harsh},
  date = {2020-12-22},
  eprint = {2007.12146},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2007.12146},
  urldate = {2021-12-08},
  abstract = {Textual cues are essential for everyday tasks like buying groceries and using public transport. To develop this assistive technology, we study the TextVQA task, i.e., reasoning about text in images to answer a question. Existing approaches are limited in their use of spatial relations and rely on fully-connected transformer-like architectures to implicitly learn the spatial structure of a scene. In contrast, we propose a novel spatially aware self-attention layer such that each visual entity only looks at neighboring entities defined by a spatial graph. Further, each head in our multi-head self-attention layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2\% overall over an improved baseline, and 4.62\% on questions that involve spatial reasoning and can be answered correctly using OCR tokens. Similarly on ST-VQA, we improve the absolute accuracy by 4.2\%. We further show that spatially aware self-attention improves visual grounding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\TXKXFZTP\\Kant et al_2020_Spatially Aware Multimodal Transformers for TextVQA.pdf;D\:\\Zotero\\storage\\F5N4GYBT\\2007.html}
}

@incollection{kantSpatiallyAwareMultimodal2020b,
  title = {Spatially {{Aware Multimodal Transformers}} for {{TextVQA}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Kant, Yash and Batra, Dhruv and Anderson, Peter and Schwing, Alexander and Parikh, Devi and Lu, Jiasen and Agrawal, Harsh},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12354},
  pages = {715--732},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58545-7_41},
  url = {https://link.springer.com/10.1007/978-3-030-58545-7_41},
  urldate = {2021-09-10},
  abstract = {Textual cues are essential for everyday tasks like buying groceries and using public transport. To develop this assistive technology, we study the TextVQA task, i.e., reasoning about text in images to answer a question. Existing approaches are limited in their use of spatial relations and rely on fully-connected transformer-based architectures to implicitly learn the spatial structure of a scene. In contrast, we propose a novel spatially aware self-attention layer such that each visual entity only looks at neighboring entities defined by a spatial graph. Further, each head in our multi-head self-attention layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2\% overall over an improved baseline, and 4.62\% on questions that involve spatial reasoning and can be answered correctly using OCR tokens. Similarly on ST-VQA, we improve the absolute accuracy by 4.2\%. We further show that spatially aware self-attention improves visual grounding.},
  isbn = {978-3-030-58544-0 978-3-030-58545-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\EZNI24BS\\Kant 等。 - 2020 - Spatially Aware Multimodal Transformers for TextVQ.pdf}
}

@inproceedings{karamchetiMindYourOutliers2021,
  title = {Mind {{Your Outliers}}! {{Investigating}} the {{Negative Impact}} of {{Outliers}} on {{Active Learning}} for {{Visual Question Answering}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Karamcheti, Siddharth and Krishna, Ranjay and Fei-Fei, Li and Manning, Christopher},
  date = {2021-08},
  pages = {7265--7281},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-long.564},
  url = {https://aclanthology.org/2021.acl-long.564},
  urldate = {2021-09-09},
  abstract = {Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers – groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  keywords = {Outliers},
  file = {D\:\\Zotero\\storage\\7VR8QZPD\\Karamcheti et al_2021_Mind Your Outliers.pdf}
}

@inproceedings{karamchetiMindYourOutliers2021a,
  title = {Mind {{Your Outliers}}! {{Investigating}} the {{Negative Impact}} of {{Outliers}} on {{Active Learning}} for {{Visual Question Answering}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Karamcheti, Siddharth and Krishna, Ranjay and Fei-Fei, Li and Manning, Christopher},
  date = {2021},
  pages = {7265--7281},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-long.564},
  url = {https://aclanthology.org/2021.acl-long.564},
  urldate = {2021-08-12},
  abstract = {Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers – groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.},
  eventtitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {D\:\\Zotero\\storage\\UNRRVHU8\\Karamcheti 等。 - 2021 - Mind Your Outliers! Investigating the Negative Imp.pdf}
}

@inproceedings{kervadecHowTransferableAre2021,
  title = {How {{Transferable}} Are {{Reasoning Patterns}} in {{VQA}}?},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kervadec, Corentin and Jaunet, Theo and Antipov, Grigory and Baccouche, Moez and Vuillemot, Romain and Wolf, Christian},
  date = {2021-06},
  pages = {4205--4214},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00419},
  url = {https://ieeexplore.ieee.org/document/9577924/},
  urldate = {2022-03-23},
  abstract = {Since its inception, Visual Question Answering (VQA) is notoriously known as a task, where models are prone to exploit biases in datasets to find shortcuts instead of performing high-level reasoning. Classical methods address this by removing biases from training data, or adding branches to models to detect and remove biases. In this paper, we argue that uncertainty in vision is a dominating factor preventing the successful learning of reasoning in vision and language problems. We train a visual oracle and in a large scale study provide experimental evidence that it is much less prone to exploiting spurious dataset biases compared to standard models. We propose to study the attention mechanisms at work in the visual oracle and compare them with a SOTA Transformer-based model. We provide an in-depth analysis and visualizations of reasoning patterns obtained with an online visualization tool which we make publicly available1. We exploit these insights by transferring reasoning patterns from the oracle to a SOTA Transformer-based VQA model taking standard noisy visual inputs via fine-tuning. In experiments we report higher overall accuracy, as well as accuracy on infrequent answers for each question type, which provides evidence for improved generalization and a decrease of the dependency on dataset biases.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\R7WE8EMB\\Kervadec 等。 - 2021 - How Transferable are Reasoning Patterns in VQA.pdf}
}

@inproceedings{kervadecHowTransferableAre2021a,
  title = {How {{Transferable Are Reasoning Patterns}} in {{VQA}}?},
  author = {Kervadec, Corentin and Jaunet, Theo and Antipov, Grigory and Baccouche, Moez and Vuillemot, Romain and Wolf, Christian},
  date = {2021},
  pages = {4207--4216},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Kervadec_How_Transferable_Are_Reasoning_Patterns_in_VQA_CVPR_2021_paper.html},
  urldate = {2021-09-09},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  keywords = {Reasoning},
  file = {D\:\\Zotero\\storage\\PPY5P8X6\\Kervadec et al_2021_How Transferable Are Reasoning Patterns in VQA.pdf;D\:\\Zotero\\storage\\LC3FB87M\\Kervadec_How_Transferable_Are_Reasoning_Patterns_in_VQA_CVPR_2021_paper.html}
}

@article{kervadecRosesAreRed,
  title = {Roses {{Are Red}}, {{Violets Are Blue}}... but {{Should VQA Expect Them To}}?},
  author = {Kervadec, Corentin and Antipov, Grigory and Baccouche, Moez and Wolf, Christian},
  pages = {10},
  abstract = {Models for Visual Question Answering (VQA) are notorious for their tendency to rely on dataset biases, as the large and unbalanced diversity of questions and concepts involved and tends to prevent models from learning to “reason”, leading them to perform “educated guesses” instead. In this paper, we claim that the standard evaluation metric, which consists in measuring the overall in-domain accuracy, is misleading. Since questions and concepts are unbalanced, this tends to favor models which exploit subtle training set statistics. Alternatively, naively introducing artificial distribution shifts between train and test splits is also not completely satisfying. First, the shifts do not reflect real-world tendencies, resulting in unsuitable models; second, since the shifts are handcrafted, trained models are specifically designed for this particular setting, and do not generalize to other configurations. We propose the GQAOOD benchmark designed to overcome these concerns: we measure and compare accuracy over both rare and frequent question-answer pairs, and argue that the former is better suited to the evaluation of reasoning abilities, which we experimentally validate with models trained to more or less exploit biases. In a large-scale study involving 7 VQA models and 3 bias reduction techniques, we also experimentally demonstrate that these models fail to address questions involving infrequent concepts and provide recommendations for future directions of research.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\Z39P9AW5\\Kervadec 等。 - Roses Are Red, Violets Are Blue... but Should VQA .pdf}
}

@article{kervadecRosesAreReda,
  title = {Roses {{Are Red}}, {{Violets Are Blue}}... but {{Should VQA Expect Them To}}?},
  author = {Kervadec, Corentin and Antipov, Grigory and Baccouche, Moez and Wolf, Christian},
  pages = {10},
  abstract = {Models for Visual Question Answering (VQA) are notorious for their tendency to rely on dataset biases, as the large and unbalanced diversity of questions and concepts involved and tends to prevent models from learning to “reason”, leading them to perform “educated guesses” instead. In this paper, we claim that the standard evaluation metric, which consists in measuring the overall in-domain accuracy, is misleading. Since questions and concepts are unbalanced, this tends to favor models which exploit subtle training set statistics. Alternatively, naively introducing artificial distribution shifts between train and test splits is also not completely satisfying. First, the shifts do not reflect real-world tendencies, resulting in unsuitable models; second, since the shifts are handcrafted, trained models are specifically designed for this particular setting, and do not generalize to other configurations. We propose the GQAOOD benchmark designed to overcome these concerns: we measure and compare accuracy over both rare and frequent question-answer pairs, and argue that the former is better suited to the evaluation of reasoning abilities, which we experimentally validate with models trained to more or less exploit biases. In a large-scale study involving 7 VQA models and 3 bias reduction techniques, we also experimentally demonstrate that these models fail to address questions involving infrequent concepts and provide recommendations for future directions of research.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\7SX4W38L\\Kervadec 等。 - Roses Are Red, Violets Are Blue... but Should VQA .pdf}
}

@inproceedings{khademiMultimodalNeuralGraph2020,
  title = {Multimodal {{Neural Graph Memory Networks}} for {{Visual Question Answering}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Khademi, Mahmoud},
  date = {2020-07},
  pages = {7177--7188},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.643},
  url = {https://aclanthology.org/2020.acl-main.643},
  urldate = {2021-10-09},
  abstract = {We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering. The MN-GMN uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, Graph Network (GN), to reason about objects and their interactions in an image. The input module of the MN-GMN generates a set of visual features plus a set of encoded region-grounded captions (RGCs) for the image. The RGCs capture object attributes and their relationships. Two GNs are constructed from the input module using the visual features and encoded RGCs. Each node of the GNs iteratively computes a question-guided contextualized representation of the visual/textual information assigned to it. Then, to combine the information from both GNs, the nodes write the updated representations to an external spatial memory. The final states of the memory cells are fed into an answer module to predict an answer. Experiments show MN-GMN rivals the state-of-the-art models on Visual7W, VQA-v2.0, and CLEVR datasets.},
  eventtitle = {{{ACL}} 2020},
  file = {D\:\\Zotero\\storage\\XES3YZ8F\\Khademi_2020_Multimodal Neural Graph Memory Networks for Visual Question Answering.pdf}
}

@unpublished{khanFoundReasonMe2021,
  title = {Found a {{Reason}} for Me? {{Weakly-supervised Grounded Visual Question Answering}} Using {{Capsules}}},
  shorttitle = {Found a {{Reason}} for Me?},
  author = {Khan, Aisha Urooj and Kuehne, Hilde and Duarte, Kevin and Gan, Chuang and Lobo, Niels and Shah, Mubarak},
  date = {2021-05-11},
  eprint = {2105.04836},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.04836},
  urldate = {2022-01-30},
  abstract = {The problem of grounding VQA tasks has seen an increased attention in the research community recently, with most attempts usually focusing on solving this task by using pretrained object detectors. However, pre-trained object detectors require bounding box annotations for detecting relevant objects in the vocabulary, which may not always be feasible for real-life large-scale applications. In this paper, we focus on a more relaxed setting: the grounding of relevant visual entities in a weakly supervised manner by training on the VQA task alone. To address this problem, we propose a visual capsule module with a query-based selection mechanism of capsule features, that allows the model to focus on relevant regions based on the textual cues about visual information in the question. We show that integrating the proposed capsule module in existing VQA systems significantly improves their performance on the weakly supervised grounding task. Overall, we demonstrate the effectiveness of our approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the CLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with ground truth bounding boxes for objects that are relevant for the correct answer, as well as on GQA, a real world VQA dataset with compositional questions. We show that the systems with the proposed capsule module consistently outperform the respective baseline systems in terms of answer grounding, while achieving comparable performance on VQA task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\N6Z6XTEI\\Khan et al_2021_Found a Reason for me.pdf;D\:\\Zotero\\storage\\PUU9HIEM\\2105.html}
}

@unpublished{kilDiscoveringUnknownKnowns2021,
  title = {Discovering the {{Unknown Knowns}}: {{Turning Implicit Knowledge}} in the {{Dataset}} into {{Explicit Training Examples}} for {{Visual Question Answering}}},
  shorttitle = {Discovering the {{Unknown Knowns}}},
  author = {Kil, Jihyung and Zhang, Cheng and Xuan, Dong and Chao, Wei-Lun},
  date = {2021-09-13},
  eprint = {2109.06122},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.06122},
  urldate = {2021-09-23},
  abstract = {Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples -- there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the "unknowns" to the learned VQA model are indeed "known" in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the "how many" question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SimpleAug to turn this "known" knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models' performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https://github.com/heendung/simpleAUG.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\ZLUU6KY8\\Kil et al_2021_Discovering the Unknown Knowns.pdf;D\:\\Zotero\\storage\\BV85U42H\\2109.html}
}

@misc{kilPreSTUPreTrainingSceneText2022,
  title = {{{PreSTU}}: {{Pre-Training}} for {{Scene-Text Understanding}}},
  shorttitle = {{{PreSTU}}},
  author = {Kil, Jihyung and Changpinyo, Soravit and Chen, Xi and Hu, Hexiang and Goodman, Sebastian and Chao, Wei-Lun and Soricut, Radu},
  date = {2022-09-12},
  number = {arXiv:2209.05534},
  eprint = {2209.05534},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.05534},
  urldate = {2022-10-13},
  abstract = {The ability to read and reason about texts in an image is often lacking in vision-and-language (V\&L) models. How can we learn V\&L models that exhibit strong scene-text understanding (STU)? In this paper, we propose PreSTU, a simple pre-training recipe specifically designed for scene-text understanding. PreSTU combines a simple OCR-aware pre-training objective with a large-scale image-text dataset with off-the-shelf OCR signals. We empirically demonstrate the superiority of this pre-training objective on TextVQA, TextCaps, ST-VQA, and VizWiz-VQA. We also study which factors affect STU performance, where we highlight the importance of image resolution and dataset scale during pre-training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\BPVZKTAW\\Kil et al_2022_PreSTU.pdf;D\:\\Zotero\\storage\\IZIHEFIG\\2209.html}
}

@unpublished{kimDenseCaptionMatchingFrameSelection2020,
  title = {Dense-{{Caption Matching}} and {{Frame-Selection Gating}} for {{Temporal Localization}} in {{VideoQA}}},
  author = {Kim, Hyounghun and Tang, Zineng and Bansal, Mohit},
  date = {2020-05-13},
  eprint = {2005.06409},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.06409},
  urldate = {2021-09-09},
  abstract = {Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09\% versus 70.52\%). We also present several word, object, and frame level visualization studies. Our code is publicly available at: https://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,VideoQA},
  file = {D\:\\Zotero\\storage\\Q5Q9NP26\\Kim et al_2020_Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in.pdf;D\:\\Zotero\\storage\\2V38EGKU\\2005.html}
}

@inproceedings{kimHypergraphAttentionNetworks2020,
  title = {Hypergraph {{Attention Networks}} for {{Multimodal Learning}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kim, Eun-Sol and Kang, Woo Young and On, Kyoung-Woon and Heo, Yu-Jung and Zhang, Byoung-Tak},
  date = {2020-06},
  pages = {14569--14578},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01459},
  url = {https://ieeexplore.ieee.org/document/9157723/},
  urldate = {2022-02-07},
  abstract = {One of the fundamental problems that arise in multimodal learning tasks is the disparity of information levels between different modalities. To resolve this problem, we propose Hypergraph Attention Networks (HANs), which define a common semantic space among the modalities with symbolic graphs and extract a joint representation of the modalities based on a co-attention map constructed in the semantic space. HANs follow the process: constructing the common semantic space with symbolic graphs of each modality, matching the semantics between sub-structures of the symbolic graphs, constructing co-attention maps between the graphs in the semantic space, and integrating the multimodal inputs using the co-attention maps to get the final joint representation. From the qualitative analysis with two Visual Question and Answering datasets, we discover that 1) the alignment of the information levels between the modalities is important, and 2) the symbolic graphs are very powerful ways to represent the information of the low-level signals in alignment. Moreover, HANs dramatically improve the state-of-the-art accuracy on the GQA dataset from 54.6\% to 61.88\% only using the symbolic information in quantitatively.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@inproceedings{kimImageCaptioningVery2019,
  title = {Image {{Captioning}} with {{Very Scarce Supervised Data}}: {{Adversarial Semi-Supervised Learning Approach}}},
  shorttitle = {Image {{Captioning}} with {{Very Scarce Supervised Data}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Kim, Dong-Jin and Choi, Jinsoo and Oh, Tae-Hyun and Kweon, In So},
  date = {2019},
  pages = {2012--2023},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1208},
  url = {https://aclanthology.org/D19-1208},
  urldate = {2022-04-20},
  abstract = {Constructing an organized dataset comprised of a large number of images and several captions for each image is a laborious task, which requires vast human effort. On the other hand, collecting a large number of images and sentences separately may be immensely easier. In this paper, we develop a novel data-efficient semi-supervised framework for training an image captioning model. We leverage massive unpaired image and caption data by learning to associate them. To this end, our proposed semi-supervised learning method assigns pseudo-labels to unpaired samples via Generative Adversarial Networks to learn the joint distribution of image and caption. To evaluate, we construct scarcely-paired COCO dataset, a modified version of MS COCO caption dataset. The empirical results show the effectiveness of our method compared to several strong baselines, especially when the amount of the paired samples are scarce.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019},
  file = {D\:\\Zotero\\storage\\K39B2655\\Kim et al_2019_Image Captioning with Very Scarce Supervised Data.pdf}
}

@unpublished{kimModalityBalancedModelsVisual2020,
  title = {Modality-{{Balanced Models}} for {{Visual Dialogue}}},
  author = {Kim, Hyounghun and Tan, Hao and Bansal, Mohit},
  date = {2020-01-17},
  eprint = {2001.06354},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2001.06354},
  urldate = {2021-12-08},
  abstract = {The Visual Dialog task requires a model to exploit both image and conversational context information to generate the next response to the dialogue. However, via manual analysis, we find that a large number of conversational questions can be answered by only looking at the image without any access to the context history, while others still need the conversation context to predict the correct answers. We demonstrate that due to this reason, previous joint-modality (history and image) models over-rely on and are more prone to memorizing the dialogue history (e.g., by extracting certain keywords or patterns in the context information), whereas image-only models are more generalizable (because they cannot memorize or extract keywords from history) and perform substantially better at the primary normalized discounted cumulative gain (NDCG) task metric which allows multiple correct answers. Hence, this observation encourages us to explicitly maintain two models, i.e., an image-only model and an image-history joint model, and combine their complementary abilities for a more balanced multimodal model. We present multiple methods for this integration of the two models, via ensemble and consensus dropout fusion with shared parameters. Empirically, our models achieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG and high balance across metrics), and substantially outperform the winner of the Visual Dialog challenge 2018 on most metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\FXLJYRV6\\Kim et al_2020_Modality-Balanced Models for Visual Dialogue.pdf;D\:\\Zotero\\storage\\6ICLR8WL\\2001.html}
}

@incollection{kimMultimodalDualAttention2018,
  title = {Multimodal {{Dual Attention Memory}} for {{Video Story Question Answering}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Kim, Kyung-Min and Choi, Seong-Ho and Kim, Jin-Hwa and Zhang, Byoung-Tak},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11219},
  pages = {698--713},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01267-0_41},
  url = {http://link.springer.com/10.1007/978-3-030-01267-0_41},
  urldate = {2021-09-10},
  abstract = {We propose a video story question-answering (QA) architecture, Multimodal Dual Attention Memory (MDAM). The key idea is to use a dual attention mechanism with late fusion. MDAM uses selfattention to learn the latent concepts in scene frames and captions. Given a question, MDAM uses the second attention over these latent concepts. Multimodal fusion is performed after the dual attention processes (late fusion). Using this processing pipeline, MDAM learns to infer a high-level vision-language joint representation from an abstraction of the full video content. We evaluate MDAM on PororoQA and MovieQA datasets which have large-scale QA annotations on cartoon videos and movies, respectively. For both datasets, MDAM achieves new state-of-the-art results with significant margins compared to the runner-up models. We confirm the best performance of the dual attention mechanism combined with late fusion by ablation studies. We also perform qualitative analysis by visualizing the inference mechanisms of MDAM.},
  isbn = {978-3-030-01266-3 978-3-030-01267-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZV5N3GXZ\\Kim 等。 - 2018 - Multimodal Dual Attention Memory for Video Story Q.pdf}
}

@inproceedings{kimProgressiveAttentionMemory2019,
  title = {Progressive {{Attention Memory Network}} for {{Movie Story Question Answering}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kim, Junyeong and Ma, Minuk and Kim, Kyungsu and Kim, Sungjin and Yoo, Chang D.},
  date = {2019-06},
  pages = {8329--8338},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00853},
  url = {https://ieeexplore.ieee.org/document/8954098/},
  urldate = {2021-03-11},
  abstract = {This paper proposes the progressive attention memory network (PAMN) for movie story question answering (QA). Movie story QA is challenging compared to VQA in two aspects: (1) pinpointing the temporal parts relevant to answer the question is difficult as the movies are typically longer than an hour, (2) it has both video and subtitle where different questions require different modality to infer the answer. To overcome these challenges, PAMN involves three main features: (1) progressive attention mechanism that utilizes cues from both question and answer to progressively prune out irrelevant temporal parts in memory, (2) dynamic modality fusion that adaptively determines the contribution of each modality for answering the current question, and (3) belief correction answering scheme that successively corrects the prediction score on each candidate answer. Experiments on publicly available benchmark datasets, MovieQA and TVQA, demonstrate that each feature contributes to our movie story QA architecture, PAMN, and improves performance to achieve the state-of-the-art result. Qualitative analysis by visualizing the inference mechanism of PAMN is also provided.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\I5HH96V4\\Kim 等。 - 2019 - Progressive Attention Memory Network for Movie Sto.pdf}
}

@unpublished{kimSingleModalEntropyBased2021,
  title = {Single-{{Modal Entropy}} Based {{Active Learning}} for {{Visual Question Answering}}},
  author = {Kim, Dong-Jin and Cho, Jae Won and Choi, Jinsoo and Jung, Yunjae and Kweon, In So},
  date = {2021-11-18},
  eprint = {2110.10906},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.10906},
  urldate = {2021-12-08},
  abstract = {Constructing a large-scale labeled dataset in the real world, especially for high-level tasks (eg, Visual Question Answering), can be expensive and time-consuming. In addition, with the ever-growing amounts of data and architecture complexity, Active Learning has become an important aspect of computer vision research. In this work, we address Active Learning in the multi-modal setting of Visual Question Answering (VQA). In light of the multi-modal inputs, image and question, we propose a novel method for effective sample acquisition through the use of ad hoc single-modal branches for each input to leverage its information. Our mutual information based sample acquisition strategy Single-Modal Entropic Measure (SMEM) in addition to our self-distillation technique enables the sample acquisitor to exploit all present modalities and find the most informative samples. Our novel idea is simple to implement, cost-efficient, and readily adaptable to other multi-modal tasks. We confirm our findings on various VQA datasets through state-of-the-art performance by comparing to existing Active Learning baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\MDTCMY4T\\Kim et al_2021_Single-Modal Entropy based Active Learning for Visual Question Answering.pdf;D\:\\Zotero\\storage\\UBJHXATS\\2110.html}
}

@unpublished{kocamanImprovingClinicalDocument2020,
  title = {Improving {{Clinical Document Understanding}} on {{COVID-19 Research}} with {{Spark NLP}}},
  author = {Kocaman, Veysel and Talby, David},
  date = {2020-12-07},
  eprint = {2012.04005},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.04005},
  urldate = {2021-12-25},
  abstract = {Following the global COVID-19 pandemic, the number of scientific papers studying the virus has grown massively, leading to increased interest in automated literate review. We present a clinical text mining system that improves on previous efforts in three ways. First, it can recognize over 100 different entity types including social determinants of health, anatomy, risk factors, and adverse events in addition to other commonly used clinical and biomedical entities. Second, the text processing pipeline includes assertion status detection, to distinguish between clinical facts that are present, absent, conditional, or about someone other than the patient. Third, the deep learning models used are more accurate than previously available, leveraging an integrated pipeline of state-of-the-art pretrained named entity recognition models, and improving on the previous best performing benchmarks for assertion status detection. We illustrate extracting trends and insights, e.g. most frequent disorders and symptoms, and most common vital signs and EKG findings, from the COVID-19 Open Research Dataset (CORD-19). The system is built using the Spark NLP library which natively supports scaling to use distributed clusters, leveraging GPUs, configurable and reusable NLP pipelines, healthcare specific embeddings, and the ability to train models to support new entity types or human languages with no code changes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\U34JRC6S\\Kocaman_Talby_2020_Improving Clinical Document Understanding on COVID-19 Research with Spark NLP.pdf;D\:\\Zotero\\storage\\RXSCH6N7\\2012.html}
}

@unpublished{konerGraphhopperMultiHopScene2021,
  title = {Graphhopper: {{Multi-Hop Scene Graph Reasoning}} for {{Visual Question Answering}}},
  shorttitle = {Graphhopper},
  author = {Koner, Rajat and Li, Hang and Hildebrandt, Marcel and Das, Deepan and Tresp, Volker and Günnemann, Stephan},
  date = {2021-07-13},
  eprint = {2107.06325},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.06325},
  urldate = {2021-09-23},
  abstract = {Visual Question Answering (VQA) is concerned with answering free-form questions about an image. Since it requires a deep semantic and linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires multi-modal reasoning from both computer vision and natural language processing. We propose Graphhopper, a novel method that approaches the task by integrating knowledge graph reasoning, computer vision, and natural language processing techniques. Concretely, our method is based on performing context-driven, sequential reasoning based on the scene entities and their semantic and spatial relationships. As a first step, we derive a scene graph that describes the objects in the image, as well as their attributes and their mutual relationships. Subsequently, a reinforcement learning agent is trained to autonomously navigate in a multi-hop manner over the extracted scene graph to generate reasoning paths, which are the basis for deriving answers. We conduct an experimental study on the challenging dataset GQA, based on both manually curated and automatically generated scene graphs. Our results show that we keep up with a human performance on manually curated scene graphs. Moreover, we find that Graphhopper outperforms another state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\V627AXEV\\Koner et al_2021_Graphhopper.pdf;D\:\\Zotero\\storage\\W3TKCZ5Y\\2107.html}
}

@unpublished{kosiorekStackedCapsuleAutoencoders2019,
  title = {Stacked {{Capsule Autoencoders}}},
  author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
  date = {2019-12-02},
  eprint = {1906.06818},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.06818},
  urldate = {2021-10-17},
  abstract = {Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55\%) and MNIST (98.7\%).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\A9QF3W75\\Kosiorek 等。 - 2019 - Stacked Capsule Autoencoders.pdf}
}

@unpublished{kreuzerRethinkingGraphTransformers2021,
  title = {Rethinking {{Graph Transformers}} with {{Spectral Attention}}},
  author = {Kreuzer, Devin and Beaini, Dominique and Hamilton, William L. and Létourneau, Vincent and Tossou, Prudencio},
  date = {2021-10-27},
  eprint = {2106.03893},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.03893},
  urldate = {2022-03-26},
  abstract = {In recent years, the Transformer architecture has proven to be very successful in sequence processing, but its application to other data structures, such as graphs, has remained limited due to the difficulty of properly defining positions. Here, we present the \$\textbackslash textit\{Spectral Attention Network\}\$ (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a wide margin, becoming the first fully-connected architecture to perform well on graph benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\342CI8D5\\Kreuzer et al_2021_Rethinking Graph Transformers with Spectral Attention.pdf;D\:\\Zotero\\storage\\VV3GQZSK\\2106.html}
}

@unpublished{krishnaInformationMaximizingVisual2019,
  title = {Information {{Maximizing Visual Question Generation}}},
  author = {Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
  date = {2019-03-26},
  eprint = {1903.11207},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1903.11207},
  urldate = {2021-09-09},
  abstract = {Though image-to-sequence generation models have become overwhelmingly popular in human-computer communications, they suffer from strongly favoring safe generic questions ("What is in this picture?"). Generating uninformative but relevant questions is not sufficient or useful. We argue that a good question is one that has a tightly focused purpose --- one that is aimed at expecting a specific type of response. We build a model that maximizes mutual information between the image, the expected answer and the generated question. To overcome the non-differentiability of discrete natural language tokens, we introduce a variational continuous latent space onto which the expected answers project. We regularize this latent space with a second latent space that ensures clustering of similar answers. Even when we don't know the expected answer, this second latent space can generate goal-driven questions specifically aimed at extracting objects ("what is the person throwing"), attributes, ("What kind of shirt is the person wearing?"), color ("what color is the frisbee?"), material ("What material is the frisbee?"), etc. We quantitatively show that our model is able to retain information about an expected answer category, resulting in more diverse, goal-driven questions. We launch our model on a set of real world images and extract previously unseen visual concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\J8DT52TI\\Krishna et al_2019_Information Maximizing Visual Question Generation.pdf;D\:\\Zotero\\storage\\MBPSPHIB\\1903.html}
}

@article{krishnaSemiSupervisedLearningVisionandLanguage,
  title = {Semi-{{Supervised Learning}} for {{Vision-and-Language Tasks}} Using {{MixMatch}}},
  author = {Krishna, Kalpesh and Suman, Videsh},
  pages = {8},
  abstract = {Semi-supervised learning algorithms attempt to train a model with limited labeled data by leveraging a large amount of unlabelled samples . However, there is limited literature on using such a strategy for visual-language tasks, mainly due to the complexity and the discreteness of the input space. We attempt to reformulate MixMatch, a recently proposed semi-supervised learning strategy, on a state-of-the-art multi-modal framework LXMERT, and report the performance on the NLVR2 dataset. We compare these results with suitable baseline experiments. To assess the applicability of textual interpolation, we conduct an interesting experiment on GPT-2. Towards the end, we propose two more modifications that were planned but couldn’t be executed due to the constraint of time.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\XENSTJBR\\Krishna 和 Suman - Semi-Supervised Learning for Vision-and-Language T.pdf}
}

@article{krishnaVisualGenomeConnecting2017,
  title = {Visual {{Genome}}: {{Connecting Language}} and {{Vision Using Crowdsourced Dense Image Annotations}}},
  shorttitle = {Visual {{Genome}}},
  author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
  date = {2017-05},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {123},
  number = {1},
  pages = {32--73},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-016-0981-7},
  url = {http://link.springer.com/10.1007/s11263-016-0981-7},
  urldate = {2021-09-11},
  langid = {english},
  file = {D\:\\Zotero\\storage\\AF42VW95\\Krishna 等。 - 2017 - Visual Genome Connecting Language and Vision Usin.pdf}
}

@article{kurzhalsVisualMovieAnalytics2016,
  title = {Visual {{Movie Analytics}}},
  author = {Kurzhals, Kuno and John, Markus and Heimerl, Florian and Kuznecov, Paul and Weiskopf, Daniel},
  date = {2016-11},
  journaltitle = {IEEE Transactions on Multimedia},
  shortjournal = {IEEE Trans. Multimedia},
  volume = {18},
  number = {11},
  pages = {2149--2160},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2016.2614184},
  url = {http://ieeexplore.ieee.org/document/7577806/},
  urldate = {2022-05-25},
  abstract = {The analysis of inherent structures of movies plays an important role in studying stylistic devices and specific, content-related questions. Examples are the analysis of personal constellations in movie scenes, dialogue-based content analysis, or the investigation of image-based features. We provide a visual analytics approach that supports the analytical reasoning process to derive higher level insights about the content on a semantic level. Combining automatic methods for semantic scene analysis based on script and subtitle text, we perform a low-level analysis of the data automatically. Our approach features an interactive visualization that allows a multilayer interpretation of descriptive features to characterize movie content. For semantic analysis, we extract scene information from movie scripts and match them with the corresponding subtitles. With text- and image-based query techniques, we facilitate an interactive comparison of different movie scenes on an image and on a semantic level. We demonstrate how our approach can be applied for content analysis on a popular Hollywood movie.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\6Z2GRAP8\\Kurzhals 等。 - 2016 - Visual Movie Analytics.pdf}
}

@incollection{kvReducingLanguageBiases2020,
  title = {Reducing {{Language Biases}} in {{Visual Question Answering}} with {{Visually-Grounded Question Encoder}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Kv, Gouthaman and Mittal, Anurag},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12358},
  pages = {18--34},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58601-0_2},
  url = {https://link.springer.com/10.1007/978-3-030-58601-0_2},
  urldate = {2021-09-10},
  abstract = {Recent studies have shown that current VQA models are heavily biased on the language priors in the train set to answer the question, irrespective of the image. E.g., overwhelmingly answer “what sport is” as “tennis” or “what color banana” as “yellow.” This behavior restricts them from real-world application scenarios. In this work, we propose a novel model-agnostic question encoder, Visually-Grounded Question Encoder (VGQE), for VQA that reduces this effect. VGQE utilizes both visual and language modalities equally while encoding the question. Hence the question representation itself gets sufficient visual-grounding, and thus reduces the dependency of the model on the language priors. We demonstrate the effect of VGQE on three recent VQA models and achieve state-of-the-art results on the bias-sensitive split of the VQAv2 dataset; VQA-CPv2. Further, unlike the existing bias-reduction techniques, on the standard VQAv2 benchmark, our approach does not drop the accuracy; instead, it improves the performance.},
  isbn = {978-3-030-58600-3 978-3-030-58601-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZKKJG7VL\\Kv 和 Mittal - 2020 - Reducing Language Biases in Visual Question Answer.pdf}
}

@article{laoMultistageHybridEmbedding2021,
  title = {Multi-Stage Hybrid Embedding Fusion Network for Visual Question Answering},
  author = {Lao, Mingrui and Guo, Yanming and Pu, Nan and Chen, Wei and Liu, Yu and Lew, Michael S.},
  date = {2021-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {423},
  pages = {541--550},
  issn = {09252312},
  doi = {10.1016/j.neucom.2020.10.071},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220316593},
  urldate = {2022-02-07},
  abstract = {Multimodal fusion is a crucial component of Visual Question Answering (VQA), which involves joint understanding and semantic integration between visual and textual information. Existing VQA learning frameworks focus mainly on Latent Embedding Fusion (LEF) method, by projecting visual and textual features into a common latent space, and fusing them with element-wise multiplication. In this paper, we intend to achieve multiple and fine-grained multimodal interactions for enhancing fusion performance. To this end, we propose a Multi-stage Hybrid Embedding Fusion (MHEF) network to fulfill our improvements in two folds: First, we introduce a Dual Embedding Fusion (DEF) approach that transforms one modal input into the reciprocal embedding space before integration, and the DEF is further incorporated with the LEF to form a novel Hybrid Embedding Fusion (HEF). Second, we design a Multi-stage Fusion Structure (MFS) for the HEF to form the MHEF network, so as to obtain diverse and better fusion features for answer prediction. By jointly training the multi-stage framework, we can not only improve the performance in each single stage, but also obtain additional accuracy improvements by integrating all prediction results from each stage. Extensive experiments verify both our proposed HEF and MFS are beneficial to multi-modal fusion. The full MHEF model outperforms the baseline LEF model with 2\% accuracy improvements, and achieves promising performance on the VQA-v1 and VQA-v2 datasets.},
  langid = {english}
}

@unpublished{leeLargeScaleAnswererQuestioner2019,
  title = {Large-{{Scale Answerer}} in {{Questioner}}'s {{Mind}} for {{Visual Dialog Question Generation}}},
  author = {Lee, Sang-Woo and Gao, Tong and Yang, Sohee and Yoo, Jaejun and Ha, Jung-Woo},
  date = {2019-02-21},
  eprint = {1902.08355},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.08355},
  urldate = {2021-09-10},
  abstract = {Answerer in Questioner's Mind (AQM) is an information-theoretic framework that has been recently proposed for task-oriented dialog systems. AQM benefits from asking a question that would maximize the information gain when it is asked. However, due to its intrinsic nature of explicitly calculating the information gain, AQM has a limitation when the solution space is very large. To address this, we propose AQM+ that can deal with a large-scale problem and ask a question that is more coherent to the current context of the dialog. We evaluate our method on GuessWhich, a challenging task-oriented visual dialog problem, where the number of candidate classes is near 10K. Our experimental results and ablation studies show that AQM+ outperforms the state-of-the-art models by a remarkable margin with a reasonable approximation. In particular, the proposed AQM+ reduces more than 60\% of error as the dialog proceeds, while the comparative algorithms diminish the error by less than 6\%. Based on our results, we argue that AQM+ is a general task-oriented dialog algorithm that can be applied for non-yes-or-no responses.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,VISUAL DIALOG},
  file = {D\:\\Zotero\\storage\\CRPUPNI2\\Lee et al_2019_Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation.pdf;D\:\\Zotero\\storage\\H3IZS77Y\\1902.html}
}

@misc{leePix2StructScreenshotParsing2022,
  title = {{{Pix2Struct}}: {{Screenshot Parsing}} as {{Pretraining}} for {{Visual Language Understanding}}},
  shorttitle = {{{Pix2Struct}}},
  author = {Lee, Kenton and Joshi, Mandar and Turc, Iulia and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
  date = {2022-10-07},
  number = {arXiv:2210.03347},
  eprint = {2210.03347},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.03347},
  urldate = {2022-10-11},
  abstract = {Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\JHFF7ZXC\\Lee et al_2022_Pix2Struct.pdf;D\:\\Zotero\\storage\\MG8DGCYT\\2210.html}
}

@article{leeRegularizingAttentionNetworks,
  title = {Regularizing {{Attention Networks}} for {{Anomaly Detection}} in {{Visual Question Answering}}},
  author = {Lee, Doyup and Cheon, Yeongjae and Han, Wook-Shin},
  pages = {16},
  abstract = {For stability and reliability of real-world applications, the robustness of DNNs in unimodal tasks has been evaluated. However, few studies consider abnormal situations that a visual question answering (VQA) model might encounter at test time after deployment in the real-world. In this study, we evaluate the robustness of state-of-the-art VQA models to five different anomalies, including worst-case scenarios, the most frequent scenarios, and the current limitation of VQA models. Different from the results in unimodal tasks, the maximum confidence of answers in VQA models cannot detect anomalous inputs, and post-training of the outputs, such as outlier exposure, is ineffective for VQA models. Thus, we propose an attention-based method, which uses confidence of reasoning between input images and questions and shows much more promising results than the previous methods in unimodal tasks. In addition, we show that a maximum entropy regularization of attention networks can significantly improve the attention-based anomaly detection of the VQA models. Thanks to the simplicity, attention-based anomaly detection and the regularization are model-agnostic methods, which can be used for various cross-modal attentions in the state-of-the-art VQA models. The results imply that crossmodal attention in VQA is important to improve not only VQA accuracy, but also the robustness to various anomalies.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\KHMLXYM9\\Lee 等。 - Regularizing Attention Networks for Anomaly Detect.pdf}
}

@inproceedings{leHierarchicalConditionalRelation2020,
  title = {Hierarchical {{Conditional Relation Networks}} for {{Video Question Answering}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
  date = {2020-06},
  pages = {9969--9978},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00999},
  url = {https://ieeexplore.ieee.org/document/9156815/},
  urldate = {2021-03-11},
  abstract = {Video question answering (VideoQA) is challenging as it requires modeling capacity to distill dynamic visual artifacts and distant relations and to associate them with linguistic concepts. We introduce a general-purpose reusable neural unit called Conditional Relation Network (CRN) that serves as a building block to construct more sophisticated structures for representation and reasoning over video. CRN takes as input an array of tensorial objects and a conditioning feature, and computes an array of encoded output objects. Model building becomes a simple exercise of replication, rearrangement and stacking of these reusable units for diverse modalities and contextual information. This design thus supports high-order relational and multi-step reasoning. The resulting architecture for VideoQA is a CRN hierarchy whose branches represent sub-videos or clips, all sharing the same question as the contextual condition. Our evaluations on well-known datasets achieved new SoTA results, demonstrating the impact of building a general-purpose reasoning unit on complex domains such as VideoQA.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4P9IW99E\\Le 等。 - 2020 - Hierarchical Conditional Relation Networks for Vid.pdf}
}

@article{leiLessMoreClipBERT,
  title = {Less {{Is More}}: {{ClipBERT}} for {{Video-and-Language Learning}} via {{Sparse Sampling}}},
  author = {Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L and Bansal, Mohit and Liu, Jingjing},
  pages = {11},
  langid = {english},
  keywords = {video},
  file = {D\:\\Zotero\\storage\\E8DS6XWH\\Lei 等。 - Less Is More ClipBERT for Video-and-Language Lear.pdf}
}

@article{leiLessMoreClipBERTa,
  title = {Less {{Is More}}: {{ClipBERT}} for {{Video-and-Language Learning}} via {{Sparse Sampling}}},
  author = {Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L and Bansal, Mohit and Liu, Jingjing},
  pages = {11},
  langid = {english},
  file = {D\:\\Zotero\\storage\\NTY2F8V8\\Lei 等。 - Less Is More ClipBERT for Video-and-Language Lear.pdf}
}

@article{leiMultiQuestionLearningVisual,
  title = {Multi-{{Question Learning}} for {{Visual Question Answering}}},
  author = {Lei, Chenyi and Wu, Lei and Liu, Dong and Li, Zhao and Wang, Guoxin and Tang, Haihong and Li, Houqiang},
  pages = {8},
  abstract = {Visual Question Answering (VQA) raises a great challenge for computer vision and natural language processing communities. Most of the existing approaches consider videoquestion pairs individually during training. However, we observe that there are usually multiple (either sequentially generated or not) questions for the target video in a VQA task, and the questions themselves have abundant semantic relations. To explore these relations, we propose a new paradigm for VQA termed Multi-Question Learning (MQL). Inspired by the multi-task learning, MQL learns from multiple questions jointly together with their corresponding answers for a target video sequence. The learned representations of videoquestion pairs are then more general to be transferred for new questions. We further propose an effective VQA framework and design a training procedure for MQL, where the specifically designed attention network models the relation between input video and corresponding questions, enabling multiple video-question pairs to be co-trained. Experimental results on public datasets show the favorable performance of the proposed MQL-VQA framework compared to state-of-the-arts.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\RY5YW2Y2\\Lei 等。 - Multi-Question Learning for Visual Question Answer.pdf}
}

@unpublished{leiTVQASpatioTemporalGrounding2020,
  title = {{{TVQA}}+: {{Spatio-Temporal Grounding}} for {{Video Question Answering}}},
  shorttitle = {{{TVQA}}+},
  author = {Lei, Jie and Yu, Licheng and Berg, Tamara L. and Bansal, Mohit},
  date = {2020-05-11},
  eprint = {1904.11574},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.11574},
  urldate = {2021-09-09},
  abstract = {We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations. Dataset and code are publicly available at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\BQ4MIVYA\\Lei et al_2020_TVQA+.pdf;D\:\\Zotero\\storage\\8BD3JZP3\\1904.html}
}

@article{leMultiVisualTextual2021,
  title = {Multi Visual and Textual Embedding on Visual Question Answering for Blind People},
  author = {Le, Tung and Nguyen, Huy Tien and Nguyen, Minh Le},
  date = {2021-11},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {465},
  pages = {451--464},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.08.117},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S092523122101328X},
  urldate = {2022-02-07},
  abstract = {Visual impairment community, especially blind people have a thirst for assistance from advanced technologies for understanding and answering the image. Through the development and intersection between vision and language, Visual Question Answering (VQA) is to predict an answer from a textual question on an image. It is essential and ideal to help blind people with capturing the image and answering their questions automatically. Traditional approaches often utilize the strength of convolution and recurrent networks, which requires a great effort for learning and optimizing. A key challenge in VQA is finding an effective way to extract and combine textual and visual features. To take advantage of previous knowledge in different domains, we propose BERT-RG, the delicate integration of pre-trained models into feature extractors, which relies on the interaction between residual and global features in the image and linguistic features in the question. Moreover, our architecture integrates a stacked attention mechanism that exploits the relationship between textual and visual objects. Specifically, the partial regions of images interact with partial keywords in question to enhance the text-vision representation. Besides, we also propose a novel perspective by considering a specific question type in VQA. Our proposal is significantly meaningful enough to develop a specialized system instead of putting forth the effort to dig for unlimited and unrealistic approaches. Experiments on VizWiz-VQA, a practical benchmark dataset, show that our proposed model outperforms existing models on the VizWiz VQA dataset in the Yes/No question type.},
  langid = {english}
}

@article{leMultiVisualTextual2021a,
  title = {Multi Visual and Textual Embedding on Visual Question Answering for Blind People},
  author = {Le, Tung and Nguyen, Huy Tien and Nguyen, Minh Le},
  date = {2021-11},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {465},
  pages = {451--464},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.08.117},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S092523122101328X},
  urldate = {2021-12-24},
  abstract = {Visual impairment community, especially blind people have a thirst for assistance from advanced technologies for understanding and answering the image. Through the development and intersection between vision and language, Visual Question Answering (VQA) is to predict an answer from a textual question on an image. It is essential and ideal to help blind people with capturing the image and answering their questions automatically. Traditional approaches often utilize the strength of convolution and recurrent networks, which requires a great effort for learning and optimizing. A key challenge in VQA is finding an effective way to extract and combine textual and visual features. To take advantage of previous knowledge in different domains, we propose BERT-RG, the delicate integration of pre-trained models into feature extractors, which relies on the interaction between residual and global features in the image and linguistic features in the question. Moreover, our architecture integrates a stacked attention mechanism that exploits the relationship between textual and visual objects. Specifically, the partial regions of images interact with partial keywords in question to enhance the text-vision representation. Besides, we also propose a novel perspective by considering a specific question type in VQA. Our proposal is significantly meaningful enough to develop a specialized system instead of putting forth the effort to dig for unlimited and unrealistic approaches. Experiments on VizWiz-VQA, a practical benchmark dataset, show that our proposed model outperforms existing models on the VizWiz VQA dataset in the Yes/No question type.},
  langid = {english}
}

@unpublished{leNeuralReasoningFast2020,
  title = {Neural {{Reasoning}}, {{Fast}} and {{Slow}}, for {{Video Question Answering}}},
  author = {Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
  date = {2020-04-10},
  eprint = {1907.04553},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.04553},
  urldate = {2022-04-21},
  abstract = {What does it take to design a machine that learns to answer natural questions about a video? A Video QA system must simultaneously understand language, represent visual content over space-time, and iteratively transform these representations in response to lingual content in the query, and finally arriving at a sensible answer. While recent advances in lingual and visual question answering have enabled sophisticated representations and neural reasoning mechanisms, major challenges in Video QA remain on dynamic grounding of concepts, relations and actions to support the reasoning process. Inspired by the dual-process account of human reasoning, we design a dual process neural architecture, which is composed of a question-guided video processing module (System 1, fast and reactive) followed by a generic reasoning module (System 2, slow and deliberative). System 1 is a hierarchical model that encodes visual patterns about objects, actions and relations in space-time given the textual cues from the question. The encoded representation is a set of high-level visual features, which are then passed to System 2. Here multi-step inference follows to iteratively chain visual elements as instructed by the textual elements. The system is evaluated on the SVQA (synthetic) and TGIF-QA datasets (real), demonstrating competitive results, with a large margin in the case of multi-step reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\VMHIJQUV\\Le et al_2020_Neural Reasoning, Fast and Slow, for Video Question Answering.pdf;D\:\\Zotero\\storage\\PJPP7GZK\\1907.html}
}

@misc{li3DSpatialReasoning2022,
  title = {Toward {{3D Spatial Reasoning}} for {{Human-like Text-based Visual Question Answering}}},
  author = {Li, Hao and Huang, Jinfa and Jin, Peng and Song, Guoli and Wu, Qi and Chen, Jie},
  date = {2022-09-21},
  number = {arXiv:2209.10326},
  eprint = {2209.10326},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.10326},
  urldate = {2022-10-13},
  abstract = {Text-based Visual Question Answering\textasciitilde (TextVQA) aims to produce correct answers for given questions about the images with multiple scene texts. In most cases, the texts naturally attach to the surface of the objects. Therefore, spatial reasoning between texts and objects is crucial in TextVQA. However, existing approaches are constrained within 2D spatial information learned from the input images and rely on transformer-based architectures to reason implicitly during the fusion process. Under this setting, these 2D spatial reasoning approaches cannot distinguish the fine-grain spatial relations between visual objects and scene texts on the same image plane, thereby impairing the interpretability and performance of TextVQA models. In this paper, we introduce 3D geometric information into a human-like spatial reasoning process to capture the contextual knowledge of key objects step-by-step. \%we formulate a human-like spatial reasoning process by introducing 3D geometric information for capturing key objects' contextual knowledge. To enhance the model's understanding of 3D spatial relationships, Specifically, (i)\textasciitilde we propose a relation prediction module for accurately locating the region of interest of critical objects; (ii)\textasciitilde we design a depth-aware attention calibration module for calibrating the OCR tokens' attention according to critical objects. Extensive experiments show that our method achieves state-of-the-art performance on TextVQA and ST-VQA datasets. More encouragingly, our model surpasses others by clear margins of 5.7\textbackslash\% and 12.1\textbackslash\% on questions that involve spatial reasoning in TextVQA and ST-VQA valid split. Besides, we also verify the generalizability of our model on the text-based image captioning task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\I4BQRHVG\\Li et al_2022_Toward 3D Spatial Reasoning for Human-like Text-based Visual Question Answering.pdf;D\:\\Zotero\\storage\\DWFSWCP8\\2209.html}
}

@unpublished{liAdversarialVQANew2021,
  title = {Adversarial {{VQA}}: {{A New Benchmark}} for {{Evaluating}} the {{Robustness}} of {{VQA Models}}},
  shorttitle = {Adversarial {{VQA}}},
  author = {Li, Linjie and Lei, Jie and Gan, Zhe and Liu, Jingjing},
  date = {2021-08-13},
  eprint = {2106.00245},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.00245},
  urldate = {2022-05-09},
  abstract = {Benefiting from large-scale pre-training, we have witnessed significant performance boost on the popular Visual Question Answering (VQA) task. Despite rapid progress, it remains unclear whether these state-of-the-art (SOTA) models are robust when encountering examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we discover several interesting findings. (i) Surprisingly, we find that during dataset collection, non-expert annotators can easily attack SOTA VQA models successfully. (ii) Both large-scale pre-trained models and adversarial training methods achieve far worse performance on the new benchmark than over standard VQA v2 dataset, revealing the fragility of these models while demonstrating the effectiveness of our adversarial dataset. (iii) When used for data augmentation, our dataset can effectively boost model performance on other robust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light on robustness study in the community and serve as a valuable benchmark for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\L2NH9ECE\\Li et al_2021_Adversarial VQA.pdf;D\:\\Zotero\\storage\\8JNABUVU\\2106.html}
}

@unpublished{liAlignFuseVision2021,
  title = {Align before {{Fuse}}: {{Vision}} and {{Language Representation Learning}} with {{Momentum Distillation}}},
  shorttitle = {Align before {{Fuse}}},
  author = {Li, Junnan and Selvaraju, Ramprasaath R. and Gotmare, Akhilesh Deepak and Joty, Shafiq and Xiong, Caiming and Hoi, Steven},
  date = {2021-10-07},
  eprint = {2107.07651},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.07651},
  urldate = {2022-03-09},
  abstract = {Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR\$\^2\$, ALBEF achieves absolute improvements of 2.37\% and 3.84\% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at https://github.com/salesforce/ALBEF/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\692R4XJN\\Li et al_2021_Align before Fuse.pdf;D\:\\Zotero\\storage\\HL8IIQG9\\2107.html}
}

@inproceedings{liangFocalVisualTextAttention2018,
  title = {Focal {{Visual-Text Attention}} for {{Visual Question Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liang, Junwei and Jiang, Lu and Cao, Liangliang and Li, Li-Jia and Hauptmann, Alexander},
  date = {2018-06},
  pages = {6135--6143},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00642},
  url = {https://ieeexplore.ieee.org/document/8578740/},
  urldate = {2021-09-09},
  abstract = {Recent insights on language and vision with neural networks have been successfully applied to simple singleimage visual question answering. However, to tackle reallife question answering problems on multimedia collections such as personal photos, we have to look at whole collections with sequences of photos or videos. When answering questions from a large collection, a natural problem is to identify snippets to support the answer. In this paper, we describe a novel neural network called Focal Visual-Text Attention network (FVTA) for collective reasoning in visual question answering, where both visual and text sequence information such as images and text metadata are presented. FVTA introduces an end-to-end approach that makes use of a hierarchical process to dynamically determine what media and what time to focus on in the sequential data to answer the question. FVTA can not only answer the questions well but also provides the justifications which the system results are based upon to get the answers. FVTA achieves state-of-the-art performance on the MemexQA dataset and competitive results on the MovieQA dataset.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {41 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\GXUSJ3ZU\\Liang 等。 - 2018 - Focal Visual-Text Attention for Visual Question An.pdf}
}

@article{liangFocalVisualTextAttention2019,
  title = {Focal {{Visual-Text Attention}} for {{Memex Question Answering}}},
  author = {Liang, Junwei and Jiang, Lu and Cao, Liangliang and Kalantidis, Yannis and Li, Li-Jia and Hauptmann, Alexander G.},
  date = {2019-08},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {41},
  number = {8},
  pages = {1893--1908},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2890628},
  abstract = {Recent insights on language and vision with neural networks have been successfully applied to simple single-image visual question answering. However, to tackle real-life question answering problems on multimedia collections such as personal photo albums, we have to look at whole collections with sequences of photos. This paper proposes a new multimodal MemexQA task: given a sequence of photos from a user, the goal is to automatically answer questions that help users recover their memory about an event captured in these photos. In addition to a text answer, a few grounding photos are also given to justify the answer. The grounding photos are necessary as they help users quickly verifying the answer. Towards solving the task, we 1) present the MemexQA dataset, the first publicly available multimodal question answering dataset consisting of real personal photo albums; 2) propose an end-to-end trainable network that makes use of a hierarchical process to dynamically determine what media and what time to focus on in the sequential data to answer the question. Experimental results on the MemexQA dataset demonstrate that our model outperforms strong baselines and yields the most relevant grounding photos on this challenging task.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Cognition,focal attention,Grounding,Knowledge discovery,memex,Metadata,Photo albums,question answering,Task analysis,vision and language,Visualization},
  file = {D\:\\Zotero\\storage\\ZL39K2Q7\\Liang et al_2019_Focal Visual-Text Attention for Memex Question Answering.pdf;D\:\\Zotero\\storage\\IFH8CVVG\\8603827.html}
}

@inproceedings{liangLearningContrastCounterfactual2020,
  title = {Learning to {{Contrast}} the {{Counterfactual Samples}} for {{Robust Visual Question Answering}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Liang, Zujie and Jiang, Weitao and Hu, Haifeng and Zhu, Jiaying},
  date = {2020-11},
  pages = {3285--3292},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.265},
  url = {https://aclanthology.org/2020.emnlp-main.265},
  urldate = {2021-09-10},
  abstract = {In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model's robustness.},
  eventtitle = {{{EMNLP}} 2020},
  keywords = {Robust},
  file = {D\:\\Zotero\\storage\\S6HUG6AX\\Liang et al_2020_Learning to Contrast the Counterfactual Samples for Robust Visual Question.pdf}
}

@inproceedings{liangLPFLanguagePriorFeedback2021,
  title = {{{LPF}}: {{A Language-Prior Feedback Objective Function}} for {{De-biased Visual Question Answering}}},
  shorttitle = {{{LPF}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Liang, Zujie and Hu, Haifeng and Zhu, Jiaying},
  date = {2021-07-11},
  pages = {1955--1959},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3462981},
  url = {https://dl.acm.org/doi/10.1145/3404835.3462981},
  urldate = {2021-09-10},
  abstract = {Most existing Visual Question Answering (VQA) systems tend to overly rely on the language bias and hence fail to reason from the visual clue. To address this issue, we propose a novel Language-Prior Feedback (LPF) objective function, to re-balance the proportion of each answer’s loss value in the total VQA loss. The LPF firstly calculates a modulating factor to determine the language bias using a question-only branch. Then, the LPF assigns a self-adaptive weight to each training sample in the training process. With this reweighting mechanism, the LPF ensures that the total VQA loss can be reshaped to a more balanced form. By this means, the samples that require certain visual information to predict will be efficiently used during training. Our method is simple to implement, model-agnostic, and end-to-end trainable. We conduct extensive experiments and the results show that the LPF (1) brings a significant improvement over various VQA models, (2) achieves competitive performance on the bias-sensitive VQA-CP v2 benchmark.},
  eventtitle = {{{SIGIR}} '21: {{The}} 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\QAU4CIZW\\Liang 等。 - 2021 - LPF A Language-Prior Feedback Objective Function .pdf}
}

@misc{liangRDropRegularizedDropout2021,
  title = {R-{{Drop}}: {{Regularized Dropout}} for {{Neural Networks}}},
  shorttitle = {R-{{Drop}}},
  author = {Liang, Xiaobo and Wu, Lijun and Li, Juntao and Wang, Yue and Meng, Qi and Qin, Tao and Chen, Wei and Zhang, Min and Liu, Tie-Yan},
  date = {2021-10-29},
  number = {arXiv:2106.14448},
  eprint = {2106.14448},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.14448},
  urldate = {2022-08-13},
  abstract = {Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on \$\textbackslash bf\{5\}\$ widely used deep learning tasks (\$\textbackslash bf\{18\}\$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English\$\textbackslash to\$German translation (\$\textbackslash bf\{30.91\}\$ BLEU) and WMT14 English\$\textbackslash to\$French translation (\$\textbackslash bf\{43.95\}\$ BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub\{\textbackslash url\{https://github.com/dropreg/R-Drop\}\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\LRKX7EEZ\\Liang et al_2021_R-Drop.pdf;D\:\\Zotero\\storage\\WM8K3TXS\\2106.html}
}

@unpublished{liangVisualSemanticGraphAttention2021,
  title = {Visual-{{Semantic Graph Attention Networks}} for {{Human-Object Interaction Detection}}},
  author = {Liang, Zhijun and Rojas, Juan and Liu, Junfa and Guan, Yisheng},
  date = {2021-03-06},
  eprint = {2001.02302},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2001.02302},
  urldate = {2022-03-09},
  abstract = {In scene understanding, robotics benefit from not only detecting individual scene instances but also from learning their possible interactions. Human-Object Interaction (HOI) Detection infers the action predicate on a {$<$}human, predicate, object{$>$} triplet. Contextual information has been found critical in inferring interactions. However, most works only use local features from single human-object pair for inference. Few works have studied the disambiguating contribution of subsidiary relations made available via graph networks. Similarly, few have learned to effectively leverage visual cues along with the intrinsic semantic regularities contained in HOIs. We contribute a dual-graph attention network that effectively aggregates contextual visual, spatial, and semantic information dynamically from primary human-object relations as well as subsidiary relations through attention mechanisms for strong disambiguating power. We achieve comparable results on two benchmarks: V-COCO and HICO-DET. Code is available at \textbackslash url\{https://github.com/birlrobotics/vs-gats\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {D\:\\Zotero\\storage\\NZYBH5EA\\Liang et al_2021_Visual-Semantic Graph Attention Networks for Human-Object Interaction Detection.pdf;D\:\\Zotero\\storage\\YGLLBH28\\2001.html}
}

@unpublished{liBipartiteGraphNetwork2021,
  title = {Bipartite {{Graph Network}} with {{Adaptive Message Passing}} for {{Unbiased Scene Graph Generation}}},
  author = {Li, Rongjie and Zhang, Songyang and Wan, Bo and He, Xuming},
  date = {2021-04-28},
  eprint = {2104.00308},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.00308},
  urldate = {2022-03-25},
  abstract = {Scene graph generation is an important visual understanding task with a broad range of vision applications. Despite recent tremendous progress, it remains challenging due to the intrinsic long-tailed class distribution and large intra-class variation. To address these issues, we introduce a novel confidence-aware bipartite graph neural network with adaptive message propagation mechanism for unbiased scene graph generation. In addition, we propose an efficient bi-level data resampling strategy to alleviate the imbalanced data distribution problem in training our graph network. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including Visual Genome, Open Images V4/V6, demonstrating its effectiveness and generality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\A5W89IW4\\Li et al_2021_Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph.pdf;D\:\\Zotero\\storage\\PZWCS7XP\\2104.html}
}

@unpublished{liBLIPBootstrappingLanguageImage2022,
  title = {{{BLIP}}: {{Bootstrapping Language-Image Pre-training}} for {{Unified Vision-Language Understanding}} and {{Generation}}},
  shorttitle = {{{BLIP}}},
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  date = {2022-02-15},
  eprint = {2201.12086},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.12086},
  urldate = {2022-03-09},
  abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\YDV27HZY\\Li et al_2022_BLIP.pdf;D\:\\Zotero\\storage\\F6DZHKEU\\2201.html}
}

@unpublished{liCapsuleNetworkRecommendation2019,
  title = {A {{Capsule Network}} for {{Recommendation}} and {{Explaining What You Like}} and {{Dislike}}},
  author = {Li, Chenliang and Quan, Cong and Peng, Li and Qi, Yunwei and Deng, Yuming and Wu, Libing},
  date = {2019-07-01},
  eprint = {1907.00687},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.00687},
  urldate = {2021-10-04},
  abstract = {User reviews contain rich semantics towards the preference of users to features of items. Recently, many deep learning based solutions have been proposed by exploiting reviews for recommendation. The attention mechanism is mainly adopted in these works to identify words or aspects that are important for rating prediction. However, it is still hard to understand whether a user likes or dislikes an aspect of an item according to what viewpoint the user holds and to what extent, without examining the review details. Here, we consider a pair of a viewpoint held by a user and an aspect of an item as a logic unit. Reasoning a rating behavior by discovering the informative logic units from the reviews and resolving their corresponding sentiments could enable a better rating prediction with explanation. To this end, in this paper, we propose a capsule network based model for rating prediction with user reviews, named CARP. For each user-item pair, CARP is devised to extract the informative logic units from the reviews and infer their corresponding sentiments. The model firstly extracts the viewpoints and aspects from the user and item review documents respectively. Then we derive the representation of each logic unit based on its constituent viewpoint and aspect. A sentiment capsule architecture with a novel Routing by Bi-Agreement mechanism is proposed to identify the informative logic unit and the sentiment based representations in user-item level for rating prediction. Extensive experiments are conducted over seven real-world datasets with diverse characteristics. Our results demonstrate that the proposed CARP obtains substantial performance gain over recently proposed state-of-the-art models in terms of prediction accuracy. Further analysis shows that our model can successfully discover the interpretable reasons at a finer level of granularity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\HBFEGB49\\Li et al_2019_A Capsule Network for Recommendation and Explaining What You Like and Dislike.pdf;D\:\\Zotero\\storage\\I66E6Y7W\\1907.html}
}

@unpublished{liCompetenceawareCurriculumVisual2020,
  title = {A {{Competence-aware Curriculum}} for {{Visual Concepts Learning}} via {{Question Answering}}},
  author = {Li, Qing and Huang, Siyuan and Hong, Yining and Zhu, Song-Chun},
  date = {2020-07-27},
  eprint = {2007.01499},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2007.01499},
  urldate = {2021-08-11},
  abstract = {Humans can progressively learn visual concepts from easy to hard questions. To mimic this efficient learning ability, we propose a competence-aware curriculum for visual concept learning in a question-answering manner. Specifically, we design a neural-symbolic concept learner for learning the visual concepts and a multi-dimensional Item Response Theory (mIRT) model for guiding the learning process with an adaptive curriculum. The mIRT effectively estimates the concept difficulty and the model competence at each learning step from accumulated model responses. The estimated concept difficulty and model competence are further utilized to select the most profitable training samples. Experimental results on CLEVR show that with a competence-aware curriculum, the proposed method achieves state-of-the-art performances with superior data efficiency and convergence speed. Specifically, the proposed model only uses 40\% of training data and converges three times faster compared with other state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Qing Li},
  file = {D\:\\Zotero\\storage\\AG5WT5BQ\\Li et al_2020_A Competence-aware Curriculum for Visual Concepts Learning via Question.pdf;D\:\\Zotero\\storage\\SAIXQUAJ\\2007.html}
}

@unpublished{liEnhancingDialogueGeneration2021,
  title = {Enhancing {{Dialogue Generation}} via {{Multi-Level Contrastive Learning}}},
  author = {Li, Xin and Li, Piji and Wang, Yan and Liu, Xiaojiang and Lam, Wai},
  date = {2021-06-22},
  eprint = {2009.09147},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2009.09147},
  urldate = {2021-10-12},
  abstract = {Most of the existing works for dialogue generation are data-driven models trained directly on corpora crawled from websites. They mainly focus on improving the model architecture to produce better responses but pay little attention to considering the quality of the training data contrastively. In this paper, we propose a multi-level contrastive learning paradigm to model the fine-grained quality of the responses with respect to the query. A Rank-aware Calibration (RC) network is designed to construct the multi-level contrastive optimization objectives. Since these objectives are calculated based on the sentence level, which may erroneously encourage/suppress the generation of uninformative/informative words. To tackle this incidental issue, on one hand, we design an exquisite token-level strategy for estimating the instance loss more accurately. On the other hand, we build a Knowledge Inference (KI) component to capture the keyword knowledge from the reference during training and exploit such information to encourage the generation of informative words. We evaluate the proposed model on a carefully annotated dialogue dataset and the results suggest that our model can generate more relevant and diverse responses compared to the baseline models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\SW3YXGBN\\Li et al_2021_Enhancing Dialogue Generation via Multi-Level Contrastive Learning.pdf;D\:\\Zotero\\storage\\9N9EJ9JQ\\2009.html}
}

@unpublished{liFactorizableNetEfficient2018,
  title = {Factorizable {{Net}}: {{An Efficient Subgraph-based Framework}} for {{Scene Graph Generation}}},
  shorttitle = {Factorizable {{Net}}},
  author = {Li, Yikang and Ouyang, Wanli and Zhou, Bolei and Shi, Jianping and Zhang, Chao and Wang, Xiaogang},
  date = {2018-08-27},
  eprint = {1806.11538},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1806.11538},
  urldate = {2022-05-05},
  abstract = {Generating scene graph to describe all the relations inside an image gains increasing interests these years. However, most of the previous methods use complicated structures with slow inference speed or rely on the external data, which limits the usage of the model in real-life scenarios. To improve the efficiency of scene graph generation, we propose a subgraph-based connection graph to concisely represent the scene graph during the inference. A bottom-up clustering method is first used to factorize the entire scene graph into subgraphs, where each subgraph contains several objects and a subset of their relationships. By replacing the numerous relationship representations of the scene graph with fewer subgraph and object features, the computation in the intermediate stage is significantly reduced. In addition, spatial information is maintained by the subgraph features, which is leveraged by our proposed Spatial-weighted Message Passing\textasciitilde (SMP) structure and Spatial-sensitive Relation Inference\textasciitilde (SRI) module to facilitate the relationship recognition. On the recent Visual Relationship Detection and Visual Genome datasets, our method outperforms the state-of-the-art method in both accuracy and speed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\PLN4WXQI\\Li et al_2018_Factorizable Net.pdf;D\:\\Zotero\\storage\\RAXTKPLV\\1806.html}
}

@article{liInterpretableGenerativeAdversarial,
  title = {Interpretable {{Generative Adversarial Networks}}},
  author = {Li, Chao and Yao, Kelu and Wang, Jin and Diao, Boyu and Xu, Yongjun and Zhang, Quanshi},
  pages = {9},
  abstract = {Learning a disentangled representation is still a challenge in the field of the interpretability of generative adversarial networks (GANs). This paper proposes a generic method to modify a traditional GAN into an interpretable GAN, which ensures that filters in an intermediate layer of the generator encode disentangled localized visual concepts. Each filter in the layer is supposed to consistently generate image regions corresponding to the same visual concept when generating different images. The interpretable GAN learns to automatically discover meaningful visual concepts without any annotations of visual concepts. The interpretable GAN enables people to modify a specific visual concept on generated images by manipulating feature maps of the corresponding filters in the layer. Our method can be broadly applied to different types of GANs. Experiments have demonstrated the effectiveness of our method. The code will be released when the paper is accepted.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\C5RWP8SX\\Li 等。 - Interpretable Generative Adversarial Networks.pdf}
}

@inproceedings{liInvariantGroundingVideo2022,
  title = {Invariant {{Grounding}} for {{Video Question Answering}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Yicong and Wang, Xiang and Xiao, Junbin and Ji, Wei and Chua, Tat-Seng},
  date = {2022-06},
  pages = {2918--2927},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00294},
  url = {https://ieeexplore.ieee.org/document/9880018/},
  urldate = {2022-10-10},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english},
  file = {D\:\\Zotero\\storage\\W9SQT9L6\\Li 等。 - 2022 - Invariant Grounding for Video Question Answering.pdf}
}

@article{liKnowledgeEnhancedPersonalizedReview2020,
  title = {Knowledge-{{Enhanced Personalized Review Generation}} with {{Capsule Graph Neural Network}}},
  author = {Li, Junyi and Li, Siqing and Zhao, Wayne Xin and He, Gaole and Wei, Zhicheng and Yuan, Nicholas Jing and Wen, Ji-Rong},
  date = {2020-10-19},
  journaltitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  eprint = {2010.01480},
  eprinttype = {arxiv},
  pages = {735--744},
  doi = {10.1145/3340531.3411893},
  url = {http://arxiv.org/abs/2010.01480},
  urldate = {2021-10-04},
  abstract = {Personalized review generation (PRG) aims to automatically produce review text reflecting user preference, which is a challenging natural language generation task. Most of previous studies do not explicitly model factual description of products, tending to generate uninformative content. Moreover, they mainly focus on word-level generation, but cannot accurately reflect more abstractive user preference in multiple aspects. To address the above issues, we propose a novel knowledge-enhanced PRG model based on capsule graph neural network\textasciitilde (Caps-GNN). We first construct a heterogeneous knowledge graph (HKG) for utilizing rich item attributes. We adopt Caps-GNN to learn graph capsules for encoding underlying characteristics from the HKG. Our generation process contains two major steps, namely aspect sequence generation and sentence generation. First, based on graph capsules, we adaptively learn aspect capsules for inferring the aspect sequence. Then, conditioned on the inferred aspect label, we design a graph-based copy mechanism to generate sentences by incorporating related entities or words from HKG. To our knowledge, we are the first to utilize knowledge graph for the PRG task. The incorporated KG information is able to enhance user preference at both aspect and word levels. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our model on the PRG task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\H6FZIYRS\\Li et al_2020_Knowledge-Enhanced Personalized Review Generation with Capsule Graph Neural.pdf;D\:\\Zotero\\storage\\CDJVQ3H4\\2010.html}
}

@inproceedings{liLearnableAggregatingNet2019,
  title = {Learnable {{Aggregating Net}} with {{Diversity Learning}} for {{Video Question Answering}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Multimedia}}},
  author = {Li, Xiangpeng and Gao, Lianli and Wang, Xuanhan and Liu, Wu and Xu, Xing and Shen, Heng Tao and Song, Jingkuan},
  date = {2019-10-15},
  pages = {1166--1174},
  publisher = {{ACM}},
  location = {{Nice France}},
  doi = {10.1145/3343031.3350971},
  url = {https://dl.acm.org/doi/10.1145/3343031.3350971},
  urldate = {2021-03-11},
  eventtitle = {{{MM}} '19: {{The}} 27th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-6889-6},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2SLHVHSZ\\Li 等。 - 2019 - Learnable Aggregating Net with Diversity Learning .pdf}
}

@inproceedings{liLearningDisambiguateAsking2017,
  title = {Learning to {{Disambiguate}} by {{Asking Discriminative Questions}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Li, Yining and Huang, Chen and Tang, Xiaoou and Loy, Chen Change},
  date = {2017-10},
  pages = {3439--3448},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.370},
  url = {http://ieeexplore.ieee.org/document/8237632/},
  urldate = {2021-09-10},
  abstract = {The ability to ask questions is a powerful tool to gather information in order to learn about the world and resolve ambiguities. In this paper, we explore a novel problem of generating discriminative questions to help disambiguate visual instances. Our work can be seen as a complement and new extension to the rich research studies on image captioning and question answering. We introduce the first large-scale dataset with over 10,000 carefully annotated images-question tuples to facilitate benchmarking. In particular, each tuple consists of a pair of images and 4.6 discriminative questions (as positive samples) and 5.9 non-discriminative questions (as negative samples) on average. In addition, we present an effective method for visual discriminative question generation. The method can be trained in a weakly supervised manner without discriminative images-question tuples but just existing visual question answering datasets. Promising results are shown against representative baselines through quantitative evaluations and user studies.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\MBLQUWV4\\Li 等。 - 2017 - Learning to Disambiguate by Asking Discriminative .pdf}
}

@incollection{liLearningHierarchicalReasoning2021,
  title = {Learning {{Hierarchical Reasoning}} for {{Text-Based Visual Question Answering}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2021},
  author = {Li, Caiyuan and Du, Qinyi and Wang, Qingqing and Jin, Yaohui},
  editor = {Farkaš, Igor and Masulli, Paolo and Otte, Sebastian and Wermter, Stefan},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12893},
  pages = {305--316},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-86365-4_25},
  url = {https://link.springer.com/10.1007/978-3-030-86365-4_25},
  urldate = {2022-02-08},
  abstract = {Text-based visual question answering (TextVQA) task needs to answer questions based on the objects and text information in image, which involves the joint reasoning over three modalities - question, visual objects, and text in image. Recent approaches on textVQA regard three modalities as joint input of transformers. However, these implicit reasoning methods do not make full use of multi-modal information, especially visual modality. To this end, we propose a novel model for textVQA based on reasoning explicitly in human-like mode. Firstly, the relevance between different objects and question is obtained. Then, the object modality is fused into the text modality weighted by obtained relevance. Finally, the amended text modality is used to predict the answer. In contrast to previous multi-modal free fusion strategy, our method can make the reasoning process more explicit and robust. Moreover, a prior-based loss is proposed to constrain object-question relevance. Extensive experimental results on several benchmark datasets well demonstrate the superior performance of our hierarchical reasoning framework over current state-of-the-art methods.},
  isbn = {978-3-030-86364-7 978-3-030-86365-4},
  langid = {english}
}

@article{linCoarsetofineCapsuleNetwork2021,
  title = {A Coarse-to-Fine Capsule Network for Fine-Grained Image Categorization},
  author = {Lin, Zhongqi and Jia, Jingdun and Huang, Feng and Gao, Wanlin},
  date = {2021-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {456},
  pages = {200--219},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.05.032},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221007852},
  urldate = {2021-10-18},
  langid = {english}
}

@article{linDynamicContextguidedCapsule2020,
  title = {Dynamic {{Context-guided Capsule Network}} for {{Multimodal Machine Translation}}},
  author = {Lin, Huan and Meng, Fandong and Su, Jinsong and Yin, Yongjing and Yang, Zhengyuan and Ge, Yubin and Zhou, Jie and Luo, Jiebo},
  date = {2020-10-12},
  journaltitle = {Proceedings of the 28th ACM International Conference on Multimedia},
  eprint = {2009.02016},
  eprinttype = {arxiv},
  pages = {1320--1329},
  doi = {10.1145/3394171.3413715},
  url = {http://arxiv.org/abs/2009.02016},
  urldate = {2021-09-26},
  abstract = {Multimodal machine translation (MMT), which mainly focuses on enhancing text-only translation with visual features, has attracted considerable attention from both computer vision and natural language processing communities. Most current MMT models resort to attention mechanism, global context modeling or multimodal joint representation learning to utilize visual features. However, the attention mechanism lacks sufficient semantic interactions between modalities while the other two provide fixed visual context, which is unsuitable for modeling the observed variability when generating translation. To address the above issues, in this paper, we propose a novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at each timestep of decoding, we first employ the conventional source-target attention to produce a timestep-specific source-side context vector. Next, DCCN takes this vector as input and uses it to guide the iterative extraction of related visual features via a context-guided dynamic routing mechanism. Particularly, we represent the input image with global and regional visual features, we introduce two parallel DCCNs to model multimodal context vectors with visual features at different granularities. Finally, we obtain two multimodal context vectors, which are fused and incorporated into the decoder for the prediction of the target word. Experimental results on the Multi30K dataset of English-to-German and English-to-French translation demonstrate the superiority of DCCN. Our code is available on https://github.com/DeepLearnXMU/MM-DCCN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\Q5DAQBNN\\Lin et al_2020_Dynamic Context-guided Capsule Network for Multimodal Machine Translation.pdf;D\:\\Zotero\\storage\\M82X794R\\2009.html}
}

@inproceedings{linFeatureEnhancementAttention2018,
  title = {Feature {{Enhancement}} in {{Attention}} for {{Visual Question Answering}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Lin, Yuetan and Pang, Zhangyang and Wang, Donghui and Zhuang, Yueting},
  date = {2018-07},
  pages = {4216--4222},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/586},
  url = {https://www.ijcai.org/proceedings/2018/586},
  urldate = {2021-09-10},
  abstract = {Attention mechanism has been an indispensable part of Visual Question Answering (VQA) models, due to the importance of its selective ability on image regions and/or question words. However, attention mechanism in almost all the VQA models takes as input the image visual and question textual features, which stem from different sources and between which there exists essential semantic gap. In order to further improve the accuracy of correlation between region and question in attention, we focus on region representation and propose the idea of feature enhancement, which includes three aspects. (1) We propose to leverage region semantic representation which is more consistent with the question representation. (2) We enrich the region representation using features from multiple hierarchies and (3) we refine the semantic representation for richer information. With these three incremental feature enhancement mechanisms, we improve the region representation and achieve better attentive effect and VQA performance. We conduct extensive experiments on the largest VQA v2.0 benchmark dataset and achieve competitive results without additional training data, and prove the effectiveness of our proposed feature-enhanced attention by visual demonstrations.},
  eventtitle = {Twenty-{{Seventh International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-18}}\vphantom\{\}},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\TWSDUTFP\\Lin 等。 - 2018 - Feature Enhancement in Attention for Visual Questi.pdf}
}

@unpublished{linImprovingGraphCollaborative2022,
  title = {Improving {{Graph Collaborative Filtering}} with {{Neighborhood-enriched Contrastive Learning}}},
  author = {Lin, Zihan and Tian, Changxin and Hou, Yupeng and Zhao, Wayne Xin},
  date = {2022-02-15},
  eprint = {2202.06200},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3485447.3512104},
  url = {http://arxiv.org/abs/2202.06200},
  urldate = {2022-03-17},
  abstract = {Recently, graph collaborative filtering methods have been proposed as an effective recommendation approach, which can capture users’ preference over items by modeling the user-item interaction graphs. Despite the effectiveness, these methods suffer from data sparsity in real scenarios. In order to reduce the influence of data sparsity, contrastive learning is adopted in graph collaborative filtering for enhancing the performance. However, these methods typically construct the contrastive pairs by random sampling, which neglect the neighboring relations among users (or items) and fail to fully exploit the potential of contrastive learning for recommendation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\R6EZGZ4A\\2202.06200 (1).pdf}
}

@unpublished{linImprovingGraphCollaborative2022a,
  title = {Improving {{Graph Collaborative Filtering}} with {{Neighborhood-enriched Contrastive Learning}}},
  author = {Lin, Zihan and Tian, Changxin and Hou, Yupeng and Zhao, Wayne Xin},
  date = {2022-02-15},
  eprint = {2202.06200},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3485447.3512104},
  url = {http://arxiv.org/abs/2202.06200},
  urldate = {2022-03-17},
  abstract = {Recently, graph collaborative filtering methods have been proposed as an effective recommendation approach, which can capture users' preference over items by modeling the user-item interaction graphs. In order to reduce the influence of data sparsity, contrastive learning is adopted in graph collaborative filtering for enhancing the performance. However, these methods typically construct the contrastive pairs by random sampling, which neglect the neighboring relations among users (or items) and fail to fully exploit the potential of contrastive learning for recommendation. To tackle the above issue, we propose a novel contrastive learning approach, named Neighborhood-enriched Contrastive Learning, named NCL, which explicitly incorporates the potential neighbors into contrastive pairs. Specifically, we introduce the neighbors of a user (or an item) from graph structure and semantic space respectively. For the structural neighbors on the interaction graph, we develop a novel structure-contrastive objective that regards users (or items) and their structural neighbors as positive contrastive pairs. In implementation, the representations of users (or items) and neighbors correspond to the outputs of different GNN layers. Furthermore, to excavate the potential neighbor relation in semantic space, we assume that users with similar representations are within the semantic neighborhood, and incorporate these semantic neighbors into the prototype-contrastive objective. The proposed NCL can be optimized with EM algorithm and generalized to apply to graph collaborative filtering methods. Extensive experiments on five public datasets demonstrate the effectiveness of the proposed NCL, notably with 26\% and 17\% performance gain over a competitive graph collaborative filtering base model on the Yelp and Amazon-book datasets respectively. Our code is available at: https://github.com/RUCAIBox/NCL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\4HD5R64V\\Lin et al_2022_Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive.pdf;D\:\\Zotero\\storage\\A5ZLVG37\\2202.html}
}

@unpublished{linImprovingGraphCollaborative2022b,
  title = {Improving {{Graph Collaborative Filtering}} with {{Neighborhood-enriched Contrastive Learning}}},
  author = {Lin, Zihan and Tian, Changxin and Hou, Yupeng and Zhao, Wayne Xin},
  date = {2022-02-15},
  eprint = {2202.06200},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3485447.3512104},
  url = {http://arxiv.org/abs/2202.06200},
  urldate = {2022-03-17},
  abstract = {Recently, graph collaborative filtering methods have been proposed as an effective recommendation approach, which can capture users' preference over items by modeling the user-item interaction graphs. In order to reduce the influence of data sparsity, contrastive learning is adopted in graph collaborative filtering for enhancing the performance. However, these methods typically construct the contrastive pairs by random sampling, which neglect the neighboring relations among users (or items) and fail to fully exploit the potential of contrastive learning for recommendation. To tackle the above issue, we propose a novel contrastive learning approach, named Neighborhood-enriched Contrastive Learning, named NCL, which explicitly incorporates the potential neighbors into contrastive pairs. Specifically, we introduce the neighbors of a user (or an item) from graph structure and semantic space respectively. For the structural neighbors on the interaction graph, we develop a novel structure-contrastive objective that regards users (or items) and their structural neighbors as positive contrastive pairs. In implementation, the representations of users (or items) and neighbors correspond to the outputs of different GNN layers. Furthermore, to excavate the potential neighbor relation in semantic space, we assume that users with similar representations are within the semantic neighborhood, and incorporate these semantic neighbors into the prototype-contrastive objective. The proposed NCL can be optimized with EM algorithm and generalized to apply to graph collaborative filtering methods. Extensive experiments on five public datasets demonstrate the effectiveness of the proposed NCL, notably with 26\% and 17\% performance gain over a competitive graph collaborative filtering base model on the Yelp and Amazon-book datasets respectively. Our code is available at: https://github.com/RUCAIBox/NCL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\PRILFWSA\\Lin et al_2022_Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive.pdf;D\:\\Zotero\\storage\\XCSGNRFY\\2202.html}
}

@unpublished{linMedicalVisualQuestion2021,
  title = {Medical {{Visual Question Answering}}: {{A Survey}}},
  shorttitle = {Medical {{Visual Question Answering}}},
  author = {Lin, Zhihong and Zhang, Donghao and Tac, Qingyi and Shi, Danli and Haffari, Gholamreza and Wu, Qi and He, Mingguang and Ge, Zongyuan},
  date = {2021-11-19},
  eprint = {2111.10056},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.10056},
  urldate = {2021-12-08},
  abstract = {Medical Visual Question Answering (VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features. In the first part of this survey, we cover and discuss the publicly available medical VQA datasets up to date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\8NUWRCX2\\Lin et al_2021_Medical Visual Question Answering.pdf;D\:\\Zotero\\storage\\LGWTAFAS\\2111.html}
}

@unpublished{linMeshGraphormer2021,
  title = {Mesh {{Graphormer}}},
  author = {Lin, Kevin and Wang, Lijuan and Liu, Zicheng},
  date = {2021-08-15},
  eprint = {2104.00272},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.00272},
  urldate = {2022-03-26},
  abstract = {We present a graph-convolution-reinforced transformer, named Mesh Graphormer, for 3D human pose and mesh reconstruction from a single image. Recently both transformers and graph convolutional neural networks (GCNNs) have shown promising progress in human mesh reconstruction. Transformer-based approaches are effective in modeling non-local interactions among 3D mesh vertices and body joints, whereas GCNNs are good at exploiting neighborhood vertex interactions based on a pre-specified mesh topology. In this paper, we study how to combine graph convolutions and self-attentions in a transformer to model both local and global interactions. Experimental results show that our proposed method, Mesh Graphormer, significantly outperforms the previous state-of-the-art methods on multiple benchmarks, including Human3.6M, 3DPW, and FreiHAND datasets. Code and pre-trained models are available at https://github.com/microsoft/MeshGraphormer},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\H6NDCZLH\\Lin et al_2021_Mesh Graphormer.pdf;D\:\\Zotero\\storage\\5XBUB6MH\\2104.html}
}

@unpublished{linSurveyTransformers2021,
  title = {A {{Survey}} of {{Transformers}}},
  author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  date = {2021-06-15},
  eprint = {2106.04554},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.04554},
  urldate = {2022-03-26},
  abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\VNRD7NCZ\\Lin et al_2021_A Survey of Transformers.pdf;D\:\\Zotero\\storage\\XSGB4XNG\\2106.html}
}

@unpublished{liOscarObjectSemanticsAligned2020,
  title = {Oscar: {{Object-Semantics Aligned Pre-training}} for {{Vision-Language Tasks}}},
  shorttitle = {Oscar},
  author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng},
  date = {2020-07-25},
  eprint = {2004.06165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.06165},
  urldate = {2021-08-25},
  abstract = {Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\VTVG7DJP\\Li 等。 - 2020 - Oscar Object-Semantics Aligned Pre-training for V.pdf}
}

@unpublished{liRelationAwareGraphAttention2019,
  title = {Relation-{{Aware Graph Attention Network}} for {{Visual Question Answering}}},
  author = {Li, Linjie and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  date = {2019-10-09},
  eprint = {1903.12314},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1903.12314},
  urldate = {2021-09-26},
  abstract = {In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA.},
  archiveprefix = {arXiv},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\GDESUZDR\\Li et al_2019_Relation-Aware Graph Attention Network for Visual Question Answering.pdf;D\:\\Zotero\\storage\\GNW3SZRN\\1903.html}
}

@article{liRNNsPositionalSelfAttention2019,
  title = {Beyond {{RNNs}}: {{Positional Self-Attention}} with {{Co-Attention}} for {{Video Question Answering}}},
  shorttitle = {Beyond {{RNNs}}},
  author = {Li, Xiangpeng and Song, Jingkuan and Gao, Lianli and Liu, Xianglong and Huang, Wenbing and He, Xiangnan and Gan, Chuang},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {8658--8665},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33018658},
  url = {https://144.208.67.177/ojs/index.php/AAAI/article/view/4887},
  urldate = {2021-09-10},
  abstract = {Most of the recent progresses on visual question answering are based on recurrent neural networks (RNNs) with attention. Despite the success, these models are often timeconsuming and having difficulties in modeling long range dependencies due to the sequential nature of RNNs. We propose a new architecture, Positional Self-Attention with Coattention (PSAC), which does not require RNNs for video question answering. Specifically, inspired by the success of self-attention in machine translation task, we propose a Positional Self-Attention to calculate the response at each position by attending to all positions within the same sequence, and then add representations of absolute positions. Therefore, PSAC can exploit the global dependencies of question and temporal information in the video, and make the process of question and video encoding executed in parallel. Furthermore, in addition to attending to the video features relevant to the given questions (i.e., video attention), we utilize the co-attention mechanism by simultaneously modeling “what words to listen to” (question attention). To the best of our knowledge, this is the first work of replacing RNNs with selfattention for the task of visual question answering. Experimental results of four tasks on the benchmark dataset show that our model significantly outperforms the state-of-the-art on three tasks and attains comparable result on the Count task. Our model requires less computation time and achieves better performance compared with the RNNs-based methods. Additional ablation study demonstrates the effect of each component of our proposed model.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\XZCVXLTU\\Li 等。 - 2019 - Beyond RNNs Positional Self-Attention with Co-Att.pdf}
}

@unpublished{liSimIPUSimple2D2022,
  title = {{{SimIPU}}: {{Simple 2D Image}} and {{3D Point Cloud Unsupervised Pre-Training}} for {{Spatial-Aware Visual Representations}}},
  shorttitle = {{{SimIPU}}},
  author = {Li, Zhenyu and Chen, Zehui and Li, Ang and Fang, Liangji and Jiang, Qinhong and Liu, Xianming and Jiang, Junjun and Zhou, Bolei and Zhao, Hang},
  date = {2022-01-17},
  eprint = {2112.04680},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.04680},
  urldate = {2022-04-25},
  abstract = {Pre-training has become a standard paradigm in many computer vision tasks. However, most of the methods are generally designed on the RGB image domain. Due to the discrepancy between the two-dimensional image plane and the three-dimensional space, such pre-trained models fail to perceive spatial information and serve as sub-optimal solutions for 3D-related tasks. To bridge this gap, we aim to learn a spatial-aware visual representation that can describe the three-dimensional space and is more suitable and effective for these tasks. To leverage point clouds, which are much more superior in providing spatial information compared to images, we propose a simple yet effective 2D Image and 3D Point cloud Unsupervised pre-training strategy, called SimIPU. Specifically, we develop a multi-modal contrastive learning framework that consists of an intra-modal spatial perception module to learn a spatial-aware representation from point clouds and an inter-modal feature interaction module to transfer the capability of perceiving spatial information from the point cloud encoder to the image encoder, respectively. Positive pairs for contrastive losses are established by the matching algorithm and the projection matrix. The whole framework is trained in an unsupervised end-to-end fashion. To the best of our knowledge, this is the first study to explore contrastive learning pre-training strategies for outdoor multi-modal datasets, containing paired camera images and LIDAR point clouds. Codes and models are available at https://github.com/zhyever/SimIPU.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\9M82M76A\\Li et al_2022_SimIPU.pdf;D\:\\Zotero\\storage\\2BWQEDCN\\2112.html}
}

@inproceedings{liTextbookQuestionAnswering2018,
  title = {Textbook {{Question Answering Under Instructor Guidance}} with {{Memory Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Juzheng and Su, Hang and Zhu, Jun and Wang, Siyu and Zhang, Bo},
  date = {2018-06},
  pages = {3655--3663},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00385},
  url = {https://ieeexplore.ieee.org/document/8578483/},
  urldate = {2021-09-09},
  abstract = {Textbook Question Answering (TQA) is a task to choose the most proper answers by reading a multi-modal context of abundant essays and images. TQA serves as a favorable test bed for visual and textual reasoning. However, most of the current methods are incapable of reasoning over the long contexts and images. To address this issue, we propose a novel approach of Instructor Guidance with Memory Networks (IGMN) which conducts the TQA task by finding contradictions between the candidate answers and their corresponding context. We build the Contradiction Entity-Relationship Graph (CERG) to extend the passage-level multi-modal contradictions to an essay level. The machine thus performs as an instructor to extract the essay-level contradictions as the Guidance. Afterwards, we exploit the memory networks to capture the information in the Guidance, and use the attention mechanisms to jointly reason over the global features of the multi-modal input. Extensive experiments demonstrate that our method outperforms the state-of-the-arts on the TQA dataset. The source code is available at https://github.com/freerailway/igmn.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  keywords = {TQA},
  annotation = {3 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\YEKQE2E8\\Li 等。 - 2018 - Textbook Question Answering Under Instructor Guida.pdf}
}

@article{liTextInstanceGraphExploring2021,
  title = {Text-{{Instance Graph}}: {{Exploring}} the {{Relational Semantics}} for {{Text-based Visual Question Answering}}},
  shorttitle = {Text-{{Instance Graph}}},
  author = {Li, Xiangpeng and Wu, Bo and Song, Jingkuan and Gao, Lianli and Zeng, Pengpeng and Gan, Chuang},
  date = {2021-11},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  pages = {108455},
  issn = {00313203},
  doi = {10.1016/j.patcog.2021.108455},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321006312},
  urldate = {2021-12-24},
  langid = {english},
  file = {D\:\\Zotero\\storage\\XFN97SG4\\Li 等。 - 2021 - Text-Instance Graph Exploring the Relational Sema.pdf}
}

@article{liTextinstanceGraphExploring2022,
  title = {Text-Instance Graph: {{Exploring}} the Relational Semantics for Text-Based Visual Question Answering},
  shorttitle = {Text-Instance Graph},
  author = {Li, Xiangpeng and Wu, Bo and Song, Jingkuan and Gao, Lianli and Zeng, Pengpeng and Gan, Chuang},
  date = {2022-04},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {124},
  pages = {108455},
  issn = {00313203},
  doi = {10.1016/j.patcog.2021.108455},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321006312},
  urldate = {2022-02-03},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BCPNZRL6\\Li 等。 - 2022 - Text-instance graph Exploring the relational sema.pdf}
}

@inproceedings{liuCambriconInstructionSet2016,
  title = {Cambricon: {{An Instruction Set Architecture}} for {{Neural Networks}}},
  shorttitle = {Cambricon},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Liu, Shaoli and Du, Zidong and Tao, Jinhua and Han, Dong and Luo, Tao and Xie, Yuan and Chen, Yunji and Chen, Tianshi},
  date = {2016-06},
  pages = {393--405},
  publisher = {{IEEE}},
  location = {{Seoul, South Korea}},
  doi = {10.1109/ISCA.2016.42},
  url = {http://ieeexplore.ieee.org/document/7551409/},
  urldate = {2021-11-24},
  abstract = {Neural Networks (NN) are a family of models for a broad range of emerging machine learning and pattern recondition applications. NN techniques are conventionally executed on general-purpose processors (such as CPU and GPGPU), which are usually not energy-efficient since they invest excessive hardware resources to flexibly support various workloads. Consequently, application-specific hardware accelerators for neural networks have been proposed recently to improve the energy-efficiency. However, such accelerators were designed for a small set of NN techniques sharing similar computational patterns, and they adopt complex and informative instructions (control signals) directly corresponding to high-level functional blocks of an NN (such as layers), or even an NN as a whole. Although straightforward and easy-to-implement for a limited set of similar NN techniques, the lack of agility in the instruction set prevents such accelerator designs from supporting a variety of different NN techniques with sufficient flexibility and efficiency.},
  eventtitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  isbn = {978-1-4673-8947-1},
  langid = {english}
}

@inproceedings{liuCambriconInstructionSet2016a,
  title = {Cambricon: {{An Instruction Set Architecture}} for {{Neural Networks}}},
  shorttitle = {Cambricon},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Liu, Shaoli and Du, Zidong and Tao, Jinhua and Han, Dong and Luo, Tao and Xie, Yuan and Chen, Yunji and Chen, Tianshi},
  date = {2016-06},
  pages = {393--405},
  publisher = {{IEEE}},
  location = {{Seoul, South Korea}},
  doi = {10.1109/ISCA.2016.42},
  url = {http://ieeexplore.ieee.org/document/7551409/},
  urldate = {2021-11-24},
  abstract = {Neural Networks (NN) are a family of models for a broad range of emerging machine learning and pattern recondition applications. NN techniques are conventionally executed on general-purpose processors (such as CPU and GPGPU), which are usually not energy-efficient since they invest excessive hardware resources to flexibly support various workloads. Consequently, application-specific hardware accelerators for neural networks have been proposed recently to improve the energy-efficiency. However, such accelerators were designed for a small set of NN techniques sharing similar computational patterns, and they adopt complex and informative instructions (control signals) directly corresponding to high-level functional blocks of an NN (such as layers), or even an NN as a whole. Although straightforward and easy-to-implement for a limited set of similar NN techniques, the lack of agility in the instruction set prevents such accelerator designs from supporting a variety of different NN techniques with sufficient flexibility and efficiency.},
  eventtitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  isbn = {978-1-4673-8947-1},
  langid = {english}
}

@inproceedings{liuCascadeReasoningNetwork2020,
  title = {Cascade {{Reasoning Network}} for {{Text-based Visual Question Answering}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Liu, Fen and Xu, Guanghui and Wu, Qi and Du, Qing and Jia, Wei and Tan, Mingkui},
  date = {2020-10-12},
  pages = {4060--4069},
  publisher = {{ACM}},
  location = {{Seattle WA USA}},
  doi = {10.1145/3394171.3413924},
  url = {https://dl.acm.org/doi/10.1145/3394171.3413924},
  urldate = {2021-12-08},
  abstract = {We study the problem of text-based visual question answering (T-VQA) in this paper. Unlike general visual question answering (VQA) which only builds connections between questions and visual contents, T-VQA requires reading and reasoning over both texts and visual concepts that appear in images. Challenges in T-VQA mainly lie in three aspects: 1) It is difficult to understand the complex logic in questions and extract specific useful information from rich image contents to answer them; 2) The text-related questions are also related to visual concepts, but it is difficult to capture cross-modal relationships between the texts and the visual concepts; 3) If the OCR (optical character recognition) system fails to detect the target text, the training will be very difficult. To address these issues, we propose a novel Cascade Reasoning Network (CRN) that consists of a progressive attention module (PAM) and a multimodal reasoning graph (MRG) module. Specifically, the PAM regards the multimodal information fusion operation as a stepwise encoding process and uses the previous attention results to guide the next fusion process. The MRG aims to explicitly model the connections and interactions between texts and visual concepts. To alleviate the dependence on the OCR system, we introduce an auxiliary task to train the model with accurate supervision signals, thereby enhancing the reasoning ability of the model in question answering. Extensive experiments on three popular T-VQA datasets demonstrate the effectiveness of our method compared with SOTA methods. The source code is available at https://github.com/guanghuixu/CRN\_tvqa.},
  eventtitle = {{{MM}} '20: {{The}} 28th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-7988-5},
  langid = {english}
}

@unpublished{liuCrossModalProgressiveComprehension2021,
  title = {Cross-{{Modal Progressive Comprehension}} for {{Referring Segmentation}}},
  author = {Liu, Si and Hui, Tianrui and Huang, Shaofei and Wei, Yunchao and Li, Bo and Li, Guanbin},
  date = {2021-05-15},
  eprint = {2105.07175},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.07175},
  urldate = {2022-04-15},
  abstract = {Given a natural language expression and an image/video, the goal of referring segmentation is to produce the pixel-level masks of the entities described by the subject of the expression. Previous approaches tackle this problem by implicit feature interaction and fusion between visual and linguistic modalities in a one-stage manner. However, human tends to solve the referring problem in a progressive manner based on informative words in the expression, i.e., first roughly locating candidate entities and then distinguishing the target one. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I (Image) module and a CMPC-V (Video) module to improve referring image and video segmentation models. For image data, our CMPC-I module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the target entity as well as suppress other irrelevant ones by spatial graph reasoning. For video data, our CMPC-V module further exploits action words based on CMPC-I to highlight the correct entity matched with the action cues by temporal graph reasoning. In addition to the CMPC, we also introduce a simple yet effective Text-Guided Feature Exchange (TGFE) module to integrate the reasoned multimodal features corresponding to different levels in the visual backbone under the guidance of textual information. In this way, multi-level features can communicate with each other and be mutually refined based on the textual context. Combining CMPC-I or CMPC-V with TGFE can form our image or video version referring segmentation frameworks and our frameworks achieve new state-of-the-art performances on four referring image segmentation benchmarks and three referring video segmentation benchmarks respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\GIFGG2NB\\Liu et al_2021_Cross-Modal Progressive Comprehension for Referring Segmentation.pdf;D\:\\Zotero\\storage\\I49PH2S9\\2105.html}
}

@article{liuDeclarationbasedPromptTuning,
  title = {Declaration-Based {{Prompt Tuning}} for {{Visual Question Answering}}},
  author = {Liu, Yuhang and Wei, Wei and Peng, Daowan and Zhu, Feida},
  pages = {14},
  abstract = {In recent years, the pre-training-then-fine-tuning paradigm has yielded immense success on a wide spectrum of cross-modal tasks, such as visual question answering (VQA), in which a visual-language (VL) model is first optimized via self-supervised task objectives, e.g., masked language modeling (MLM) and image-text matching (ITM), and then fine-tuned to adapt to downstream task (e.g., VQA) via a brand-new objective function, e.g., answer prediction. However, the inconsistency of the objective forms not only severely limits the generalization of pre-trained VL models to downstream tasks, but also requires a large amount of labeled data for fine-tuning. To alleviate the problem, we propose an innovative VL fine-tuning paradigm (named Declaration-based Prompt Tuning, abbreviated as DPT), which fine-tunes the model for downstream VQA using the pre-training objectives, boosting the effective adaptation of pre-trained models to the downstream task. Specifically, DPT reformulates the VQA task via (1) textual adaptation, which converts the given questions into declarative sentence form for prompt-tuning, and (2) task adaptation, which optimizes the objective function of VQA problem in the manner of pre-training phase. Experimental results on GQA dataset show that DPT outperforms the fine-tuned counterpart by a large margin regarding accuracy in both fully-supervised (2.68\%) and zero-shot/fewshot (over 31\%) settings. The data and codes are available at https://github.com/CCIIPLab/DPT.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\EYRXVZ3B\\Liu 等。 - Declaration-based Prompt Tuning for Visual Questio.pdf}
}

@article{liuIVQAInverseVisual,
  title = {{{IVQA}}: {{Inverse Visual Question Answering}}},
  author = {Liu, Feng and Xiang, Tao and Hospedales, Timothy M and Yang, Wankou and Sun, Changyin},
  pages = {9},
  abstract = {We propose the inverse problem of Visual question answering (iVQA), and explore its suitability as a benchmark for visuo-linguistic understanding. The iVQA task is to generate a question that corresponds to a given image and answer pair. Since the answers are less informative than the questions, and the questions have less learnable bias, an iVQA model needs to better understand the image to be successful than a VQA model. We pose question generation as a multi-modal dynamic inference process and propose an iVQA model that can gradually adjust its focus of attention guided by both a partially generated question and the answer. For evaluation, apart from existing linguistic metrics, we propose a new ranking metric. This metric compares the ground truth question’s rank among a list of distractors, which allows the drawbacks of different algorithms and sources of error to be studied. Experimental results show that our model can generate diverse, grammatically correct and content correlated questions that match the given answer.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\TQPG9IY2\\Liu 等。 - IVQA Inverse Visual Question Answering.pdf}
}

@unpublished{liUniFormerUnifiedTransformer2022,
  title = {{{UniFormer}}: {{Unified Transformer}} for {{Efficient Spatiotemporal Representation Learning}}},
  shorttitle = {{{UniFormer}}},
  author = {Li, Kunchang and Wang, Yali and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  date = {2022-02-08},
  eprint = {2201.04676},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.04676},
  urldate = {2022-03-15},
  abstract = {It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local context to suppress local redundancy from a small 3D neighborhood, it lacks the capability to capture global dependency because of the limited receptive field. Alternatively, vision transformers can effectively capture long-range dependency by self-attention mechanism, while having the limitation on reducing local redundancy with blind similarity comparison among all the tokens in each layer. Based on these observations, we propose a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatiotemporal self-attention in a concise transformer format, and achieves a preferable balance between computation and accuracy. Different from traditional transformers, our relation aggregator can tackle both spatiotemporal redundancy and dependency, by learning local and global token affinity respectively in shallow and deep layers. We conduct extensive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1\&V2. With only ImageNet-1K pretraining, our UniFormer achieves 82.9\%/84.8\% top-1 accuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than other state-of-the-art methods. For Something-Something V1 and V2, our UniFormer achieves new state-of-the-art performances of 60.9\% and 71.2\% top-1 accuracy respectively. Code is available at https://github.com/Sense-X/UniFormer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\XAES8D2C\\Li et al_2022_UniFormer.pdf;D\:\\Zotero\\storage\\H2UINC6Q\\2201.html}
}

@unpublished{liuNomMerNominateSynergistic2022,
  title = {{{NomMer}}: {{Nominate Synergistic Context}} in {{Vision Transformer}} for {{Visual Recognition}}},
  shorttitle = {{{NomMer}}},
  author = {Liu, Hao and Jiang, Xinghua and Li, Xin and Bao, Zhimin and Jiang, Deqiang and Ren, Bo},
  date = {2022-03-14},
  eprint = {2111.12994},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.12994},
  urldate = {2022-05-03},
  abstract = {Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demonstrated great potential in the computer vision community. For the sake of trade-off between efficiency and performance, a group of works merely perform SA operation within local patches, whereas the global contextual information is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subsequent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model. Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different visual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dynamically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern of our proposed NomMer, we further explore what context information is focused. Beneficial from this "dynamic nomination" mechanism, without bells and whistles, the NomMer can not only achieve 84.5\% Top-1 classification accuracy on ImageNet with only 73M parameters, but also show promising performance on dense prediction tasks, i.e., object detection and semantic segmentation. The code and models will be made publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\LFPFEJJ8\\Liu et al_2022_NomMer.pdf;D\:\\Zotero\\storage\\4H93PV9T\\2111.html}
}

@inproceedings{liuPuDianNaoPolyvalentMachine2015,
  title = {{{PuDianNao}}: {{A Polyvalent Machine Learning Accelerator}}},
  shorttitle = {{{PuDianNao}}},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji},
  date = {2015-03-14},
  pages = {369--381},
  publisher = {{ACM}},
  location = {{Istanbul Turkey}},
  doi = {10.1145/2694344.2694358},
  url = {https://dl.acm.org/doi/10.1145/2694344.2694358},
  urldate = {2021-11-24},
  abstract = {Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energyefficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energyefficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous NoFree-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.},
  eventtitle = {{{ASPLOS}} '15: {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  isbn = {978-1-4503-2835-7},
  langid = {english}
}

@unpublished{liuSAMSelfadaptiveAttention2021,
  title = {{{SAM}}: {{A Self-adaptive Attention Module}} for {{Context-Aware Recommendation System}}},
  shorttitle = {{{SAM}}},
  author = {Liu, Jiabin and Wei, Zheng and Li, Zhengpin and Mao, Xiaojun and Wang, Jian and Wei, Zhongyu and Zhang, Qi},
  date = {2021-10-13},
  eprint = {2110.00452},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.00452},
  urldate = {2022-03-23},
  abstract = {Recently, textual information has been proved to play a positive role in recommendation systems. However, most of the existing methods only focus on representation learning of textual information in ratings, while potential selection bias induced by the textual information is ignored. In this work, we propose a novel and general self-adaptive module, the Self-adaptive Attention Module (SAM), which adjusts the selection bias by capturing contextual information based on its representation. This module can be embedded into recommendation systems that contain learning components of contextual information. Experimental results on three real-world datasets demonstrate the effectiveness of our proposal, and the state-of-the-art models with SAM significantly outperform the original ones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\SGKC5QVZ\\Liu et al_2021_SAM.pdf;D\:\\Zotero\\storage\\JKVTEY9B\\2110.html}
}

@misc{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-08-17},
  number = {arXiv:2103.14030},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2022-08-20},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{liVisualQuestionAnswering2019,
  title = {Visual {{Question Answering}} as {{Reading Comprehension}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Hui and Wang, Peng and Shen, Chunhua and van den Hengel, Anton},
  date = {2019-06},
  pages = {6312--6321},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00648},
  url = {https://ieeexplore.ieee.org/document/8953527/},
  urldate = {2021-09-09},
  abstract = {Visual question answering (VQA) demands simultaneous comprehension of both the image visual content and natural language questions. In some cases, the reasoning needs the help of common sense or general knowledge which usually appear in the form of text. Current methods jointly embed both the visual information and the textual feature into the same space. Nevertheless, how to model the complex interactions between the two different modalities is not an easy work. In contrast to struggling on multimodal feature fusion, in this paper, we propose to unify all the input information by natural language so as to convert VQA into a machine reading comprehension problem. With this transformation, our method not only can tackle VQA datasets that focus on observation based questions, but can also be naturally extended to handle knowledge-based VQA which requires to explore large-scale external knowledge base. It is a step towards being able to exploit large volumes of text and natural language processing techniques to address VQA problem. Two types of models are proposed to deal with open-ended VQA and multiple-choice VQA respectively. We evaluate our models on three VQA benchmarks. The comparable performance with the state-of-theart demonstrates the effectiveness of the proposed method.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {9 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\3NZLRBQF\\Li 等。 - 2019 - Visual Question Answering as Reading Comprehension.pdf}
}

@article{liVisualQuestionGeneration,
  title = {Visual {{Question Generation}} as {{Dual Task}} of {{Visual Question Answering}}},
  author = {Li, Yikang and Duan, Nan and Zhou, Bolei and Chu, Xiao and Ouyang, Wanli and Wang, Xiaogang and Zhou, Ming},
  pages = {9},
  langid = {english},
  annotation = {00000},
  file = {D\:\\Zotero\\storage\\PFWFCS93\\Li 等。 - Visual Question Generation as Dual Task of Visual .pdf}
}

@unpublished{liVisualQuestionGeneration2017,
  title = {Visual {{Question Generation}} as {{Dual Task}} of {{Visual Question Answering}}},
  author = {Li, Yikang and Duan, Nan and Zhou, Bolei and Chu, Xiao and Ouyang, Wanli and Wang, Xiaogang},
  date = {2017-09-21},
  eprint = {1709.07192},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1709.07192},
  urldate = {2021-08-26},
  abstract = {Recently visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, which have been explored separately. In this work, we propose an end-to-end unified framework, the Invertible Question Answering Network (iQAN), to leverage the complementary relations between questions and answers in images by jointly training the model on VQA and VQG tasks. Corresponding parameter sharing scheme and regular terms are proposed as constraints to explicitly leverage Q,A's dependencies to guide the training process. After training, iQAN can take either question or answer as input, then output the counterpart. Evaluated on the large-scale visual question answering datasets CLEVR and VQA2, our iQAN improves the VQA accuracy over the baselines. We also show the dual learning framework of iQAN can be generalized to other VQA architectures and consistently improve the results over both the VQA and VQG tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\F85299PS\\Li et al_2017_Visual Question Generation as Dual Task of Visual Question Answering.pdf;D\:\\Zotero\\storage\\JTRVC887\\1709.html}
}

@article{liVisualSemanticGraphReasoning2019,
  title = {Visual-{{Semantic Graph Reasoning}} for {{Pedestrian Attribute Recognition}}},
  author = {Li, Qiaozhe and Zhao, Xin and He, Ran and Huang, Kaiqi},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {8634--8641},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33018634},
  url = {https://144.208.67.177/ojs/index.php/AAAI/article/view/4884},
  urldate = {2022-03-09},
  abstract = {Pedestrian attribute recognition in surveillance is a challenging task due to poor image quality, significant appearance variations and diverse spatial distribution of different attributes. This paper treats pedestrian attribute recognition as a sequential attribute prediction problem and proposes a novel visual-semantic graph reasoning framework to address this problem. Our framework contains a spatial graph and a directed semantic graph. By performing reasoning using the Graph Convolutional Network (GCN), one graph captures spatial relations between regions and the other learns potential semantic relations between attributes. An end-to-end architecture is presented to perform mutual embedding between these two graphs to guide the relational learning for each other. We verify the proposed framework on three large scale pedestrian attribute datasets including PETA, RAP, and PA100k. Experiments show superiority of the proposed method over state-of-the-art methods and effectiveness of our joint GCN structures for sequential attribute prediction.},
  langid = {english}
}

@unpublished{liZeroShotTransferVQA2018,
  title = {Zero-{{Shot Transfer VQA Dataset}}},
  author = {Li, Yuanpeng and Yang, Yi and Wang, Jianyu and Xu, Wei},
  date = {2018-11-01},
  eprint = {1811.00692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.00692},
  urldate = {2021-09-10},
  abstract = {Acquiring a large vocabulary is an important aspect of human intelligence. Onecommon approach for human to populating vocabulary is to learn words duringreading or listening, and then use them in writing or speaking. This ability totransfer from input to output is natural for human, but it is difficult for machines.Human spontaneously performs this knowledge transfer in complicated multimodaltasks, such as Visual Question Answering (VQA). In order to approach human-levelArtificial Intelligence, we hope to equip machines with such ability. Therefore, toaccelerate this research, we propose a newzero-shot transfer VQA(ZST-VQA)dataset by reorganizing the existing VQA v1.0 dataset in the way that duringtraining, some words appear only in one module (i.e. questions) but not in theother (i.e. answers). In this setting, an intelligent model should understand andlearn the concepts from one module (i.e. questions), and at test time, transfer themto the other (i.e. predict the concepts as answers). We conduct evaluation on thisnew dataset using three existing state-of-the-art VQA neural models. Experimentalresults show a significant drop in performance on this dataset, indicating existingmethods do not address the zero-shot transfer problem. Besides, our analysis findsthat this may be caused by the implicit bias learned during training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\KFIKFT82\\Li et al_2018_Zero-Shot Transfer VQA Dataset.pdf;D\:\\Zotero\\storage\\2EPBCY34\\1811.html}
}

@article{longImprovingReasoningContrastive2021,
  title = {Improving Reasoning with Contrastive Visual Information for Visual Question Answering},
  author = {Long, Yu and Tang, Pengjie and Wang, Hanli and Yu, Jian},
  date = {2021-09},
  journaltitle = {Electronics Letters},
  shortjournal = {Electron. lett.},
  volume = {57},
  number = {20},
  pages = {758--760},
  issn = {0013-5194, 1350-911X},
  doi = {10.1049/ell2.12255},
  url = {https://onlinelibrary.wiley.com/doi/10.1049/ell2.12255},
  urldate = {2021-10-16},
  langid = {english}
}

@unpublished{luBestBothWorlds2017,
  title = {Best of {{Both Worlds}}: {{Transferring Knowledge}} from {{Discriminative Learning}} to a {{Generative Visual Dialog Model}}},
  shorttitle = {Best of {{Both Worlds}}},
  author = {Lu, Jiasen and Kannan, Anitha and Yang, Jianwei and Parikh, Devi and Batra, Dhruv},
  date = {2017-10-27},
  eprint = {1706.01554},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1706.01554},
  urldate = {2021-12-08},
  abstract = {We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses ("I don't know", "I can't tell"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it cannot be deployed to have real conversations with users. Our work aims to achieve the best of both worlds -- the practical usefulness of G and the strong performance of D -- via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -- specifically, an RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67\% on recall@10). The source code can be downloaded from https://github.com/jiasenlu/visDial.pytorch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\ZRA5B8YY\\Lu et al_2017_Best of Both Worlds.pdf;D\:\\Zotero\\storage\\64B5DHRZ\\1706.html}
}

@article{luCoAttendingFreeFormRegions,
  title = {Co-{{Attending Free-Form Regions}} and {{Detections}} with {{Multi-Modal Multiplicative Feature Embedding}} for {{Visual Question Answering}}},
  author = {Lu, Pan and Li, Hongsheng and Zhang, Wei and Wang, Jianyong and Wang, Xiaogang},
  pages = {8},
  abstract = {Recently, the Visual Question Answering (VQA) task has gained increasing attention in artificial intelligence. Existing VQA methods mainly adopt the visual attention mechanism to associate the input question with corresponding image regions for effective question answering. The free-form region based and the detection-based visual attention mechanisms are mostly investigated, with the former ones attending free-form image regions and the latter ones attending prespecified detection-box regions. We argue that the two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve the VQA problem. In this paper, we propose a novel deep neural network for VQA that integrates both attention mechanisms. Our proposed framework effectively fuses features from free-form image regions, detection boxes, and question representations via a multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes for more accurate question answering. The proposed method is extensively evaluated on two publicly available datasets, COCO-QA and VQA, and outperforms state-of-the-art approaches. Source code is available at https://github.com/lupantech/dual-mfa-vqa.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\PK7NT5QI\\Lu 等。 - Co-Attending Free-Form Regions and Detections with.pdf}
}

@unpublished{luHierarchicalQuestionImageCoAttention2017,
  title = {Hierarchical {{Question-Image Co-Attention}} for {{Visual Question Answering}}},
  author = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  date = {2017-01-19},
  eprint = {1606.00061},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.00061},
  urldate = {2021-10-11},
  abstract = {A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling “where to look” or visual attention, it is equally important to model “what words to listen to” or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3\% to 60.5\%, and from 61.6\% to 63.3\% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1\% for VQA and 65.4\% for COCO-QA.1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\BDGPPCAZ\\Lu 等。 - 2017 - Hierarchical Question-Image Co-Attention for Visua.pdf}
}

@unpublished{luLocalizeGroupSelect2021,
  title = {Localize, {{Group}}, and {{Select}}: {{Boosting Text-VQA}} by {{Scene Text Modeling}}},
  shorttitle = {Localize, {{Group}}, and {{Select}}},
  author = {Lu, Xiaopeng and Fan, Zhen and Wang, Yansen and Oh, Jean and Rose, Carolyn P.},
  date = {2021-08-19},
  eprint = {2108.08965},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.08965},
  urldate = {2021-12-10},
  abstract = {As an important task in multimodal context understanding, Text-VQA (Visual Question Answering) aims at question answering through reading text information in images. It differentiates from the original VQA task as Text-VQA requires large amounts of scene-text relationship understanding, in addition to the cross-modal grounding capability. In this paper, we propose Localize, Group, and Select (LOGOS), a novel model which attempts to tackle this problem from multiple aspects. LOGOS leverages two grounding tasks to better localize the key information of the image, utilizes scene text clustering to group individual OCR tokens, and learns to select the best answer from different sources of OCR (Optical Character Recognition) texts. Experiments show that LOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks without using additional OCR annotation data. Ablation studies and analysis demonstrate the capability of LOGOS to bridge different modalities and better understand scene text.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\ZUAKM8JX\\Lu et al_2021_Localize, Group, and Select.pdf;D\:\\Zotero\\storage\\HZE9TAAH\\2108.html}
}

@inproceedings{luoLearningMultidimensionalEdge2022,
  title = {Learning {{Multi-dimensional Edge Feature-based AU Relation Graph}} for {{Facial Action Unit Recognition}}},
  booktitle = {Proceedings of the {{Thirty-First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Luo, Cheng and Song, Siyang and Xie, Weicheng and Shen, Linlin and Gunes, Hatice},
  date = {2022-07},
  eprint = {2205.01782},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1239--1246},
  doi = {10.24963/ijcai.2022/173},
  url = {http://arxiv.org/abs/2205.01782},
  urldate = {2022-09-23},
  abstract = {The activations of Facial Action Units (AUs) mutually influence one another. While the relationship between a pair of AUs can be complex and unique, existing approaches fail to specifically and explicitly represent such cues for each pair of AUs in each facial display. This paper proposes an AU relationship modelling approach that deep learns a unique graph to explicitly describe the relationship between each pair of AUs of the target facial display. Our approach first encodes each AU's activation status and its association with other AUs into a node feature. Then, it learns a pair of multi-dimensional edge features to describe multiple task-specific relationship cues between each pair of AUs. During both node and edge feature learning, our approach also considers the influence of the unique facial display on AUs' relationship by taking the full face representation as an input. Experimental results on BP4D and DISFA datasets show that both node and edge feature learning modules provide large performance improvements for CNN and transformer-based backbones, with our best systems achieving the state-of-the-art AU recognition results. Our approach not only has a strong capability in modelling relationship cues for AU recognition but also can be easily incorporated into various backbones. Our PyTorch code is made available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\QL6Y47YE\\Luo et al_2022_Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial.pdf;D\:\\Zotero\\storage\\9VY57JLG\\2205.html}
}

@inproceedings{luoVisualAttentionMultiLabel2019,
  title = {Visual {{Attention}} in {{Multi-Label Image Classification}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Luo, Yan and Jiang, Ming and Zhao, Qi},
  date = {2019-06},
  pages = {820--827},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPRW.2019.00110},
  url = {https://ieeexplore.ieee.org/document/9025569/},
  urldate = {2021-09-23},
  abstract = {One of the most significant challenges in multi-label image classification is the learning of representative features that capture the rich semantic information in a cluttered scene. As an information bottleneck, the visual attention mechanism allows humans to selectively process the most important visual input, enabling rapid and accurate scene understanding. In this work, we study the correlation between visual attention and multi-label image classification, and exploit an extra attention pathway for improving multilabel image classification performance. Specifically, we propose a dual-stream neural network that consists of two sub-networks: one is a conventional classification model, and the other is a saliency prediction model trained with human fixations. Features computed with the two subnetworks are trained separately and then fine-tuned jointly using a multiple cross entropy loss. Experimental results show that the new saliency sub-network improves multilabel image classification performance on the MS COCO dataset. The improvement is consistent across various levels of scene clutteredness.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-72812-506-0},
  langid = {english}
}

@article{luRVQALearningVisual2018,
  title = {R-{{VQA}}: {{Learning Visual Relation Facts}} with {{Semantic Attention}} for {{Visual Question Answering}}},
  shorttitle = {R-{{VQA}}},
  author = {Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong},
  date = {2018-07-19},
  journaltitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  eprint = {1805.09701},
  eprinttype = {arxiv},
  pages = {1880--1889},
  doi = {10.1145/3219819.3220036},
  url = {http://arxiv.org/abs/1805.09701},
  urldate = {2022-03-15},
  abstract = {Recently, Visual Question Answering (VQA) has emerged as one of the most significant tasks in multimodal learning as it requires understanding both visual and textual modalities. Existing methods mainly rely on extracting image and question features to learn their joint feature embedding via multimodal fusion or attention mechanism. Some recent studies utilize external VQA-independent models to detect candidate entities or attributes in images, which serve as semantic knowledge complementary to the VQA task. However, these candidate entities or attributes might be unrelated to the VQA task and have limited semantic capacities. To better utilize semantic knowledge in images, we propose a novel framework to learn visual relation facts for VQA. Specifically, we build up a Relation-VQA (R-VQA) dataset based on the Visual Genome dataset via a semantic similarity module, in which each data consists of an image, a corresponding question, a correct answer and a supporting relation fact. A well-defined relation detector is then adopted to predict visual question-related relation facts. We further propose a multi-step attention model composed of visual attention and semantic attention sequentially to extract related visual knowledge and semantic knowledge. We conduct comprehensive experiments on the two benchmark datasets, demonstrating that our model achieves state-of-the-art performance and verifying the benefit of considering visual relation facts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\2NR77ZFM\\Lu et al_2018_R-VQA.pdf;D\:\\Zotero\\storage\\6TYXGWR8\\1805.html}
}

@article{luTransformerbasedCrossmodalFusion,
  title = {A {{Transformer-based Cross-modal Fusion Model}} with {{Adversarial Training}} for {{VQA Challenge}} 202},
  author = {Lu, Ke-Han and Fang, Bo-Han and Chen, Kuan-Yu},
  pages = {3},
  abstract = {In this paper, inspired by the successes of vision-language pre-trained models and the benefits from training with adversarial attacks, we present a novel Transformer-based cross-modal fusion modeling by incorporating the both notions for VQA challenge 2021. Specifically, the proposed model is on top of the architecture of VinVL model [19], and the adversarial training strategy [4] is applied to make the model robust and generalized. Moreover, two implementation tricks are also used in our system to obtain better results. The experiments demonstrate that the novel framework can achieve 76.72\% on VQAv2 test-std set.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\DUPN6NAQ\\Lu 等。 - A Transformer-based Cross-modal Fusion Model with .pdf}
}

@article{luViLBERTPretrainingTaskAgnostic,
  title = {{{ViLBERT}}: {{Pretraining Task-Agnostic Visiolinguistic Representations}} for {{Vision-and-Language Tasks}}},
  author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  pages = {11},
  abstract = {We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks – visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval – by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models –achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4AKX5DUY\\Lu 等。 - ViLBERT Pretraining Task-Agnostic Visiolinguistic.pdf}
}

@misc{lvCausalityInspiredRepresentation2022,
  title = {Causality {{Inspired Representation Learning}} for {{Domain Generalization}}},
  author = {Lv, Fangrui and Liang, Jian and Li, Shuang and Zang, Bin and Liu, Chi Harold and Wang, Ziteng and Liu, Di},
  date = {2022-03-27},
  number = {arXiv:2203.14237},
  eprint = {2203.14237},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.14237},
  urldate = {2022-06-02},
  abstract = {Domain generalization (DG) is essentially an out-of-distribution problem, aiming to generalize the knowledge learned from multiple source domains to an unseen target domain. The mainstream is to leverage statistical models to model the dependence between data and labels, intending to learn representations independent of domain. Nevertheless, the statistical models are superficial descriptions of reality since they are only required to model dependence instead of the intrinsic causal mechanism. When the dependence changes with the target distribution, the statistic models may fail to generalize. In this regard, we introduce a general structural causal model to formalize the DG problem. Specifically, we assume that each input is constructed from a mix of causal factors (whose relationship with the label is invariant across domains) and non-causal factors (category-independent), and only the former cause the classification judgments. Our goal is to extract the causal factors from inputs and then reconstruct the invariant causal mechanisms. However, the theoretical idea is far from practical of DG since the required causal/non-causal factors are unobserved. We highlight that ideal causal factors should meet three basic properties: separated from the non-causal ones, jointly independent, and causally sufficient for the classification. Based on that, we propose a Causality Inspired Representation Learning (CIRL) algorithm that enforces the representations to satisfy the above properties and then uses them to simulate the causal factors, which yields improved generalization ability. Extensive experimental results on several widely used datasets verify the effectiveness of our approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\4U29GDTE\\Lv 等。 - 2022 - Causality Inspired Representation Learning for Dom.pdf}
}

@unpublished{lyu2DAttentionalIrregular2019,
  title = {{{2D Attentional Irregular Scene Text Recognizer}}},
  author = {Lyu, Pengyuan and Yang, Zhicheng and Leng, Xinhang and Wu, Xiaojun and Li, Ruiyu and Shen, Xiaoyong},
  date = {2019-06-13},
  eprint = {1906.05708},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1906.05708},
  urldate = {2022-04-02},
  abstract = {Irregular scene text, which has complex layout in 2D space, is challenging to most previous scene text recognizers. Recently, some irregular scene text recognizers either rectify the irregular text to regular text image with approximate 1D layout or transform the 2D image feature map to 1D feature sequence. Though these methods have achieved good performance, the robustness and accuracy are still limited due to the loss of spatial information in the process of 2D to 1D transformation. Different from all of previous, we in this paper propose a framework which transforms the irregular text with 2D layout to character sequence directly via 2D attentional scheme. We utilize a relation attention module to capture the dependencies of feature maps and a parallel attention module to decode all characters in parallel, which make our method more effective and efficient. Extensive experiments on several public benchmarks as well as our collected multi-line text dataset show that our approach is effective to recognize regular and irregular scene text and outperforms previous methods both in accuracy and speed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\LR8UD65N\\Lyu et al_2019_2D Attentional Irregular Scene Text Recognizer.pdf;D\:\\Zotero\\storage\\JUZUTSI5\\1906.html}
}

@unpublished{maAreMultimodalTransformers2022,
  title = {Are {{Multimodal Transformers Robust}} to {{Missing Modality}}?},
  author = {Ma, Mengmeng and Ren, Jian and Zhao, Long and Testuggine, Davide and Peng, Xi},
  date = {2022-04-11},
  eprint = {2204.05454},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2204.05454},
  urldate = {2022-04-14},
  abstract = {Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture designs or pre-training strategies; whether Transformer models are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sensitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What surprised us is that the optimal fusion strategy is dataset dependent even for the same Transformer model; there does not exist a universal strategy that works in general cases. Based on these findings, we propose a principle method to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Experimental validations on three benchmarks support the superior performance of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\HXXBLEQM\\Ma et al_2022_Are Multimodal Transformers Robust to Missing Modality.pdf;D\:\\Zotero\\storage\\YVPPKBCR\\2204.html}
}

@unpublished{maAreMultimodalTransformers2022a,
  title = {Are {{Multimodal Transformers Robust}} to {{Missing Modality}}?},
  author = {Ma, Mengmeng and Ren, Jian and Zhao, Long and Testuggine, Davide and Peng, Xi},
  date = {2022-04-11},
  eprint = {2204.05454},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2204.05454},
  urldate = {2022-04-14},
  abstract = {Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture designs or pre-training strategies; whether Transformer models are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sensitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What surprised us is that the optimal fusion strategy is dataset dependent even for the same Transformer model; there does not exist a universal strategy that works in general cases. Based on these findings, we propose a principle method to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Experimental validations on three benchmarks support the superior performance of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\2B2Z7CMZ\\Ma et al_2022_Are Multimodal Transformers Robust to Missing Modality.pdf;D\:\\Zotero\\storage\\GPQHL27I\\2204.html}
}

@inproceedings{maflaMultiModalReasoningGraph2021,
  title = {Multi-{{Modal Reasoning Graph}} for {{Scene-Text Based Fine-Grained Image Classification}} and {{Retrieval}}},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Mafla, Andres and Dey, Sounak and Biten, Ali Furkan and Gomez, Lluis and Karatzas, Dimosthenis},
  date = {2021-01},
  pages = {4022--4032},
  publisher = {{IEEE}},
  location = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV48630.2021.00407},
  url = {https://ieeexplore.ieee.org/document/9423205/},
  urldate = {2022-02-10},
  abstract = {Scene text instances found in natural images carry explicit semantic information that can provide important cues to solve a wide array of computer vision problems. In this paper, we focus on leveraging multi-modal content in the form of visual and textual cues to tackle the task of finegrained image classification and retrieval. First, we obtain the text instances from images by employing a text reading system. Then, we combine textual features with salient image regions to exploit the complementary information carried by the two sources. Specifically, we employ a Graph Convolutional Network to perform multi-modal reasoning and obtain relationship-enhanced features by learning a common semantic space between salient objects and text found in an image. By obtaining an enhanced set of visual and textual features, the proposed model greatly outperforms previous state-of-the-art in two different tasks, fine-grained classification and image retrieval in the ConText[23] and Drink Bottle[4] datasets.},
  eventtitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-66540-477-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ETK6W4Y8\\Mafla_Multi-Modal_Reasoning_Graph_for_Scene-Text_Based_Fine-Grained_Image_Classification_and_WACV_2021_paper.pdf}
}

@inproceedings{maflaMultiModalReasoningGraph2021a,
  title = {Multi-{{Modal Reasoning Graph}} for {{Scene-Text Based Fine-Grained Image Classification}} and {{Retrieval}}},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Mafla, Andres and Dey, Sounak and Biten, Ali Furkan and Gomez, Lluis and Karatzas, Dimosthenis},
  date = {2021-01},
  pages = {4022--4032},
  publisher = {{IEEE}},
  location = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV48630.2021.00407},
  url = {https://ieeexplore.ieee.org/document/9423205/},
  urldate = {2022-02-08},
  abstract = {Scene text instances found in natural images carry explicit semantic information that can provide important cues to solve a wide array of computer vision problems. In this paper, we focus on leveraging multi-modal content in the form of visual and textual cues to tackle the task of finegrained image classification and retrieval. First, we obtain the text instances from images by employing a text reading system. Then, we combine textual features with salient image regions to exploit the complementary information carried by the two sources. Specifically, we employ a Graph Convolutional Network to perform multi-modal reasoning and obtain relationship-enhanced features by learning a common semantic space between salient objects and text found in an image. By obtaining an enhanced set of visual and textual features, the proposed model greatly outperforms previous state-of-the-art in two different tasks, fine-grained classification and image retrieval in the ConText[23] and Drink Bottle[4] datasets.},
  eventtitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-66540-477-8},
  langid = {english}
}

@inproceedings{maflaStacMRSceneTextAware2021,
  title = {{{StacMR}}: {{Scene-Text Aware Cross-Modal Retrieval}}},
  shorttitle = {{{StacMR}}},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Mafla, Andres and Rezende, Rafael S. and Gomez, Lluis and Larlus, Diane and Karatzas, Dimosthenis},
  date = {2021-01},
  pages = {2219--2229},
  publisher = {{IEEE}},
  location = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV48630.2021.00227},
  url = {https://ieeexplore.ieee.org/document/9423139/},
  urldate = {2022-02-10},
  abstract = {Recent models for cross-modal retrieval have benefited from an increasingly rich understanding of visual scenes, afforded by scene graphs and object interactions to mention a few. This has resulted in an improved matching between the visual representation of an image and the textual representation of its caption. Yet, current visual representations overlook a key aspect: the text appearing in images, which may contain crucial information for retrieval. In this paper, we first propose a new dataset that allows exploration of cross-modal retrieval where images contain scene-text instances. Then, armed with this dataset, we describe several approaches which leverage scene text, including a better scene-text aware cross-modal retrieval method which uses specialized representations for text from the captions and text from the visual scene, and reconcile them in a common embedding space. Extensive experiments confirm that cross-modal retrieval approaches benefit from scene text and highlight interesting research questions worth exploring further. Dataset and code are available at europe.naverlabs.com/stacmr.},
  eventtitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-66540-477-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\6BT99DC8\\Mafla 等。 - 2021 - StacMR Scene-Text Aware Cross-Modal Retrieval.pdf}
}

@unpublished{maiAnalyzingUnalignedMultimodal2021,
  title = {Analyzing {{Unaligned Multimodal Sequence}} via {{Graph Convolution}} and {{Graph Pooling Fusion}}},
  author = {Mai, Sijie and Xing, Songlong and He, Jiaxuan and Zeng, Ying and Hu, Haifeng},
  date = {2021-04-23},
  eprint = {2011.13572},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.13572},
  urldate = {2022-02-08},
  abstract = {In this paper, we study the task of multimodal sequence analysis which aims to draw inferences from visual, language and acoustic sequences. A majority of existing works generally focus on aligned fusion, mostly at word level, of the three modalities to accomplish this task, which is impractical in real-world scenarios. To overcome this issue, we seek to address the task of multimodal sequence analysis on unaligned modality sequences which is still relatively underexplored and also more challenging. Recurrent neural network (RNN) and its variants are widely used in multimodal sequence analysis, but they are susceptible to the issues of gradient vanishing/explosion and high time complexity due to its recurrent nature. Therefore, we propose a novel model, termed Multimodal Graph, to investigate the effectiveness of graph neural networks (GNN) on modeling multimodal sequential data. The graph-based structure enables parallel computation in time dimension and can learn longer temporal dependency in long unaligned sequences. Specifically, our Multimodal Graph is hierarchically structured to cater to two stages, i.e., intra- and inter-modal dynamics learning. For the first stage, a graph convolutional network is employed for each modality to learn intra-modal dynamics. In the second stage, given that the multimodal sequences are unaligned, the commonly considered word-level fusion does not pertain. To this end, we devise a graph pooling fusion network to automatically learn the associations between various nodes from different modalities. Additionally, we define multiple ways to construct the adjacency matrix for sequential data. Experimental results suggest that our graph-based model reaches state-of-the-art performance on two benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {D\:\\Zotero\\storage\\PLHUHLM2\\Mai et al_2021_Analyzing Unaligned Multimodal Sequence via Graph Convolution and Graph Pooling.pdf}
}

@incollection{malinowskiLearningVisualQuestion2018,
  title = {Learning {{Visual Question Answering}} by {{Bootstrapping Hard Attention}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Malinowski, Mateusz and Doersch, Carl and Santoro, Adam and Battaglia, Peter},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11210},
  pages = {3--20},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01231-1_1},
  url = {http://link.springer.com/10.1007/978-3-030-01231-1_1},
  urldate = {2021-09-10},
  abstract = {Attention mechanisms in biological perception are thought to select subsets of perceptual information for more sophisticated processing which would be prohibitive to perform on all sensory inputs. In computer vision, however, there has been relatively little exploration of hard attention, where some information is selectively ignored, in spite of the success of soft attention, where information is re-weighted and aggregated, but never filtered out. Here, we introduce a new approach for hard attention and find it achieves very competitive performance on a recently-released visual question answering datasets, equalling and in some cases surpassing similar soft attention architectures while entirely ignoring some features. Even though the hard attention mechanism is thought to be non-differentiable, we found that the feature magnitudes correlate with semantic relevance, and provide a useful signal for our mechanism’s attentional selection criterion. Because hard attention selects important features of the input information, it can also be more efficient than analogous soft attention mechanisms. This is especially important for recent approaches that use non-local pairwise operations, whereby computational and memory costs are quadratic in the size of the set of features.},
  isbn = {978-3-030-01230-4 978-3-030-01231-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\DLLPK6QE\\Malinowski 等。 - 2018 - Learning Visual Question Answering by Bootstrappin.pdf}
}

@article{manjunathaExplicitBiasDiscovery,
  title = {Explicit {{Bias Discovery}} in {{Visual Question Answering Models}}},
  author = {Manjunatha, Varun and Saini, Nirat and Davis, Larry S},
  pages = {10},
  abstract = {Researchers have observed that Visual Question Answering (VQA ) models tend to answer questions by learning statistical biases in the data. For example, their answer to the question “What is the color of the grass?” is usually “Green”, whereas a question like “What is the title of the book?” cannot be answered by inferring statistical biases. It is of interest to the community to explicitly discover such biases, both for understanding the behavior of such models, and towards debugging them. Our work address this problem. In a database, we store the words of the question, answer and visual words corresponding to regions of interest in attention maps. By running simple rule mining algorithms on this database, we discover human-interpretable rules which give us unique insight into the behavior of such models. Our results also show examples of unusual behaviors learned by models in attempting VQA tasks.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\TC3VTDB7\\Manjunatha 等。 - Explicit Bias Discovery in Visual Question Answeri.pdf}
}

@article{manmadhanMultiTierAttentionNetwork2021,
  title = {Multi-{{Tier Attention Network}} Using {{Term-weighted Question Features}} for {{Visual Question Answering}}},
  author = {Manmadhan, Sruthy and Kovoor, Binsu C.},
  date = {2021-11},
  journaltitle = {Image and Vision Computing},
  shortjournal = {Image and Vision Computing},
  volume = {115},
  pages = {104291},
  issn = {02628856},
  doi = {10.1016/j.imavis.2021.104291},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885621001967},
  urldate = {2022-02-10},
  abstract = {Visual Question Answering (VQA) is a multi-modal challenging task that accepts an image and a natural language question about that image as inputs and desires to find the correct answer. This AI-complete task necessitates the fine-grained joint understanding of the two input modalities. Inspired by the success of attention mechanism in the task of efficient comprehension of visual-language features for VQA, this paper proposes a Multi-Tier Attention Network (MTAN) with the major component being term-weighted question-guided visual attention. Additionally, we introduce a novel Supervised Term Weighting (STW) scheme named ‘qf.obj.cos’ to semantically weight words utilizing the notion of visual object detection. This can be generalized to other vision-language comprehension tasks like image captioning, text-to-image-retrieval, multi-modal summarization etc. In effect, the proposed system allows the generation of more discriminative visual features from the progressive steps of question guided visual attention where question embedding is indeed guided by semantic term weighting. MTAN is quantitatively and qualitatively evaluated on the benchmark DAQUAR dataset and an extensive set of ablations are studied to demonstrate the individual significance of each of the components of the system. Experimental results certify that MTAN performs better than the previous works using the same dataset.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\PJWCITRE\\Manmadhan 和 Kovoor - 2021 - Multi-Tier Attention Network using Term-weighted Q.pdf}
}

@unpublished{marasovicNaturalLanguageRationales2020,
  title = {Natural {{Language Rationales}} with {{Full-Stack Visual Reasoning}}: {{From Pixels}} to {{Semantic Frames}} to {{Commonsense Graphs}}},
  shorttitle = {Natural {{Language Rationales}} with {{Full-Stack Visual Reasoning}}},
  author = {Marasović, Ana and Bhagavatula, Chandra and Park, Jae Sung and Bras, Ronan Le and Smith, Noah A. and Choi, Yejin},
  date = {2020-10-15},
  eprint = {2010.07526},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.07526},
  urldate = {2021-09-26},
  abstract = {Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale\^VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\USPI5QUE\\Marasović et al_2020_Natural Language Rationales with Full-Stack Visual Reasoning.pdf;D\:\\Zotero\\storage\\9KUBQ9EG\\2010.html}
}

@article{marinoKRISPIntegratingImplicit,
  title = {{{KRISP}}: {{Integrating Implicit}} and {{Symbolic Knowledge}} for {{Open-Domain Knowledge-Based VQA}}},
  author = {Marino, Kenneth and Chen, Xinlei and Parikh, Devi and Gupta, Abhinav and Rohrbach, Marcus},
  pages = {11},
  langid = {english},
  file = {D\:\\Zotero\\storage\\AXTTH5EX\\Marino 等。 - KRISP Integrating Implicit and Symbolic Knowledge.pdf}
}

@inproceedings{marinoOKVQAVisualQuestion2019,
  title = {{{OK-VQA}}: {{A Visual Question Answering Benchmark Requiring External Knowledge}}},
  shorttitle = {{{OK-VQA}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  date = {2019-06},
  pages = {3190--3199},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00331},
  url = {https://ieeexplore.ieee.org/document/8953725/},
  urldate = {2021-09-09},
  abstract = {Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {21 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\VK2SGRH5\\Marino 等。 - 2019 - OK-VQA A Visual Question Answering Benchmark Requ.pdf}
}

@unpublished{mathewInfographicVQA2021,
  title = {{{InfographicVQA}}},
  author = {Mathew, Minesh and Bagal, Viraj and Tito, Rubèn Pérez and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, C. V.},
  date = {2021-08-22},
  eprint = {2104.12756},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.12756},
  urldate = {2022-03-24},
  abstract = {Infographics are documents designed to effectively communicate information using a combination of textual, graphical and visual elements. In this work, we explore the automatic understanding of infographic images by using Visual Question Answering technique.To this end, we present InfographicVQA, a new dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills. Finally, we evaluate two strong baselines based on state of the art multi-modal VQA models, and establish baseline performance for the new task. The dataset, code and leaderboard will be made available at http://docvqa.org},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\4XTZY6QJ\\Mathew 等。 - 2021 - InfographicVQA.pdf}
}

@article{mavadatiDISFASpontaneousFacial2013,
  title = {{{DISFA}}: {{A Spontaneous Facial Action Intensity Database}}},
  shorttitle = {{{DISFA}}},
  author = {Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey F.},
  date = {2013-04},
  journaltitle = {IEEE Transactions on Affective Computing},
  shortjournal = {IEEE Trans. Affective Comput.},
  volume = {4},
  number = {2},
  pages = {151--160},
  issn = {1949-3045},
  doi = {10.1109/T-AFFC.2013.4},
  url = {http://ieeexplore.ieee.org/document/6475933/},
  urldate = {2022-09-23},
  abstract = {Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions [1], publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system [2]. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4G3Q4RKK\\Mavadati 等。 - 2013 - DISFA A Spontaneous Facial Action Intensity Datab.pdf}
}

@inproceedings{maVisualQuestionAnswering2018,
  title = {Visual {{Question Answering}} with {{Memory-Augmented Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ma, Chao and Shen, Chunhua and Dick, Anthony and Wu, Qi and Wang, Peng and van den Hengel, Anton and Reid, Ian},
  date = {2018-06},
  pages = {6975--6984},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00729},
  url = {https://ieeexplore.ieee.org/document/8578827/},
  urldate = {2021-09-09},
  abstract = {In this paper, we exploit memory-augmented neural networks to predict accurate answers to visual questions, even when those answers rarely occur in the training set. The memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar. We show that memory-augmented neural networks are able to maintain a relatively long-term memory of scarce training exemplars, which is important for visual question answering due to the heavy-tailed distribution of answers in a general VQA setting. Experimental results in two large-scale benchmark datasets show the favorable performance of the proposed algorithm with the comparison to state of the art.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {23 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\NKG7SYR6\\Ma 等。 - 2018 - Visual Question Answering with Memory-Augmented Ne.pdf}
}

@online{MCGNJUNanJingDaXueMeiTiJiSuanYanJiuZu,
  title = {{{MCG}}@{{NJU}} 南京大学媒体计算研究组},
  url = {http://mcg.nju.edu.cn/},
  urldate = {2022-05-08},
  file = {D\:\\Zotero\\storage\\LZVL7JLN\\mcg.nju.edu.cn.html}
}

@inproceedings{mcintoshVisualTextualCapsuleRouting2020,
  title = {Visual-{{Textual Capsule Routing}} for {{Text-Based Video Segmentation}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {McIntosh, Bruce and Duarte, Kevin and Rawat, Yogesh S and Shah, Mubarak},
  date = {2020-06},
  pages = {9939--9948},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00996},
  url = {https://ieeexplore.ieee.org/document/9156482/},
  urldate = {2021-10-08},
  abstract = {Joint understanding of vision and natural language is a challenging problem with a wide range of applications in artificial intelligence. In this work, we focus on integration of video and text for the task of actor and action video segmentation from a sentence. We propose a capsule-based approach which performs pixel-level localization based on a natural language query describing the actor of interest. We encode both the video and textual input in the form of capsules, which provide a more effective representation in comparison with standard convolution based features. Our novel visual-textual routing mechanism allows for the fusion of video and text capsules to successfully localize the actor and action. The existing works on actor-action localization are mainly focused on localization in a single frame instead of the full video. Different from existing works, we propose to perform the localization on all frames of the video. To validate the potential of the proposed network for actor and action video localization, we extend an existing actor-action dataset (A2D) with annotations for all the frames. The experimental evaluation demonstrates the effectiveness of our capsule network for text selective actor and action localization in videos. The proposed method also improves upon the performance of the existing stateof-the art works on single frame-based localization.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@inproceedings{mcintoshVisualTextualCapsuleRouting2020a,
  title = {Visual-{{Textual Capsule Routing}} for {{Text-Based Video Segmentation}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {McIntosh, Bruce and Duarte, Kevin and Rawat, Yogesh S and Shah, Mubarak},
  date = {2020-06},
  pages = {9939--9948},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00996},
  url = {https://ieeexplore.ieee.org/document/9156482/},
  urldate = {2021-10-08},
  abstract = {Joint understanding of vision and natural language is a challenging problem with a wide range of applications in artificial intelligence. In this work, we focus on integration of video and text for the task of actor and action video segmentation from a sentence. We propose a capsule-based approach which performs pixel-level localization based on a natural language query describing the actor of interest. We encode both the video and textual input in the form of capsules, which provide a more effective representation in comparison with standard convolution based features. Our novel visual-textual routing mechanism allows for the fusion of video and text capsules to successfully localize the actor and action. The existing works on actor-action localization are mainly focused on localization in a single frame instead of the full video. Different from existing works, we propose to perform the localization on all frames of the video. To validate the potential of the proposed network for actor and action video localization, we extend an existing actor-action dataset (A2D) with annotations for all the frames. The experimental evaluation demonstrates the effectiveness of our capsule network for text selective actor and action localization in videos. The proposed method also improves upon the performance of the existing stateof-the art works on single frame-based localization.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@inproceedings{mcmahanCommunicationEfficientLearningDeep2017,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  date = {2017-04-10},
  pages = {1273--1282},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v54/mcmahan17a.html},
  urldate = {2022-01-19},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2G9T6JFZ\\McMahan et al_2017_Communication-Efficient Learning of Deep Networks from Decentralized Data.pdf;D\:\\Zotero\\storage\\XJYPFUW2\\McMahan 等。 - 2017 - Communication-Efficient Learning of Deep Networks .pdf}
}

@misc{merulloLinearlyMappingImage2022,
  title = {Linearly {{Mapping}} from {{Image}} to {{Text Space}}},
  author = {Merullo, Jack and Castricato, Louis and Eickhoff, Carsten and Pavlick, Ellie},
  date = {2022-09-29},
  number = {arXiv:2209.15162},
  eprint = {2209.15162},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.15162},
  urldate = {2022-10-11},
  abstract = {The extent to which text-only language models (LMs) learn to represent the physical, non-linguistic world is an open question. Prior work has shown that pretrained LMs can be taught to ``understand'' visual inputs when the models' parameters are updated on image captioning tasks. We test a stronger hypothesis: that the conceptual representations learned by text-only models are functionally equivalent (up to a linear transformation) to those learned by models trained on vision tasks. Specifically, we show that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection. Using these to prompt the LM achieves competitive performance on captioning and visual question answering tasks compared to models that tune both the image encoder and text decoder (such as the MAGMA model). We compare three image encoders with increasing amounts of linguistic supervision seen during pretraining: BEIT (no linguistic information), NF-ResNET (lexical category information), and CLIP (full natural language descriptions). We find that all three encoders perform equally well at transferring visual property information to the language model (e.g., whether an animal is large or small), but that image encoders pretrained with linguistic supervision more saliently encode category information (e.g., distinguishing hippo vs.\textbackslash{} elephant) and thus perform significantly better on benchmark language-and-vision tasks. Our results indicate that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\2GEN8JI7\\Merullo et al_2022_Linearly Mapping from Image to Text Space.pdf;D\:\\Zotero\\storage\\FS7ECDZC\\2209.html}
}

@unpublished{mialonGraphiTEncodingGraph2021,
  title = {{{GraphiT}}: {{Encoding Graph Structure}} in {{Transformers}}},
  shorttitle = {{{GraphiT}}},
  author = {Mialon, Grégoire and Chen, Dexiong and Selosse, Margot and Mairal, Julien},
  date = {2021-06-10},
  eprint = {2106.05667},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.05667},
  urldate = {2022-03-26},
  abstract = {We show that viewing graphs as sets of node features and incorporating structural and positional information into a transformer architecture is able to outperform representations learned with classical graph neural networks (GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative positional encoding strategies in self-attention scores based on positive definite kernels on graphs, and (ii) enumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate these two ideas on many classification and regression tasks, demonstrating the effectiveness of each of them independently, as well as their combination. In addition to performing well on standard benchmarks, our model also admits natural visualization mechanisms for interpreting graph motifs explaining the predictions, making it a potentially strong candidate for scientific applications where interpretation is important. Code available at https://github.com/inria-thoth/GraphiT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\2VCGXCHG\\Mialon et al_2021_GraphiT.pdf;D\:\\Zotero\\storage\\I4X28F7T\\2106.html}
}

@unpublished{mialonGraphiTEncodingGraph2021a,
  title = {{{GraphiT}}: {{Encoding Graph Structure}} in {{Transformers}}},
  shorttitle = {{{GraphiT}}},
  author = {Mialon, Grégoire and Chen, Dexiong and Selosse, Margot and Mairal, Julien},
  date = {2021-06-10},
  eprint = {2106.05667},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.05667},
  urldate = {2022-03-26},
  abstract = {We show that viewing graphs as sets of node features and incorporating structural and positional information into a transformer architecture is able to outperform representations learned with classical graph neural networks (GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative positional encoding strategies in self-attention scores based on positive definite kernels on graphs, and (ii) enumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate these two ideas on many classification and regression tasks, demonstrating the effectiveness of each of them independently, as well as their combination. In addition to performing well on standard benchmarks, our model also admits natural visualization mechanisms for interpreting graph motifs explaining the predictions, making it a potentially strong candidate for scientific applications where interpretation is important. Code available at https://github.com/inria-thoth/GraphiT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\5ZSNNF73\\Mialon et al_2021_GraphiT.pdf;D\:\\Zotero\\storage\\S3MSJZ8A\\2106.html}
}

@inproceedings{miHierarchicalGraphAttention2020,
  title = {Hierarchical {{Graph Attention Network}} for {{Visual Relationship Detection}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mi, Li and Chen, Zhenzhong},
  date = {2020-06},
  pages = {13883--13892},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01390},
  url = {https://ieeexplore.ieee.org/document/9156392/},
  urldate = {2022-03-15},
  abstract = {Visual Relationship Detection (VRD) aims to describe the relationship between two objects by providing a structural triplet shown as {$<$}subject-predicate-object{$>$}. Existing graph-based methods mainly represent the relationships by an object-level graph, which ignores to model the tripletlevel dependencies. In this work, a Hierarchical Graph Attention Network (HGAT) is proposed to capture the dependencies on both object-level and triplet-level. Objectlevel graph aims to capture the interactions between objects, while the triplet-level graph models the dependencies among relation triplets. In addition, prior knowledge and attention mechanism are introduced to fix the redundant or missing edges on graphs that are constructed according to spatial correlation. With these approaches, nodes are allowed to attend over their spatial and semantic neighborhoods’ features based on the visual or semantic feature correlation. Experimental results on the well-known VG and VRD datasets demonstrate that our model significantly outperforms the state-of-the-art methods.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@inproceedings{miHierarchicalGraphAttention2020a,
  title = {Hierarchical {{Graph Attention Network}} for {{Visual Relationship Detection}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mi, Li and Chen, Zhenzhong},
  date = {2020-06},
  pages = {13883--13892},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01390},
  url = {https://ieeexplore.ieee.org/document/9156392/},
  urldate = {2021-10-11},
  abstract = {Visual Relationship Detection (VRD) aims to describe the relationship between two objects by providing a structural triplet shown as {$<$}subject-predicate-object{$>$}. Existing graph-based methods mainly represent the relationships by an object-level graph, which ignores to model the tripletlevel dependencies. In this work, a Hierarchical Graph Attention Network (HGAT) is proposed to capture the dependencies on both object-level and triplet-level. Objectlevel graph aims to capture the interactions between objects, while the triplet-level graph models the dependencies among relation triplets. In addition, prior knowledge and attention mechanism are introduced to fix the redundant or missing edges on graphs that are constructed according to spatial correlation. With these approaches, nodes are allowed to attend over their spatial and semantic neighborhoods’ features based on the visual or semantic feature correlation. Experimental results on the well-known VG and VRD datasets demonstrate that our model significantly outperforms the state-of-the-art methods.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@inproceedings{miHierarchicalGraphAttention2020b,
  title = {Hierarchical {{Graph Attention Network}} for {{Visual Relationship Detection}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mi, Li and Chen, Zhenzhong},
  date = {2020-06},
  pages = {13883--13892},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01390},
  url = {https://ieeexplore.ieee.org/document/9156392/},
  urldate = {2021-10-11},
  abstract = {Visual Relationship Detection (VRD) aims to describe the relationship between two objects by providing a structural triplet shown as {$<$}subject-predicate-object{$>$}. Existing graph-based methods mainly represent the relationships by an object-level graph, which ignores to model the tripletlevel dependencies. In this work, a Hierarchical Graph Attention Network (HGAT) is proposed to capture the dependencies on both object-level and triplet-level. Objectlevel graph aims to capture the interactions between objects, while the triplet-level graph models the dependencies among relation triplets. In addition, prior knowledge and attention mechanism are introduced to fix the redundant or missing edges on graphs that are constructed according to spatial correlation. With these approaches, nodes are allowed to attend over their spatial and semantic neighborhoods’ features based on the visual or semantic feature correlation. Experimental results on the well-known VG and VRD datasets demonstrate that our model significantly outperforms the state-of-the-art methods.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@unpublished{minBuildingFoodKnowledge2021,
  title = {Towards {{Building}} a {{Food Knowledge Graph}} for {{Internet}} of {{Food}}},
  author = {Min, Weiqing and Liu, Chunlin and Jiang, Shuqiang},
  date = {2021-07-13},
  eprint = {2107.05869},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.05869},
  urldate = {2021-09-23},
  abstract = {Background: The deployment of various networks (e.g., Internet of Things (IoT) and mobile networks) and databases (e.g., nutrition tables and food compositional databases) in the food system generates massive information silos due to the well-known data harmonization problem. The food knowledge graph provides a unified and standardized conceptual terminology and their relationships in a structured form and thus can transform these information silos across the whole food system to a more reusable globally digitally connected Internet of Food, enabling every stage of the food system from farm-to-fork. Scope and approach: We review the evolution of food knowledge organization, from food classification, food ontology to food knowledge graphs. We then discuss the progress in food knowledge graphs from several representative applications. We finally discuss the main challenges and future directions. Key findings and conclusions: Our comprehensive summary of current research on food knowledge graphs shows that food knowledge graphs play an important role in food-oriented applications, including food search and Question Answering (QA), personalized dietary recommendation, food analysis and visualization, food traceability, and food machinery intelligent manufacturing. Future directions for food knowledge graphs cover several fields such as multimodal food knowledge graphs and food intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\FNEWSKVB\\Min et al_2021_Towards Building a Food Knowledge Graph for Internet of Food.pdf;D\:\\Zotero\\storage\\MBCTL8UP\\2107.html}
}

@unpublished{minMaskedTransformerNeighhourhoodaware2022,
  title = {Masked {{Transformer}} for {{Neighhourhood-aware Click-Through Rate Prediction}}},
  author = {Min, Erxue and Rong, Yu and Xu, Tingyang and Bian, Yatao and Zhao, Peilin and Huang, Junzhou and Luo, Da and Lin, Kangyi and Ananiadou, Sophia},
  date = {2022-01-25},
  eprint = {2201.13311},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.13311},
  urldate = {2022-03-26},
  abstract = {Click-Through Rate (CTR) prediction, is an essential component of online advertising. The mainstream techniques mostly focus on feature interaction or user interest modeling, which rely on users' directly interacted items. The performance of these methods are usally impeded by inactive behaviours and system's exposure, incurring that the features extracted do not contain enough information to represent all potential interests. For this sake, we propose Neighbor-Interaction based CTR prediction, which put this task into a Heterogeneous Information Network (HIN) setting, then involves local neighborhood of the target user-item pair in the HIN to predict their linkage. In order to enhance the representation of the local neighbourhood, we consider four types of topological interaction among the nodes, and propose a novel Graph-masked Transformer architecture to effectively incorporates both feature and topological information. We conduct comprehensive experiments on two real world datasets and the experimental results show that our proposed method outperforms state-of-the-art CTR models significantly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\4ISS8HET\\Min et al_2022_Masked Transformer for Neighhourhood-aware Click-Through Rate Prediction.pdf;D\:\\Zotero\\storage\\3KTBLKI9\\2201.html}
}

@unpublished{minTransformerGraphsOverview2022,
  title = {Transformer for {{Graphs}}: {{An Overview}} from {{Architecture Perspective}}},
  shorttitle = {Transformer for {{Graphs}}},
  author = {Min, Erxue and Chen, Runfa and Bian, Yatao and Xu, Tingyang and Zhao, Kangfei and Huang, Wenbing and Zhao, Peilin and Huang, Junzhou and Ananiadou, Sophia and Rong, Yu},
  date = {2022-02-17},
  eprint = {2202.08455},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.08455},
  urldate = {2022-03-26},
  abstract = {Recently, Transformer model, which has achieved great success in many artificial intelligence fields, has demonstrated its great potential in modeling graph-structured data. Till now, a great variety of Transformers has been proposed to adapt to the graph-structured data. However, a comprehensive literature review and systematical evaluation of these Transformer variants for graphs are still unavailable. It's imperative to sort out the existing Transformer models for graphs and systematically investigate their effectiveness on various graph tasks. In this survey, we provide a comprehensive review of various Graph Transformer models from the architectural design perspective. We first disassemble the existing models and conclude three typical ways to incorporate the graph information into the vanilla Transformer: 1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and 3) Improved Attention Matrix from Graphs. Furthermore, we implement the representative components in three groups and conduct a comprehensive comparison on various kinds of famous graph data benchmarks to investigate the real performance gain of each component. Our experiments confirm the benefits of current graph-specific modules on Transformer and reveal their advantages on different kinds of graph tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\KXBHTT5S\\Min et al_2022_Transformer for Graphs.pdf;D\:\\Zotero\\storage\\CYBIMFUF\\2202.html}
}

@inproceedings{mishraOCRVQAVisualQuestion2019,
  title = {{{OCR-VQA}}: {{Visual Question Answering}} by {{Reading Text}} in {{Images}}},
  shorttitle = {{{OCR-VQA}}},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  date = {2019-09},
  pages = {947--952},
  publisher = {{IEEE}},
  location = {{Sydney, Australia}},
  doi = {10.1109/ICDAR.2019.00156},
  url = {https://ieeexplore.ieee.org/document/8978122/},
  urldate = {2022-05-16},
  abstract = {The problem of answering questions about an image is popularly known as visual question answering (or VQA in short). It is a well-established problem in computer vision. However, none of the VQA methods currently utilize the text often present in the image. These “texts in images” provide additional useful cues and facilitate better understanding of the visual content. In this paper, we introduce a novel task of visual question answering by reading text in images, i.e., by optical character recognition or OCR. We refer to this problem as OCR-VQA. To facilitate a systematic way of studying this new problem, we introduce a large-scale dataset, namely OCRVQA–200K. This dataset comprises of 207,572 images of book covers and contains more than 1 million question-answer pairs about these images. We judiciously combine well-established techniques from OCR and VQA domains to present a novel baseline for OCR-VQA–200K. The experimental results and rigorous analysis demonstrate various challenges present in this dataset leaving ample scope for the future research. We are optimistic that this new task along with compiled dataset will open-up many exciting research avenues both for the document image analysis and the VQA communities.},
  eventtitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  isbn = {978-1-72813-014-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2PXXE4C7\\Mishra 等。 - 2019 - OCR-VQA Visual Question Answering by Reading Text.pdf}
}

@inproceedings{mishraOCRVQAVisualQuestion2019a,
  title = {{{OCR-VQA}}: {{Visual Question Answering}} by {{Reading Text}} in {{Images}}},
  shorttitle = {{{OCR-VQA}}},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  date = {2019-09},
  pages = {947--952},
  publisher = {{IEEE}},
  location = {{Sydney, Australia}},
  doi = {10.1109/ICDAR.2019.00156},
  url = {https://ieeexplore.ieee.org/document/8978122/},
  urldate = {2021-12-08},
  abstract = {The problem of answering questions about an image is popularly known as visual question answering (or VQA in short). It is a well-established problem in computer vision. However, none of the VQA methods currently utilize the text often present in the image. These “texts in images” provide additional useful cues and facilitate better understanding of the visual content. In this paper, we introduce a novel task of visual question answering by reading text in images, i.e., by optical character recognition or OCR. We refer to this problem as OCR-VQA. To facilitate a systematic way of studying this new problem, we introduce a large-scale dataset, namely OCRVQA–200K. This dataset comprises of 207,572 images of book covers and contains more than 1 million question-answer pairs about these images. We judiciously combine well-established techniques from OCR and VQA domains to present a novel baseline for OCR-VQA–200K. The experimental results and rigorous analysis demonstrate various challenges present in this dataset leaving ample scope for the future research. We are optimistic that this new task along with compiled dataset will open-up many exciting research avenues both for the document image analysis and the VQA communities.},
  eventtitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  isbn = {978-1-72813-014-9},
  langid = {english}
}

@inproceedings{misraLearningAskingQuestions2018,
  title = {Learning by {{Asking Questions}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Misra, Ishan and Girshick, Ross and Fergus, Rob and Hebert, Martial and Gupta, Abhinav and van der Maaten, Laurens},
  options = {useprefix=true},
  date = {2018-06},
  pages = {11--20},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00009},
  url = {https://ieeexplore.ieee.org/document/8578107/},
  urldate = {2021-09-09},
  abstract = {We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA). We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more dataefficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {7 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\L69PQHDY\\Misra 等。 - 2018 - Learning by Asking Questions.pdf}
}

@unpublished{mouselinosMeasuringCLEVRnessBlackbox2022,
  title = {Measuring {{CLEVRness}}: {{Blackbox}} Testing of {{Visual Reasoning Models}}},
  shorttitle = {Measuring {{CLEVRness}}},
  author = {Mouselinos, Spyridon and Michalewski, Henryk and Malinowski, Mateusz},
  date = {2022-02-28},
  eprint = {2202.12162},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.12162},
  urldate = {2022-05-09},
  abstract = {How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate. To answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a human level, can easily be fooled by our agent. Our results put in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\YTPIJ5J3\\Mouselinos et al_2022_Measuring CLEVRness.pdf;D\:\\Zotero\\storage\\4STRHHEN\\2202.html}
}

@inproceedings{muhammadFedFastGoingAverage2020,
  title = {{{FedFast}}: {{Going Beyond Average}} for {{Faster Training}} of {{Federated Recommender Systems}}},
  shorttitle = {{{FedFast}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Muhammad, Khalil and Wang, Qinqin and O'Reilly-Morgan, Diarmuid and Tragos, Elias and Smyth, Barry and Hurley, Neil and Geraci, James and Lawlor, Aonghus},
  date = {2020-08-23},
  pages = {1234--1242},
  publisher = {{ACM}},
  location = {{Virtual Event CA USA}},
  doi = {10.1145/3394486.3403176},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403176},
  urldate = {2022-01-19},
  abstract = {Federated learning (FL) is quickly becoming the de facto standard for the distributed training of deep recommendation models, using on-device user data and reducing server costs. In a typical FL process, a central server tasks end-users to train a shared recommendation model using their local data. The local models are trained over several rounds on the users’ devices and the server combines them into a global model, which is sent to the devices for the purpose of providing recommendations. Standard FL approaches use randomly selected users for training at each round, and simply average their local models to compute the global model. The resulting federated recommendation models require significant client effort to train and many communication rounds before they converge to a satisfactory accuracy. Users are left with poor quality recommendations until the late stages of training. We present a novel technique, FedFast, to accelerate distributed learning which achieves good accuracy for all users very early in the training process. We achieve this by sampling from a diverse set of participating clients in each training round and applying an active aggregation method that propagates the updated model to the other clients. Consequently, with FedFast the users benefit from far lower communication costs and more accurate models that can be consumed anytime during the training process even at the very early stages. We demonstrate the efficacy of our approach across a variety of benchmark datasets and in comparison to state-of-the-art recommendation techniques.},
  eventtitle = {{{KDD}} '20: {{The}} 26th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-7998-4},
  langid = {english},
  file = {D\:\\Zotero\\storage\\GZ6YRITC\\Muhammad 等。 - 2020 - FedFast Going Beyond Average for Faster Training .pdf}
}

@inproceedings{munLearningSpecializeKnowledge2018,
  title = {Learning to {{Specialize}} with {{Knowledge Distillation}} for {{Visual Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mun, Jonghwan and Lee, Kimin and Shin, Jinwoo and Han, Bohyung},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2018/hash/0f2818101a7ac4b96ceeba38de4b934c-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Visual Question Answering (VQA) is a notoriously challenging problem because it involves various heterogeneous tasks defined by questions within a unified framework. Learning specialized models for individual types of tasks is intuitively attracting but surprisingly difficult; it is not straightforward to outperform naive independent ensemble approach. We present a principled algorithm to learn specialized models with knowledge distillation under a multiple choice learning (MCL) framework, where training examples are assigned dynamically to a subset of models for updating network parameters. The assigned and non-assigned models are learned to predict ground-truth answers and imitate their own base models before specialization, respectively. Our approach alleviates the limitation of data deficiency in existing MCL frameworks, and allows each model to learn its own specialized expertise without forgetting general knowledge. The proposed framework is model-agnostic and applicable to any tasks other than VQA, e.g., image classification with a large number of labels but few per-class examples, which is known to be difficult under existing MCL schemes. Our experimental results indeed demonstrate that our method outperforms other baselines for VQA and image classification.},
  file = {D\:\\Zotero\\storage\\M3BB8GSU\\Mun et al_2018_Learning to Specialize with Knowledge Distillation for Visual Question Answering.pdf}
}

@article{munMarioQAAnsweringQuestions,
  title = {{{MarioQA}}: {{Answering Questions}} by {{Watching Gameplay Videos}}},
  author = {Mun, Jonghwan and Seo, Paul Hongsuck and Jung, Ilchae and Han, Bohyung},
  pages = {9},
  abstract = {We present a framework to analyze various aspects of models for video question answering (VideoQA) using customizable synthetic datasets, which are constructed automatically from gameplay videos. Our work is motivated by the fact that existing models are often tested only on datasets that require excessively high-level reasoning or mostly contain instances accessible through single frame inferences. Hence, it is difficult to measure capacity and flexibility of trained models, and existing techniques often rely on adhoc implementations of deep neural networks without clear insight into datasets and models. We are particularly interested in understanding temporal relationships between video events to solve VideoQA problems; this is because reasoning temporal dependency is one of the most distinct components in videos from images. To address this objective, we automatically generate a customized synthetic VideoQA dataset using Super Mario Bros. gameplay videos so that it contains events with different levels of reasoning complexity. Using the dataset, we show that properly constructed datasets with events in various complexity levels are critical to learn effective models and improve overall performance.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\H6FBEAZL\\Mun 等。 - MarioQA Answering Questions by Watching Gameplay .pdf}
}

@article{munMarioQAAnsweringQuestionsa,
  title = {{{MarioQA}}: {{Answering Questions}} by {{Watching Gameplay Videos}}},
  author = {Mun, Jonghwan and Seo, Paul Hongsuck and Jung, Ilchae and Han, Bohyung},
  pages = {9},
  abstract = {We present a framework to analyze various aspects of models for video question answering (VideoQA) using customizable synthetic datasets, which are constructed automatically from gameplay videos. Our work is motivated by the fact that existing models are often tested only on datasets that require excessively high-level reasoning or mostly contain instances accessible through single frame inferences. Hence, it is difficult to measure capacity and flexibility of trained models, and existing techniques often rely on adhoc implementations of deep neural networks without clear insight into datasets and models. We are particularly interested in understanding temporal relationships between video events to solve VideoQA problems; this is because reasoning temporal dependency is one of the most distinct components in videos from images. To address this objective, we automatically generate a customized synthetic VideoQA dataset using Super Mario Bros. gameplay videos so that it contains events with different levels of reasoning complexity. Using the dataset, we show that properly constructed datasets with events in various complexity levels are critical to learn effective models and improve overall performance.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\K3A8KIJY\\Mun 等。 - MarioQA Answering Questions by Watching Gameplay .pdf}
}

@unpublished{murahariLargescalePretrainingVisual2020,
  title = {Large-Scale {{Pretraining}} for {{Visual Dialog}}: {{A Simple State-of-the-Art Baseline}}},
  shorttitle = {Large-Scale {{Pretraining}} for {{Visual Dialog}}},
  author = {Murahari, Vishvak and Batra, Dhruv and Parikh, Devi and Das, Abhishek},
  date = {2020-03-30},
  eprint = {1912.02379},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.02379},
  urldate = {2021-12-08},
  abstract = {Prior work in visual dialog has focused on training deep neural models on VisDial in isolation. Instead, we present an approach to leverage pretraining on related vision-language datasets before transferring to visual dialog. We adapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn visually-grounded conversations. Our model is pretrained on the Conceptual Captions and Visual Question Answering datasets, and finetuned on VisDial. Our best single model outperforms prior published work (including model ensembles) by more than 1\% absolute on NDCG and MRR. Next, we find that additional finetuning using "dense" annotations in VisDial leads to even higher NDCG -- more than 10\% over our base model -- but hurts MRR -- more than 17\% below our base model! This highlights a trade-off between the two primary metrics -- NDCG and MRR -- which we find is due to dense annotations not correlating well with the original ground-truth answers to questions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\AA9RJKC9\\Murahari et al_2020_Large-scale Pretraining for Visual Dialog.pdf;D\:\\Zotero\\storage\\NWXZCSHK\\1912.html}
}

@inproceedings{narasimhanOutBoxReasoning2018,
  title = {Out of the {{Box}}: {{Reasoning}} with {{Graph Convolution Nets}} for {{Factual Visual Question Answering}}},
  shorttitle = {Out of the {{Box}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Narasimhan, Medhini and Lazebnik, Svetlana and Schwing, Alexander},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2018/hash/c26820b8a4c1b3c2aa868d6d57e14a79-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Accurately answering a question about a given image requires combining observations with general knowledge. While this is effortless for humans, reasoning with general knowledge remains an algorithmic challenge. To advance research in this direction a novel fact-based' visual question answering (FVQA) task has been introduced recently along with a large set of curated facts which link two entities, i.e., two possible answers, via a relation. Given a question-image pair, deep network techniques have been employed to successively reduce the large set of facts until one of the two entities of the final remaining fact is predicted as the answer. We observe that a successive process which considers one fact at a time to form a local decision is sub-optimal. Instead, we develop an entity graph and use a graph convolutional network toreason' about the correct answer by jointly considering all entities. We show on the challenging FVQA dataset that this leads to an improvement in accuracy of around 7\% compared to the state-of-the-art.},
  file = {D\:\\Zotero\\storage\\TL96EBR8\\Narasimhan et al_2018_Out of the Box.pdf}
}

@incollection{narasimhanStraightFactsLearning2018,
  title = {Straight to the {{Facts}}: {{Learning Knowledge Base Retrieval}} for {{Factual Visual Question Answering}}},
  shorttitle = {Straight to the {{Facts}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Narasimhan, Medhini and Schwing, Alexander G.},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11212},
  pages = {460--477},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01237-3_28},
  url = {http://link.springer.com/10.1007/978-3-030-01237-3_28},
  urldate = {2021-09-10},
  abstract = {Question answering is an important task for autonomous agents and virtual assistants alike and was shown to support the disabled in efficiently navigating an overwhelming environment. Many existing methods focus on observation-based questions, ignoring our ability to seamlessly combine observed content with general knowledge. To understand interactions with a knowledge base, a dataset has been introduced recently and keyword matching techniques were shown to yield compelling results despite being vulnerable to misconceptions due to synonyms and homographs. To address this issue, we develop a learningbased approach which goes straight to the facts via a learned embedding space. We demonstrate state-of-the-art results on the challenging recently introduced fact-based visual question answering dataset, outperforming competing methods by more than 5\%.},
  isbn = {978-3-030-01236-6 978-3-030-01237-3},
  langid = {english},
  keywords = {Knowledge,Retrieval},
  file = {D\:\\Zotero\\storage\\WMWL5DPI\\Narasimhan 和 Schwing - 2018 - Straight to the Facts Learning Knowledge Base Ret.pdf}
}

@inproceedings{nguyenCapsuleNetworkbasedEmbedding2019,
  title = {A {{Capsule Network-based Embedding Model}} for {{Knowledge Graph Completion}} and {{Search Personalization}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Nguyen, Dai Quoc and Vu, Thanh and Nguyen, Tu Dinh and Nguyen, Dat Quoc and Phung, Dinh},
  date = {2019-06},
  pages = {2180--2189},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1226},
  url = {https://aclanthology.org/N19-1226},
  urldate = {2021-10-10},
  abstract = {In this paper, we introduce an embedding model, named CapsE, exploring a capsule network to model relationship triples (subject, relation, object). Our CapsE represents each triple as a 3-column matrix where each column vector represents the embedding of an element in the triple. This 3-column matrix is then fed to a convolution layer where multiple filters are operated to generate different feature maps. These feature maps are reconstructed into corresponding capsules which are then routed to another capsule to produce a continuous vector. The length of this vector is used to measure the plausibility score of the triple. Our proposed CapsE obtains better performance than previous state-of-the-art embedding models for knowledge graph completion on two benchmark datasets WN18RR and FB15k-237, and outperforms strong search personalization baselines on SEARCH17.},
  eventtitle = {{{NAACL-HLT}} 2019},
  annotation = {45 citations (Crossref) [2021-10-16]},
  file = {D\:\\Zotero\\storage\\SY522WKQ\\Nguyen et al_2019_A Capsule Network-based Embedding Model for Knowledge Graph Completion and.pdf}
}

@unpublished{nguyenCoarsetoFineReasoningVisual2021,
  title = {Coarse-to-{{Fine Reasoning}} for {{Visual Question Answering}}},
  author = {Nguyen, Binh X. and Do, Tuong and Tran, Huy and Tjiputra, Erman and Tran, Quang D. and Nguyen, Anh},
  date = {2021-10-06},
  eprint = {2110.02526},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.02526},
  urldate = {2021-10-26},
  abstract = {Bridging the semantic gap between image and question is an important step to improve the accuracy of the Visual Question Answering (VQA) task. However, most of the existing VQA methods focus on attention mechanisms or visual relations for reasoning the answer, while the features at different semantic levels are not fully utilized. In this paper, we present a new reasoning framework to fill the gap between visual features and semantic clues in the VQA task. Our method first extracts the features and predicates from the image and question. We then propose a new reasoning framework to effectively jointly learn these features and predicates in a coarse-to-fine manner. The intensively experimental results on three large-scale VQA datasets show that our proposed approach achieves superior accuracy comparing with other state-of-the-art methods. Furthermore, our reasoning framework also provides an explainable way to understand the decision of the deep neural network when predicting the answer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\BQSZ39R4\\Nguyen et al_2021_Coarse-to-Fine Reasoning for Visual Question Answering.pdf;D\:\\Zotero\\storage\\SGXJ4ILU\\2110.html}
}

@unpublished{nguyenCoarsetoFineReasoningVisual2021a,
  title = {Coarse-to-{{Fine Reasoning}} for {{Visual Question Answering}}},
  author = {Nguyen, Binh X. and Do, Tuong and Tran, Huy and Tjiputra, Erman and Tran, Quang D. and Nguyen, Anh},
  date = {2021-10-06},
  eprint = {2110.02526},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.02526},
  urldate = {2021-10-18},
  abstract = {Bridging the semantic gap between image and question is an important step to improve the accuracy of the Visual Question Answering (VQA) task. However, most of the existing VQA methods focus on attention mechanisms or visual relations for reasoning the answer, while the features at different semantic levels are not fully utilized. In this paper, we present a new reasoning framework to fill the gap between visual features and semantic clues in the VQA task. Our method first extracts the features and predicates from the image and question. We then propose a new reasoning framework to effectively jointly learn these features and predicates in a coarse-to-fine manner. The intensively experimental results on three large-scale VQA datasets show that our proposed approach achieves superior accuracy comparing with other state-of-the-art methods. Furthermore, our reasoning framework also provides an explainable way to understand the decision of the deep neural network when predicting the answer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\QGYZQYXE\\Nguyen et al_2021_Coarse-to-Fine Reasoning for Visual Question Answering.pdf;D\:\\Zotero\\storage\\L7WQGAN7\\2110.html}
}

@inproceedings{nguyenDictionaryguidedSceneText2021,
  title = {Dictionary-Guided {{Scene Text Recognition}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nguyen, Nguyen and Nguyen, Thu and Tran, Vinh and Tran, Minh-Triet and Ngo, Thanh Duc and Huu Nguyen, Thien and Hoai, Minh},
  date = {2021-06},
  pages = {7379--7388},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00730},
  url = {https://ieeexplore.ieee.org/document/9577624/},
  urldate = {2022-04-02},
  abstract = {Language prior plays an important role in the way humans detect and recognize text in the wild. Current scene text recognition methods do use lexicons to improve recognition performance, but their naive approach of casting the output into a dictionary word based purely on the edit distance has many limitations. In this paper, we present a novel approach to incorporate a dictionary in both the training and inference stage of a scene text recognition system. We use the dictionary to generate a list of possible outcomes and find the one that is most compatible with the visual appearance of the text. The proposed method leads to a robust scene text recognition model, which is better at handling ambiguous cases encountered in the wild, and improves the overall performance of state-of-the-art scene text spotting frameworks. Our work suggests that incorporating language prior is a potential approach to advance scene text detection and recognition methods. Besides, we contribute VinText, a challenging scene text dataset for Vietnamese, where some characters are equivocal in the visual form due to accent symbols. This dataset will serve as a challenging benchmark for measuring the applicability and robustness of scene text detection and recognition algorithms. Code and dataset are available at https: //github.com/VinAIResearch/dict-guided.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\DK6U7RJ7\\Nguyen 等。 - 2021 - Dictionary-guided Scene Text Recognition.pdf}
}

@unpublished{nguyenEfficientAttentionMechanism2020,
  title = {Efficient {{Attention Mechanism}} for {{Visual Dialog}} That Can {{Handle All}} the {{Interactions}} between {{Multiple Inputs}}},
  author = {Nguyen, Van-Quang and Suganuma, Masanori and Okatani, Takayuki},
  date = {2020-07-17},
  eprint = {1911.11390},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.11390},
  urldate = {2021-12-08},
  abstract = {It has been a primary concern in recent studies of vision and language tasks to design an effective attention mechanism dealing with interactions between the two modalities. The Transformer has recently been extended and applied to several bi-modal tasks, yielding promising results. For visual dialog, it becomes necessary to consider interactions between three or more inputs, i.e., an image, a question, and a dialog history, or even its individual dialog components. In this paper, we present a neural architecture named Light-weight Transformer for Many Inputs (LTMI) that can efficiently deal with all the interactions between multiple such inputs in visual dialog. It has a block structure similar to the Transformer and employs the same design of attention computation, whereas it has only a small number of parameters, yet has sufficient representational power for the purpose. Assuming a standard setting of visual dialog, a layer built upon the proposed attention block has less than one-tenth of parameters as compared with its counterpart, a natural Transformer extension. The experimental results on the VisDial datasets validate the effectiveness of the proposed approach, showing improvements of the best NDCG score on the VisDial v1.0 dataset from 57.59 to 60.92 with a single model, from 64.47 to 66.53 with ensemble models, and even to 74.88 with additional finetuning. Our implementation code is available at https://github.com/davidnvq/visdial.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\9T9UPMP3\\Nguyen et al_2020_Efficient Attention Mechanism for Visual Dialog that can Handle All the.pdf;D\:\\Zotero\\storage\\U4HBH6TH\\1911.html}
}

@inproceedings{nguyenImprovedFusionVisual2018,
  title = {Improved {{Fusion}} of {{Visual}} and {{Language Representations}} by {{Dense Symmetric Co-attention}} for {{Visual Question Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Nguyen, Duy-Kien and Okatani, Takayuki},
  date = {2018-06},
  pages = {6087--6096},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00637},
  url = {https://ieeexplore.ieee.org/document/8578735/},
  urldate = {2021-09-09},
  abstract = {A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {77 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\2DTTJYZK\\Nguyen 和 Okatani - 2018 - Improved Fusion of Visual and Language Representat.pdf}
}

@article{niuCounterfactualVQACauseEffect,
  title = {Counterfactual {{VQA}}: {{A Cause-Effect Look}} at {{Language Bias}}},
  author = {Niu, Yulei and Tang, Kaihua and Zhang, Hanwang and Lu, Zhiwu and Hua, Xian-Sheng and Wen, Ji-Rong},
  pages = {11},
  abstract = {VQA models may tend to rely on language bias as a shortcut and thus fail to sufficiently learn the multi-modal knowledge from both vision and language. Recent debiasing methods proposed to exclude the language prior during inference. However, they fail to disentangle the “good” language context and “bad” language bias from the whole. In this paper, we investigate how to mitigate language bias in VQA. Motivated by causal effects, we proposed a novel counterfactual inference framework, which enables us to capture the language bias as the direct causal effect of questions on answers and reduce the language bias by subtracting the direct language effect from the total causal effect. Experiments demonstrate that our proposed counterfactual inference framework 1) is general to various VQA backbones and fusion strategies, 2) achieves competitive performance on the language-bias sensitive VQA-CP dataset while performs robustly on the balanced VQA v2 dataset without any augmented data. The code is available at https://github.com/yuleiniu/cfvqa.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\L3SEN435\\Niu 等。 - Counterfactual VQA A Cause-Effect Look at Languag.pdf}
}

@inproceedings{niuCounterfactualVQACauseEffect2021,
  title = {Counterfactual {{VQA}}: {{A Cause-Effect Look}} at {{Language Bias}}},
  shorttitle = {Counterfactual {{VQA}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Niu, Yulei and Tang, Kaihua and Zhang, Hanwang and Lu, Zhiwu and Hua, Xian-Sheng and Wen, Ji-Rong},
  date = {2021-06},
  pages = {12695--12705},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01251},
  url = {https://ieeexplore.ieee.org/document/9578738/},
  urldate = {2022-03-23},
  abstract = {VQA models may tend to rely on language bias as a shortcut and thus fail to sufficiently learn the multi-modal knowledge from both vision and language. Recent debiasing methods proposed to exclude the language prior during inference. However, they fail to disentangle the “good” language context and “bad” language bias from the whole. In this paper, we investigate how to mitigate language bias in VQA. Motivated by causal effects, we proposed a novel counterfactual inference framework, which enables us to capture the language bias as the direct causal effect of questions on answers and reduce the language bias by subtracting the direct language effect from the total causal effect. Experiments demonstrate that our proposed counterfactual inference framework 1) is general to various VQA backbones and fusion strategies, 2) achieves competitive performance on the language-bias sensitive VQA-CP dataset while performs robustly on the balanced VQA v2 dataset without any augmented data. The code is available at https://github.com/yuleiniu/cfvqa.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\C66FA664\\[GT]2021_AAAI_A Generalization of Transformer Networks to Graphs.pdf;D\:\\Zotero\\storage\\V3JYLTLJ\\Niu 等。 - 2021 - Counterfactual VQA A Cause-Effect Look at Languag.pdf}
}

@inproceedings{niuCounterfactualVQACauseEffect2021a,
  title = {Counterfactual {{VQA}}: {{A Cause-Effect Look}} at {{Language Bias}}},
  shorttitle = {Counterfactual {{VQA}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Niu, Yulei and Tang, Kaihua and Zhang, Hanwang and Lu, Zhiwu and Hua, Xian-Sheng and Wen, Ji-Rong},
  date = {2021-06},
  pages = {12695--12705},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01251},
  url = {https://ieeexplore.ieee.org/document/9578738/},
  urldate = {2022-03-23},
  abstract = {VQA models may tend to rely on language bias as a shortcut and thus fail to sufficiently learn the multi-modal knowledge from both vision and language. Recent debiasing methods proposed to exclude the language prior during inference. However, they fail to disentangle the “good” language context and “bad” language bias from the whole. In this paper, we investigate how to mitigate language bias in VQA. Motivated by causal effects, we proposed a novel counterfactual inference framework, which enables us to capture the language bias as the direct causal effect of questions on answers and reduce the language bias by subtracting the direct language effect from the total causal effect. Experiments demonstrate that our proposed counterfactual inference framework 1) is general to various VQA backbones and fusion strategies, 2) achieves competitive performance on the language-bias sensitive VQA-CP dataset while performs robustly on the balanced VQA v2 dataset without any augmented data. The code is available at https://github.com/yuleiniu/cfvqa.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\6VSJRZYH\\Niu 等。 - 2021 - Counterfactual VQA A Cause-Effect Look at Languag.pdf}
}

@unpublished{niuRecursiveVisualAttention2019,
  title = {Recursive {{Visual Attention}} in {{Visual Dialog}}},
  author = {Niu, Yulei and Zhang, Hanwang and Zhang, Manli and Zhang, Jianhong and Lu, Zhiwu and Wen, Ji-Rong},
  date = {2019-04-06},
  eprint = {1812.02664},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1812.02664},
  urldate = {2021-12-08},
  abstract = {Visual dialog is a challenging vision-language task, which requires the agent to answer multi-round questions about an image. It typically needs to address two major problems: (1) How to answer visually-grounded questions, which is the core challenge in visual question answering (VQA); (2) How to infer the co-reference between questions and the dialog history. An example of visual co-reference is: pronouns (\textbackslash eg, ``they'') in the question (\textbackslash eg, ``Are they on or off?'') are linked with nouns (\textbackslash eg, ``lamps'') appearing in the dialog history (\textbackslash eg, ``How many lamps are there?'') and the object grounded in the image. In this work, to resolve the visual co-reference for visual dialog, we propose a novel attention mechanism called Recursive Visual Attention (RvA). Specifically, our dialog agent browses the dialog history until the agent has sufficient confidence in the visual co-reference resolution, and refines the visual attention recursively. The quantitative and qualitative experimental results on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the proposed RvA not only outperforms the state-of-the-art methods, but also achieves reasonable recursion and interpretable attention maps without additional annotations. The code is available at \textbackslash url\{https://github.com/yuleiniu/rva\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\HKFYSZCT\\Niu et al_2019_Recursive Visual Attention in Visual Dialog.pdf;D\:\\Zotero\\storage\\WETFC79P\\1812.html}
}

@unpublished{nohTransferLearningUnsupervised2019,
  title = {Transfer {{Learning}} via {{Unsupervised Task Discovery}} for {{Visual Question Answering}}},
  author = {Noh, Hyeonwoo and Kim, Taehoon and Mun, Jonghwan and Han, Bohyung},
  date = {2019-04-07},
  eprint = {1810.02358},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.02358},
  urldate = {2021-09-09},
  abstract = {We study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering task. Existing large-scale visual datasets with annotations such as image class labels, bounding boxes and region descriptions are good sources for learning rich and diverse visual concepts. However, it is not straightforward how the visual concepts can be captured and transferred to visual question answering models due to missing link between question dependent answering models and visual data without question. We tackle this problem in two steps: 1) learning a task conditional visual classifier, which is capable of solving diverse question-specific visual recognition tasks, based on unsupervised task discovery and 2) transferring the task conditional visual classifier to visual question answering models. Specifically, we employ linguistic knowledge sources such as structured lexical database (e.g. WordNet) and visual descriptions for unsupervised task discovery, and transfer a learned task conditional visual classifier as an answering unit in a visual question answering model. We empirically show that the proposed algorithm generalizes to out-of-vocabulary answers successfully using the knowledge transferred from the visual dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,Transfer Learning},
  file = {D\:\\Zotero\\storage\\VC8WP9V4\\Noh et al_2019_Transfer Learning via Unsupervised Task Discovery for Visual Question Answering.pdf}
}

@inproceedings{norcliffe-brownLearningConditionedGraph2018,
  title = {Learning {{Conditioned Graph Structures}} for {{Interpretable Visual Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Norcliffe-Brown, Will and Vafeias, Stathis and Parisot, Sarah},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2018/hash/4aeae10ea1c6433c926cdfa558d31134-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on  higher level image representations, which can capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain promising results with 66.18\% accuracy and demonstrate the interpretability of the proposed method. Code can be found at github.com/aimbrain/vqa-project.},
  file = {D\:\\Zotero\\storage\\QF2ZCPPT\\Norcliffe-Brown et al_2018_Learning Conditioned Graph Structures for Interpretable Visual Question.pdf}
}

@inproceedings{nugrohoLargeScaleNewsClassification2021,
  title = {Large-{{Scale News Classification}} Using {{BERT Language Model}}: {{Spark NLP Approach}}},
  shorttitle = {Large-{{Scale News Classification}} Using {{BERT Language Model}}},
  booktitle = {6th {{International Conference}} on {{Sustainable Information Engineering}} and {{Technology}} 2021},
  author = {Nugroho, Kuncahyo Setyo and Sukmadewa, Anantha Yullian and Yudistira, Novanto},
  date = {2021-09-13},
  pages = {240--246},
  publisher = {{ACM}},
  location = {{Malang Indonesia}},
  doi = {10.1145/3479645.3479658},
  url = {https://dl.acm.org/doi/10.1145/3479645.3479658},
  urldate = {2021-12-25},
  eventtitle = {{{SIET}} '21: 6th {{International Conference}} on {{Sustainable Information Engineering}} and {{Technology}} 2021},
  isbn = {978-1-4503-8407-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\5KX2PKZ8\\Nugroho 等。 - 2021 - Large-Scale News Classification using BERT Languag.pdf}
}

@unpublished{oualiSpatialContrastiveLearning2021,
  title = {Spatial {{Contrastive Learning}} for {{Few-Shot Classification}}},
  author = {Ouali, Yassine and Hudelot, Céline and Tami, Myriam},
  date = {2021-06-20},
  eprint = {2012.13831},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.13831},
  urldate = {2022-03-17},
  abstract = {In this paper, we explore contrastive learning for few-shot classification, in which we propose to use it as an additional auxiliary training objective acting as a data-dependent regularizer to promote more general and transferable features. In particular, we present a novel attention-based spatial contrastive objective to learn locally discriminative and class-agnostic features. As a result, our approach overcomes some of the limitations of the cross-entropy loss, such as its excessive discrimination towards seen classes, which reduces the transferability of features to unseen classes. With extensive experiments, we show that the proposed method outperforms state-of-the-art approaches, confirming the importance of learning good and transferable embeddings for few-shot learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\2MJA8R59\\Ouali et al_2021_Spatial Contrastive Learning for Few-Shot Classification.pdf;D\:\\Zotero\\storage\\JQXXKAME\\2012.html}
}

@inproceedings{parkBridgeAnswerStructureaware2021,
  title = {Bridge to {{Answer}}: {{Structure-aware Graph Interaction Network}} for {{Video Question Answering}}},
  shorttitle = {Bridge to {{Answer}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Park, Jungin and Lee, Jiyoung and Sohn, Kwanghoon},
  date = {2021-06},
  pages = {15521--15530},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01527},
  url = {https://ieeexplore.ieee.org/document/9578727/},
  urldate = {2022-02-05},
  abstract = {This paper presents a novel method, termed Bridge to Answer, to infer correct answers for questions about a given video by leveraging adequate graph interactions of heterogeneous crossmodal graphs. To realize this, we learn question conditioned visual graphs by exploiting the relation between video and question to enable each visual node using question-to-visual interactions to encompass both visual and linguistic cues. In addition, we propose bridged visualto-visual interactions to incorporate two complementary visual information on appearance and motion by placing the question graph as an intermediate bridge. This bridged architecture allows reliable message passing through compositional semantics of the question to generate an appropriate answer. As a result, our method can learn the question conditioned visual representations attributed to appearance and motion that show powerful capability for video question answering. Extensive experiments prove that the proposed method provides effective and superior performance than state-of-the-art methods on several benchmarks.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english}
}

@unpublished{parkGRPERelativePositional2022,
  title = {{{GRPE}}: {{Relative Positional Encoding}} for {{Graph Transformer}}},
  shorttitle = {{{GRPE}}},
  author = {Park, Wonpyo and Chang, Woonggi and Lee, Donggeon and Kim, Juntae and Hwang, Seung-won},
  date = {2022-03-16},
  eprint = {2201.12787},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.12787},
  urldate = {2022-03-26},
  abstract = {Designing an efficient model to encode graphs is a key challenge of molecular representation learning. Transformer built upon efficient self-attention is a natural choice for graph processing, but it requires explicit incorporation of positional information. Existing approaches either linearize a graph to encode absolution position in the sequence of nodes, or encode relative position with another node using bias terms. The former loses preciseness of relative position from linearization, while the latter loses a tight integration of node-edge and node-spatial information. In this work, we propose relative positional encoding for a graph to overcome the weakness of the previous approaches. Our method encodes a graph without linearization and considers both node-spatial relation and node-edge relation. We name our method Graph Relative Positional Encoding dedicated to graph representation learning. Experiments conducted on various molecular property prediction datasets show that the proposed method outperforms previous approaches significantly. Our code is publicly available at https://github.com/lenscloth/GRPE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\NP3GKX8R\\Park et al_2022_GRPE.pdf;D\:\\Zotero\\storage\\2EJQIPZW\\2201.html}
}

@unpublished{parkMultiViewAttentionNetwork2020,
  title = {Multi-{{View Attention Network}} for {{Visual Dialog}}},
  author = {Park, Sungjin and Whang, Taesun and Yoon, Yeochan and Lim, Heuiseok},
  date = {2020-10-06},
  eprint = {2004.14025},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.14025},
  urldate = {2021-12-08},
  abstract = {Visual dialog is a challenging vision-language task in which a series of questions visually grounded by a given image are answered. To resolve the visual dialog task, a high-level understanding of various multimodal inputs (e.g., question, dialog history, and image) is required. Specifically, it is necessary for an agent to 1) determine the semantic intent of question and 2) align question-relevant textual and visual contents among heterogeneous modality inputs. In this paper, we propose Multi-View Attention Network (MVAN), which leverages multiple views about heterogeneous inputs based on attention mechanisms. MVAN effectively captures the question-relevant information from the dialog history with two complementary modules (i.e., Topic Aggregation and Context Matching), and builds multimodal representations through sequential alignment processes (i.e., Modality Alignment). Experimental results on VisDial v1.0 dataset show the effectiveness of our proposed model, which outperforms the previous state-of-the-art methods with respect to all evaluation metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\3YGNRGV3\\Park et al_2020_Multi-View Attention Network for Visual Dialog.pdf;D\:\\Zotero\\storage\\KV2RA3SH\\2004.html}
}

@incollection{patelRecentAdvancesVideo2021,
  title = {Recent {{Advances}} in {{Video Question Answering}}: {{A Review}} of {{Datasets}} and {{Methods}}},
  shorttitle = {Recent {{Advances}} in {{Video Question Answering}}},
  booktitle = {Pattern {{Recognition}}. {{ICPR International Workshops}} and {{Challenges}}},
  author = {Patel, Devshree and Parikh, Ratnam and Shastri, Yesha},
  editor = {Del Bimbo, Alberto and Cucchiara, Rita and Sclaroff, Stan and Farinella, Giovanni Maria and Mei, Tao and Bertini, Marco and Escalante, Hugo Jair and Vezzani, Roberto},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12662},
  pages = {339--356},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-68790-8_27},
  url = {http://link.springer.com/10.1007/978-3-030-68790-8_27},
  urldate = {2021-03-11},
  abstract = {Video Question Answering (VQA) is a recent emerging challenging task in the field of Computer Vision. Several visual information retrieval techniques like Video Captioning/Description and Video-guided Machine Translation have preceded the task of VQA. VQA helps to retrieve temporal and spatial information from the video scenes and interpret it. In this survey, we review a number of methods and datasets for the task of VQA. To the best of our knowledge, no previous survey has been conducted for the VQA task.},
  isbn = {978-3-030-68789-2 978-3-030-68790-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\G7X52P3L\\Patel 等。 - 2021 - Recent Advances in Video Question Answering A Rev.pdf}
}

@inproceedings{patroDeepBayesianNetwork2020,
  title = {Deep {{Bayesian Network}} for {{Visual Question Generation}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Patro, Badri N. and Kurmi, Vinod K. and Kumar, Sandeep and Namboodiri, Vinay P.},
  date = {2020-03},
  pages = {1555--1565},
  publisher = {{IEEE}},
  location = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093293},
  url = {https://ieeexplore.ieee.org/document/9093293/},
  urldate = {2021-09-10},
  abstract = {Generating natural questions from an image is a semantic task that requires using vision and language modalities to learn multimodal representations. Images can have multiple visual and language cues such as places, captions, and tags. In this paper, we propose a principled deep Bayesian learning framework that combines these cues to produce natural questions. We observe that with the addition of more cues and by minimizing uncertainty in the among cues, the Bayesian network becomes more confident. We propose a Minimizing Uncertainty of Mixture of Cues (MUMC), that minimizes uncertainty present in a mixture of cues experts for generating probabilistic questions. This is a Bayesian framework and the results show a remarkable similarity to natural questions as validated by a human study. We observe that with the addition of more cues and by minimizing uncertainty among the cues, the Bayesian framework becomes more confident. Ablation studies of our model indicate that a subset of cues is inferior at this task and hence the principled fusion of cues is preferred. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE, and CIDEr). Here we provide project link for Deep Bayesian VQG https: //delta-lab-iitk.github.io/BVQG/.},
  eventtitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-72816-553-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\U3G2HAYU\\Patro_Deep_Bayesian_Network_for_Visual_Question_Generation_WACV_2020_paper.pdf}
}

@inproceedings{patroDifferentialAttentionVisual2018,
  title = {Differential {{Attention}} for {{Visual Question Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Patro, Badri and Namboodiri, Vinay P.},
  date = {2018-06},
  pages = {7680--7688},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00801},
  url = {https://ieeexplore.ieee.org/document/8578899/},
  urldate = {2021-09-09},
  abstract = {In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a number of images during training. A number of methods have focused on solving this problem by using image based attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention region. This differential attention is closer to human attention than other image based attention methods. It also helps in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We perform better than other image based attention methods and are competitive with other state of the art methods that focus on both image and questions.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {23 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\28NHXJRN\\Patro 和 Namboodiri - 2018 - Differential Attention for Visual Question Answeri.pdf}
}

@article{patroExplanationVsAttention2020,
  title = {Explanation vs {{Attention}}: {{A Two-Player Game}} to {{Obtain Attention}} for {{VQA}}},
  shorttitle = {Explanation vs {{Attention}}},
  author = {Patro, Badri and {Anupriy} and Namboodiri, Vinay},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {07},
  pages = {11848--11855},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i07.6858},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6858},
  urldate = {2021-09-10},
  abstract = {In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. Visualization of the results also confirms our hypothesis that attention maps improve using this form of supervision.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\9KSKKFMC\\Patro 等。 - 2020 - Explanation vs Attention A Two-Player Game to Obt.pdf}
}

@unpublished{patroGranularMultimodalAttention2019,
  title = {Granular {{Multimodal Attention Networks}} for {{Visual Dialog}}},
  author = {Patro, Badri N. and Patel, Shivansh and Namboodiri, Vinay P.},
  date = {2019-10-13},
  eprint = {1910.05728},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1910.05728},
  urldate = {2021-12-08},
  abstract = {Vision and language tasks have benefited from attention. There have been a number of different attention models proposed. However, the scale at which attention needs to be applied has not been well examined. Particularly, in this work, we propose a new method Granular Multi-modal Attention, where we aim to particularly address the question of the right granularity at which one needs to attend while solving the Visual Dialog task. The proposed method shows improvement in both image and text attention networks. We then propose a granular Multi-modal Attention network that jointly attends on the image and text granules and shows the best performance. With this work, we observe that obtaining granular attention and doing exhaustive Multi-modal Attention appears to be the best way to attend while solving visual dialog.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\JSW3EV6G\\Patro et al_2019_Granular Multimodal Attention Networks for Visual Dialog.pdf;D\:\\Zotero\\storage\\WVD4IKWX\\1910.html}
}

@inproceedings{patroRobustExplanationsVisual2020,
  title = {Robust {{Explanations}} for {{Visual Question Answering}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Patro, Badri N. and Patel, Shivansh and Namboodiri, Vinay P.},
  date = {2020-03},
  pages = {1566--1575},
  publisher = {{IEEE}},
  location = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093295},
  url = {https://ieeexplore.ieee.org/document/9093295/},
  urldate = {2021-09-10},
  abstract = {In this paper, we propose a method to obtain robust explanations for visual question answering(VQA) that correlate well with the answers. Our model explains the answers obtained through a VQA model by providing visual and textual explanations. The main challenges that we address are i) Answers and textual explanations obtained by current methods are not well correlated and ii) Current methods for visual explanation do not focus on the right location for explaining the answer. We address both these challenges by using a collaborative correlated module which ensures that even if we do not train for noise based attacks, the enhanced correlation ensures that the right explanation and answer can be generated. We further show that this also aids in improving the generated visual and textual explanations. The use of the correlated module can be thought of as a robust method to verify if the answer and explanations are coherent. We evaluate this model using VQA-X dataset. We observe that the proposed method yields better textual and visual justification that supports the decision. We showcase the robustness of the model against a noise-based perturbation attack using corresponding visual and textual explanations. A detailed empirical analysis is shown.},
  eventtitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-72816-553-0},
  langid = {english},
  keywords = {Explanations,Robust},
  file = {D\:\\Zotero\\storage\\YVY67MJY\\Patro 等。 - 2020 - Robust Explanations for Visual Question Answering.pdf}
}

@unpublished{pengBalancedMultimodalLearning2022,
  title = {Balanced {{Multimodal Learning}} via {{On-the-fly Gradient Modulation}}},
  author = {Peng, Xiaokang and Wei, Yake and Deng, Andong and Wang, Dong and Hu, Di},
  date = {2022-03-29},
  number = {arXiv:2203.15332},
  eprint = {2203.15332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.15332},
  urldate = {2022-05-19},
  abstract = {Multimodal learning helps to comprehensively understand the world, by integrating different senses. Accordingly, multiple input modalities are expected to boost model performance, but we actually find that they are not fully exploited even when the multimodal model outperforms its uni-modal counterpart. Specifically, in this paper we point out that existing multimodal discriminative models, in which uniform objective is designed for all modalities, could remain under-optimized uni-modal representations, caused by another dominated modality in some scenarios, e.g., sound in blowing wind event, vision in drawing picture event, etc. To alleviate this optimization imbalance, we propose on-the-fly gradient modulation to adaptively control the optimization of each modality, via monitoring the discrepancy of their contribution towards the learning objective. Further, an extra Gaussian noise that changes dynamically is introduced to avoid possible generalization drop caused by gradient modulation. As a result, we achieve considerable improvement over common fusion methods on different multimodal tasks, and this simple strategy can also boost existing multimodal methods, which illustrates its efficacy and versatility. The source code is available at \textbackslash url\{https://github.com/GeWu-Lab/OGM-GE\_CVPR2022\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\PSISTARF\\Peng et al_2022_Balanced Multimodal Learning via On-the-fly Gradient Modulation.pdf;D\:\\Zotero\\storage\\9DIELZPZ\\2203.html}
}

@unpublished{pengBalancedMultimodalLearning2022a,
  title = {Balanced {{Multimodal Learning}} via {{On-the-fly Gradient Modulation}}},
  author = {Peng, Xiaokang and Wei, Yake and Deng, Andong and Wang, Dong and Hu, Di},
  date = {2022-03-29},
  eprint = {2203.15332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2203.15332},
  urldate = {2022-04-18},
  abstract = {Multimodal learning helps to comprehensively understand the world, by integrating different senses. Accordingly, multiple input modalities are expected to boost model performance, but we actually find that they are not fully exploited even when the multimodal model outperforms its uni-modal counterpart. Specifically, in this paper we point out that existing multimodal discriminative models, in which uniform objective is designed for all modalities, could remain under-optimized uni-modal representations, caused by another dominated modality in some scenarios, e.g., sound in blowing wind event, vision in drawing picture event, etc. To alleviate this optimization imbalance, we propose on-the-fly gradient modulation to adaptively control the optimization of each modality, via monitoring the discrepancy of their contribution towards the learning objective. Further, an extra Gaussian noise that changes dynamically is introduced to avoid possible generalization drop caused by gradient modulation. As a result, we achieve considerable improvement over common fusion methods on different multimodal tasks, and this simple strategy can also boost existing multimodal methods, which illustrates its efficacy and versatility. The source code is available at \textbackslash url\{https://github.com/GeWu-Lab/OGM-GE\_CVPR2022\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\EVNSEBRE\\Peng et al_2022_Balanced Multimodal Learning via On-the-fly Gradient Modulation.pdf;D\:\\Zotero\\storage\\QPSY25CE\\2203.html}
}

@unpublished{pengHierarchicalTaxonomyAwareAttentional2019,
  title = {Hierarchical {{Taxonomy-Aware}} and {{Attentional Graph Capsule RCNNs}} for {{Large-Scale Multi-Label Text Classification}}},
  author = {Peng, Hao and Li, Jianxin and Gong, Qiran and Wang, Senzhang and He, Lifang and Li, Bo and Wang, Lihong and Yu, Philip S.},
  date = {2019-06-09},
  eprint = {1906.04898},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.04898},
  urldate = {2021-10-10},
  abstract = {CNNs, RNNs, GCNs, and CapsNets have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. However, most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them both coherently is less studied. In addition, most existing methods treat output labels as independent methods, but ignore the hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent CNNs framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule recurrent CNNs for more effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\DBENNCGL\\Peng et al_2019_Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for Large-Scale.pdf;D\:\\Zotero\\storage\\M8UJ97IF\\1906.html}
}

@inproceedings{pengMultiModalityLatentInteraction2019,
  title = {Multi-{{Modality Latent Interaction Network}} for {{Visual Question Answering}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Peng, Gao and You, Haoxuan and Zhang, Zhanpeng and Wang, Xiaogang and Li, Hongsheng},
  date = {2019-10},
  pages = {5824--5834},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00592},
  url = {https://ieeexplore.ieee.org/document/9010837/},
  urldate = {2021-03-11},
  abstract = {Exploiting relationships between visual regions and question words have achieved great success in learning multi-modality features for Visual Question Answering (VQA). However, we argue that existing methods [29] mostly model relations between individual visual regions and words, which are not enough to correctly answer the question. From humans’ perspective, answering a visual question requires understanding the summarizations of visual and language information. In this paper, we proposed the Multi-modality Latent Interaction module (MLI) to tackle this problem. The proposed module learns the cross-modality relationships between latent visual and language summarizations, which summarize visual regions and question into a small number of latent representations to avoid modeling uninformative individual region-word relations. The cross-modality information between the latent summarizations are propagated to fuse valuable information from both modalities and are used to update the visual and word features. Such MLI modules can be stacked for several stages to model complex and latent relations between the two modalities and achieves highly competitive performance on public VQA benchmarks, VQA v2.0 [12] and TDIUC [20]. In addition, we show that the performance of our methods could be significantly improved by combining with pre-trained language model BERT[6].},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\CCR9LMHA\\Peng 等。 - 2019 - Multi-Modality Latent Interaction Network for Visu.pdf}
}

@unpublished{pengSemanticAwareDomainGeneralized2022,
  title = {Semantic-{{Aware Domain Generalized Segmentation}}},
  author = {Peng, Duo and Lei, Yinjie and Hayat, Munawar and Guo, Yulan and Li, Wen},
  date = {2022-04-02},
  eprint = {2204.00822},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2204.00822},
  urldate = {2022-04-20},
  abstract = {Deep models trained on source domain lack generalization when evaluated on unseen target domains with different data distributions. The problem becomes even more pronounced when we have no access to target domain samples for adaptation. In this paper, we address domain generalized semantic segmentation, where a segmentation model is trained to be domain-invariant without using any target domain data. Existing approaches to tackle this problem standardize data into a unified distribution. We argue that while such a standardization promotes global normalization, the resulting features are not discriminative enough to get clear segmentation boundaries. To enhance separation between categories while simultaneously promoting domain invariance, we propose a framework including two novel modules: Semantic-Aware Normalization (SAN) and Semantic-Aware Whitening (SAW). Specifically, SAN focuses on category-level center alignment between features from different image styles, while SAW enforces distributed alignment for the already center-aligned features. With the help of SAN and SAW, we encourage both intra-category compactness and inter-category separability. We validate our approach through extensive experiments on widely-used datasets (i.e. GTAV, SYNTHIA, Cityscapes, Mapillary and BDDS). Our approach shows significant improvements over existing state-of-the-art on various backbone networks. Code is available at https://github.com/leolyj/SAN-SAW},
  archiveprefix = {arXiv},
  keywords = {68T45,Computer Science - Computer Vision and Pattern Recognition,I.2.10,I.4.6},
  file = {D\:\\Zotero\\storage\\TFFMXK2X\\Peng et al_2022_Semantic-Aware Domain Generalized Segmentation.pdf;D\:\\Zotero\\storage\\H22EF2RR\\2204.html}
}

@inproceedings{perez-ruaMFASMultimodalFusion2019,
  title = {{{MFAS}}: {{Multimodal Fusion Architecture Search}}},
  shorttitle = {{{MFAS}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Perez-Rua, Juan-Manuel and Vielzeuf, Valentin and Pateux, Stephane and Baccouche, Moez and Jurie, Frederic},
  date = {2019-06},
  pages = {6959--6968},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00713},
  url = {https://ieeexplore.ieee.org/document/8954353/},
  urldate = {2021-03-11},
  abstract = {We tackle the problem of finding good architectures for multimodal classification problems. We propose a novel and generic search space that spans a large number of possible fusion architectures. In order to find an optimal architecture for a given dataset in the proposed search space, we leverage an efficient sequential model-based exploration approach that is tailored for the problem. We demonstrate the value of posing multimodal fusion as a neural architecture search problem by extensive experimentation on a toy dataset and two other real multimodal datasets. We discover fusion architectures that exhibit state-of-the-art performance for problems with different domain and dataset size, including the NTU RGB+D dataset, the largest multimodal action recognition dataset available.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\G5TJYIDK\\Perez-Rua 等。 - 2019 - MFAS Multimodal Fusion Architecture Search.pdf}
}

@inproceedings{pingaliMultimodalGraphbasedTransformer2021,
  title = {Multimodal {{Graph-based Transformer Framework}} for {{Biomedical Relation Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Pingali, Sriram and Yadav, Shweta and Dutta, Pratik and Saha, Sriparna},
  date = {2021},
  pages = {3741--3747},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.328},
  url = {https://aclanthology.org/2021.findings-acl.328},
  urldate = {2022-02-10},
  eventtitle = {Findings 2021},
  file = {D\:\\Zotero\\storage\\TPA6W4XJ\\Pingali et al_2021_Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction.pdf}
}

@article{PRELIMINARYVERSIONNOT,
  title = {{{PRELIMINARY VERSION DO NOT CITE}}},
  pages = {9},
  abstract = {Image captioning is a challenging computer vision task, which aims to generate a natural language description of an image. Most recent researches follow the encoder-decoder framework which depends heavily on the previous generated words for the current prediction. Such methods can not effectively take advantage of the future predicted information to learn complete semantics. In this paper, we propose Context-Aware Auxiliary Guidance (CAAG) mechanism that can guide the captioning model to perceive global contexts. Upon the captioning model, CAAG performs semantic attention that selectively concentrates on useful information of the global predictions to reproduce the current generation. To validate the adaptability of the method, we apply CAAG to three popular captioners and our proposal achieves competitive performance on the challenging Microsoft COCO image captioning benchmark, e.g. 132.2 CIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official online evaluation server.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\KZUZGFVU\\PRELIMINARY VERSION DO NOT CITE.pdf}
}

@article{PRELIMINARYVERSIONNOTa,
  title = {{{PRELIMINARY VERSION DO NOT CITE}}},
  pages = {9},
  abstract = {Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed to recognize objects from images. The current literature demonstrates many advantages of CapsNets over CNNs. However, how to create explanations for individual classifications of CapsNets has not been well explored. The widely used saliency methods are mainly proposed for explaining CNN-based classifications; they create saliency map explanations by combining activation values and the corresponding gradients, e.g., Grad-CAM. These saliency methods require a specific architecture of the underlying classifiers and cannot be trivially applied to CapsNets due to the iterative routing mechanism therein. To overcome the lack of interpretability, we can either propose new post-hoc interpretation methods for CapsNets or modifying the model to have build-in explanations. In this work, we explore the latter. Specifically, we propose interpretable Graph Capsule Networks (GraCapsNets), where we replace the routing part with a multi-head attention-based Graph Pooling approach. In the proposed model, individual classification explanations can be created effectively and efficiently. Our model also demonstrates some unexpected benefits, even though it replaces the fundamental part of CapsNets. Our GraCapsNets achieve better classification performance with fewer parameters and better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets also keep other advantages of CapsNets, namely, disentangled representations and affine transformation robustness.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\3JBKZ4SF\\PRELIMINARY VERSION DO NOT CITE.pdf}
}

@article{PRELIMINARYVERSIONNOTb,
  title = {{{PRELIMINARY VERSION DO NOT CITE}}},
  pages = {9},
  abstract = {Sentence semantic matching is one of the fundamental tasks in natural language processing, which requires an agent to determine the semantic relation among input sentences. Recently, deep neural networks have achieved impressive performance in this area, especially BERT. Despite their effectiveness, most of these models treat output labels as meaningless one-hot vectors, underestimating the semantic information and guidance of relations that these labels reveal, especially for tasks with a small number of labels. To address this problem, we propose a Relation of Relation Learning Network (R2-Net) for sentence semantic matching. Specifically, we first employ BERT to encode the input sentences from a global perspective. Then a CNN-based encoder is designed to capture keywords and phrase information from a local perspective. To fully leverage labels for better relation information extraction, we introduce a self-supervised relation of relation classification task for guiding R2-Net to consider more about relations. Meanwhile, a triplet loss is employed to distinguish the intra-class and inter-class relations in a finer granularity. Empirical experiments on two sentence semantic matching tasks demonstrate the superiority of our proposed model. As a byproduct, we have released the codes to facilitate other researches.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4ERPJJGF\\PRELIMINARY VERSION DO NOT CITE.pdf}
}

@article{PRELIMINARYVERSIONNOTc,
  title = {{{PRELIMINARY VERSION DO NOT CITE}}},
  pages = {9},
  abstract = {Sentence semantic matching is one of the fundamental tasks in natural language processing, which requires an agent to determine the semantic relation among input sentences. Recently, deep neural networks have achieved impressive performance in this area, especially BERT. Despite their effectiveness, most of these models treat output labels as meaningless one-hot vectors, underestimating the semantic information and guidance of relations that these labels reveal, especially for tasks with a small number of labels. To address this problem, we propose a Relation of Relation Learning Network (R2-Net) for sentence semantic matching. Specifically, we first employ BERT to encode the input sentences from a global perspective. Then a CNN-based encoder is designed to capture keywords and phrase information from a local perspective. To fully leverage labels for better relation information extraction, we introduce a self-supervised relation of relation classification task for guiding R2-Net to consider more about relations. Meanwhile, a triplet loss is employed to distinguish the intra-class and inter-class relations in a finer granularity. Empirical experiments on two sentence semantic matching tasks demonstrate the superiority of our proposed model. As a byproduct, we have released the codes to facilitate other researches.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\AKZ6A5TL\\PRELIMINARY VERSION DO NOT CITE.pdf}
}

@article{PRELIMINARYVERSIONNOTd,
  title = {{{PRELIMINARY VERSION DO NOT CITE}}},
  pages = {9},
  abstract = {Visual reasoning and question-answering have gathered attention in recent years. Many datasets and evaluation protocols have been proposed; some have been shown to contain bias that allows models to “cheat” without performing true, generalizable reasoning. A well-known bias is dependence on language priors (frequency of answers) resulting in the model not looking at the image. We discover a new type of bias in the Visual Commonsense Reasoning (VCR) dataset. In particular we show that most state-of-the-art models exploit co-occurring text between input (question) and output (answer options), and rely on only a few pieces of information in the candidate options, to make a decision. Unfortunately, relying on such superficial evidence causes models to be very fragile. To measure fragility, we propose two ways to modify the validation data, in which a few words in the answer choices are modified without significant changes in meaning. We find such insignificant changes cause models’ performance to degrade significantly. To resolve the issue, we propose a curriculum-based masking approach, as a mechanism to perform more robust training. Our method improves the baseline by requiring it to pay attention to the answers as a whole, and is more effective than prior masking strategies. Our code and data are available at https://github.com/yekeren/VCR-shortcut-effects-study.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\IERZ9M37\\PRELIMINARY VERSION DO NOT CITE.pdf}
}

@article{qiaoExploringHumanLikeAttention,
  title = {Exploring {{Human-Like Attention Supervision}} in {{Visual Question Answering}}},
  author = {Qiao, Tingting and Dong, Jianfeng and Xu, Duanqing},
  pages = {8},
  abstract = {Attention mechanisms have been widely applied in the Visual Question Answering (VQA) task, as they help to focus on the area-of-interest of both visual and textual information. To answer the questions correctly, the model needs to selectively target different areas of an image, which suggests that an attention-based model may benefit from an explicit attention supervision. In this work, we aim to address the problem of adding attention supervision to VQA models. Since there is a lack of human attention data, we first propose a Human Attention Network (HAN) to generate human-like attention maps, training on a recently released dataset called Human ATtention Dataset (VQA-HAT). Then, we apply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the human-like attention maps for all image-question pairs. The generated human-like attention map dataset for the VQA v2.0 dataset is named as Human-Like ATtention (HLAT) dataset. Finally, we apply human-like attention supervision to an attention-based VQA model. The experiments show that adding human-like supervision yields a more accurate attention together with a better performance, showing a promising future for human-like attention supervision in VQA.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZAPARVYG\\Qiao 等。 - Exploring Human-Like Attention Supervision in Visu.pdf}
}

@unpublished{qiaoWinnerTeamMia2021,
  title = {Winner {{Team Mia}} at {{TextVQA Challenge}} 2021: {{Vision-and-Language Representation Learning}} with {{Pre-trained Sequence-to-Sequence Model}}},
  shorttitle = {Winner {{Team Mia}} at {{TextVQA Challenge}} 2021},
  author = {Qiao, Yixuan and Chen, Hao and Wang, Jun and Chen, Yihao and Ye, Xianbin and Li, Ziliang and Qi, Xianbiao and Gao, Peng and Xie, Guotong},
  date = {2021-06-24},
  eprint = {2106.15332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.15332},
  urldate = {2021-12-10},
  abstract = {TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions. In this challenge, we use generative model T5 for TextVQA task. Based on pre-trained checkpoint T5-3B from HuggingFace repository, two other pre-training tasks including masked language modeling(MLM) and relative position prediction(RPP) are designed to better align object feature and scene text. In the stage of pre-training, encoder is dedicate to handle the fusion among multiple modalities: question text, object text labels, scene text labels, object visual features, scene visual features. After that decoder generates the text sequence step-by-step, cross entropy loss is required by default. We use a large-scale scene text dataset in pre-training and then fine-tune the T5-3B with the TextVQA dataset only.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\YQZRZXFG\\Qiao et al_2021_Winner Team Mia at TextVQA Challenge 2021.pdf;D\:\\Zotero\\storage\\MZBURZDA\\2106.html}
}

@unpublished{qiTwoCausalPrinciples2020,
  title = {Two {{Causal Principles}} for {{Improving Visual Dialog}}},
  author = {Qi, Jiaxin and Niu, Yulei and Huang, Jianqiang and Zhang, Hanwang},
  date = {2020-03-02},
  eprint = {1911.10496},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.10496},
  urldate = {2021-12-08},
  abstract = {This paper unravels the design tricks adopted by us, the champion team MReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for improving Visual Dialog (VisDial). By "improving", we mean that they can promote almost every existing VisDial model to the state-of-the-art performance on the leader-board. Such a major improvement is only due to our careful inspection on the causality behind the model and data, finding that the community has overlooked two causalities in VisDial. Intuitively, Principle 1 suggests: we should remove the direct input of the dialog history to the answer model, otherwise a harmful shortcut bias will be introduced; Principle 2 says: there is an unobserved confounder for history, question, and answer, leading to spurious correlations from training data. In particular, to remove the confounder suggested in Principle 2, we propose several causal intervention algorithms, which make the training fundamentally different from the traditional likelihood estimation. Note that the two principles are model-agnostic, so they are applicable in any VisDial model. The code is available at https://github.com/simpleshinobu/visdial-principles.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\KNCQ6PN7\\Qi et al_2020_Two Causal Principles for Improving Visual Dialog.pdf;D\:\\Zotero\\storage\\QTCZ3GIS\\1911.html}
}

@inproceedings{qiuIncorporating3DInformation2019,
  title = {Incorporating {{3D Information Into Visual Question Answering}}},
  booktitle = {2019 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Qiu, Yue and Satoh, Yutaka and Suzuki, Ryota and Kataoka, Hirokatsu},
  date = {2019-09},
  pages = {756--765},
  publisher = {{IEEE}},
  location = {{Québec City, QC, Canada}},
  doi = {10.1109/3DV.2019.00088},
  url = {https://ieeexplore.ieee.org/document/8885753/},
  urldate = {2022-06-02},
  abstract = {We propose a tactic of advancing Visual Question Answering (VQA) task by incorporating 3D information via multi-view images. Conventional VQA approaches, which reply an answer in words against a linguistic question about a given RGB image, have less ability to recognize geometrical information so that they tend to fail to count things or guess positional relationship. Moreover, they have no ability to determine blinded space, so it is not feasible to invent VQA function to robots which will work in highly-occluded real-world environments. To achieve the situation, we introduce a new multi-view VQA dataset along with an approach that incorporating 3D scene information directly captured from multi-view images into VQA without using depth images or employing SLAM. Our proposed approach achieves strong performance with an overall accuracy of 95.4\% on the challenging multi-view VQA dataset setup, which contains relatively severe occlusion. This work also demonstrates the promising aspects of bridging the gap between 3D vision and language.},
  eventtitle = {2019 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  isbn = {978-1-72813-131-3},
  langid = {english},
  file = {D\:\\Zotero\\storage\\RZVI26EH\\Qiu 等。 - 2019 - Incorporating 3D Information Into Visual Question .pdf}
}

@inproceedings{quPassageRetrievalOutsideKnowledge2021,
  title = {Passage {{Retrieval}} for {{Outside-Knowledge Visual Question Answering}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Qu, Chen and Zamani, Hamed and Yang, Liu and Croft, W. Bruce and Learned-Miller, Erik},
  date = {2021-07-11},
  pages = {1753--1757},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3462987},
  url = {https://dl.acm.org/doi/10.1145/3404835.3462987},
  urldate = {2021-09-10},
  abstract = {In this work, we address multi-modal information needs that contain text questions and images by focusing on passage retrieval for outside-knowledge visual question answering. This task requires access to outside knowledge, which in our case we define to be a large unstructured passage collection. We first conduct sparse retrieval with BM25 and study expanding the question with object names and image captions. We verify that visual clues play an important role and captions tend to be more informative than object names in sparse retrieval. We then construct a dual-encoder dense retriever, with the query encoder being LXMERT [35], a multi-modal pre-trained transformer. We further show that dense retrieval significantly outperforms sparse retrieval that uses object expansion. Moreover, dense retrieval matches the performance of sparse retrieval that leverages human-generated captions.},
  eventtitle = {{{SIGIR}} '21: {{The}} 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\P6FMIGSV\\Qu 等。 - 2021 - Passage Retrieval for Outside-Knowledge Visual Que.pdf}
}

@unpublished{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2022-06-12},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\T236FMUI\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf;D\:\\Zotero\\storage\\PJKXY5XI\\2103.html}
}

@unpublished{radfordLearningTransferableVisual2021a,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2021-09-13},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\SB2I2EP2\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf;D\:\\Zotero\\storage\\UQ3DIZ5T\\2103.html}
}

@unpublished{radfordLearningTransferableVisual2021b,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2021-09-13},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\Q8IMDJYQ\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf;D\:\\Zotero\\storage\\WGEF76KH\\2103.html}
}

@article{rahmanImprovedAttentionVisual,
  title = {An {{Improved Attention}} for {{Visual Question Answering}}},
  author = {Rahman, Tanzila and Chou, Shih-Han and Sigal, Leonid and Carenini, Giuseppe},
  pages = {10},
  abstract = {We consider the problem of Visual Question Answering (VQA). Given an image and a free-form, open-ended, question, expressed in natural language, the goal of VQA system is to provide accurate answer to this question with respect to the image. The task is challenging because it requires simultaneous and intricate understanding of both visual and textual information. Attention, which captures intra- and inter-modal dependencies, has emerged as perhaps the most widely used mechanism for addressing these challenges. In this paper, we propose an improved attention-based architecture to solve VQA. We incorporate an Attention on Attention (AoA) module within encoder-decoder framework, which is able to determine the relation between attention results and queries. Attention module generates weighted average for each query. On the other hand, AoA module first generates an information vector and an attention gate using attention results and current context; and then adds another attention to generate final attended information by multiplying the two. We also propose multimodal fusion module to combine both visual and textual information. The goal of this fusion module is to dynamically decide how much information should be considered from each modality. Extensive experiments on VQA-v2 benchmark dataset show that our method achieves the state-of-the-art performance.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\6LD3JL5S\\Rahman 等。 - An Improved Attention for Visual Question Answerin.pdf}
}

@inproceedings{ramakrishnanOvercomingLanguagePriors2018,
  title = {Overcoming {{Language Priors}} in {{Visual Question Answering}} with {{Adversarial Regularization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ramakrishnan, Sainandan and Agrawal, Aishwarya and Lee, Stefan},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2018/hash/67d96d458abdef21792e6d8e590244e7-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training -- \textbackslash eg overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings.},
  file = {D\:\\Zotero\\storage\\3DE2TDHB\\Ramakrishnan et al_2018_Overcoming Language Priors in Visual Question Answering with Adversarial.pdf}
}

@unpublished{raoFirstLookExplainable2021,
  title = {A {{First Look}}: {{Towards Explainable TextVQA Models}} via {{Visual}} and {{Textual Explanations}}},
  shorttitle = {A {{First Look}}},
  author = {Rao, Varun Nagaraj and Zhen, Xingjian and Hovsepian, Karen and Shen, Mingwei},
  date = {2021-04-28},
  eprint = {2105.02626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.02626},
  urldate = {2021-12-10},
  abstract = {Explainable deep learning models are advantageous in many situations. Prior work mostly provide unimodal explanations through post-hoc approaches not part of the original system design. Explanation mechanisms also ignore useful textual information present in images. In this paper, we propose MTXNet, an end-to-end trainable multimodal architecture to generate multimodal explanations, which focuses on the text in the image. We curate a novel dataset TextVQA-X, containing ground truth visual and multi-reference textual explanations that can be leveraged during both training and evaluation. We then quantitatively show that training with multimodal explanations complements model performance and surpasses unimodal baselines by up to 7\% in CIDEr scores and 2\% in IoU. More importantly, we demonstrate that the multimodal explanations are consistent with human interpretations, help justify the models' decision, and provide useful insights to help diagnose an incorrect prediction. Finally, we describe a real-world e-commerce application for using the generated multimodal explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\JVCL7US6\\Rao et al_2021_A First Look.pdf;D\:\\Zotero\\storage\\UI82PFF6\\2105.html}
}

@unpublished{reichAdventurerTreasureHunt2021,
  title = {Adventurer's {{Treasure Hunt}}: {{A Transparent System}} for {{Visually Grounded Compositional Visual Question Answering}} Based on {{Scene Graphs}}},
  shorttitle = {Adventurer's {{Treasure Hunt}}},
  author = {Reich, Daniel and Putze, Felix and Schultz, Tanja},
  date = {2021-06-28},
  eprint = {2106.14476},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.14476},
  urldate = {2022-02-10},
  abstract = {With the expressed goal of improving system transparency and visual grounding in the reasoning process in VQA, we present a modular system for the task of compositional VQA based on scene graphs. Our system is called "Adventurer's Treasure Hunt" (or ATH), named after an analogy we draw between our model's search procedure for an answer and an adventurer's search for treasure. We developed ATH with three characteristic features in mind: 1. By design, ATH allows us to explicitly quantify the impact of each of the sub-components on overall VQA performance, as well as their performance on their individual sub-task. 2. By modeling the search task after a treasure hunt, ATH inherently produces an explicit, visually grounded inference path for the processed question. 3. ATH is the first GQA-trained VQA system that dynamically extracts answers by querying the visual knowledge base directly, instead of selecting one from a specially learned classifier's output distribution over a pre-fixed answer vocabulary. We report detailed results on all components and their contributions to overall VQA performance on the GQA dataset and show that ATH achieves the highest visual grounding score among all examined systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\DBCCNQKE\\Reich et al_2021_Adventurer's Treasure Hunt.pdf;D\:\\Zotero\\storage\\ASR3Y6ML\\2106.html}
}

@unpublished{rongSelfSupervisedGraphTransformer2020,
  title = {Self-{{Supervised Graph Transformer}} on {{Large-Scale Molecular Data}}},
  author = {Rong, Yu and Bian, Yatao and Xu, Tingyang and Xie, Weiyang and Wei, Ying and Huang, Wenbing and Huang, Junzhou},
  date = {2020-10-28},
  eprint = {2007.02835},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2007.02835},
  urldate = {2022-03-26},
  abstract = {How to obtain informative representations of molecules is a crucial prerequisite in AI-driven drug design and discovery. Recent researches abstract molecules as graphs and employ Graph Neural Networks (GNNs) for molecular representation learning. Nevertheless, two issues impede the usage of GNNs in real scenarios: (1) insufficient labeled molecules for supervised training; (2) poor generalization capability to new-synthesized molecules. To address them both, we propose a novel framework, GROVER, which stands for Graph Representation frOm self-superVised mEssage passing tRansformer. With carefully designed self-supervised tasks in node-, edge- and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks into the Transformer-style architecture to deliver a class of more expressive encoders of molecules. The flexibility of GROVER allows it to be trained efficiently on large-scale molecular dataset without requiring any supervision, thus being immunized to the two issues mentioned above. We pre-train GROVER with 100 million parameters on 10 million unlabelled molecules -- the biggest GNN and the largest training dataset in molecular representation learning. We then leverage the pre-trained GROVER for molecular property prediction followed by task-specific fine-tuning, where we observe a huge improvement (more than 6\% on average) from current state-of-the-art methods on 11 challenging benchmarks. The insights we gained are that well-designed self-supervision losses and largely-expressive pre-trained models enjoy the significant potential on performance boosting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.2.0,J.3,Quantitative Biology - Biomolecules},
  file = {D\:\\Zotero\\storage\\GKEKVV6V\\Rong et al_2020_Self-Supervised Graph Transformer on Large-Scale Molecular Data.pdf;D\:\\Zotero\\storage\\FCMKPBWJ\\2007.html}
}

@inproceedings{rosenbergAreVQASystems2021,
  title = {Are {{VQA Systems RAD}}? {{Measuring Robustness}} to {{Augmented Data}} with {{Focused Interventions}}},
  shorttitle = {Are {{VQA Systems RAD}}?},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Rosenberg, Daniel and Gat, Itai and Feder, Amir and Reichart, Roi},
  date = {2021-08},
  pages = {61--70},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-short.10},
  url = {https://aclanthology.org/2021.acl-short.10},
  urldate = {2021-09-09},
  abstract = {Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their robustness to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented Data (RAD), which measures the consistency of model predictions between original and augmented examples. Through extensive experimentation, we show that RAD, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to counterfactuals. We find substantial failure cases which reveal that current VQA systems are still brittle. Finally, we connect between robustness and generalization, demonstrating the predictive power of RAD for performance on unseen augmentations.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  keywords = {Robustness},
  file = {D\:\\Zotero\\storage\\J7K2N28K\\Rosenberg et al_2021_Are VQA Systems RAD.pdf}
}

@unpublished{royEfficientContentBasedSparse2020,
  title = {Efficient {{Content-Based Sparse Attention}} with {{Routing Transformers}}},
  author = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  date = {2020-10-24},
  eprint = {2003.05997},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/2003.05997},
  urldate = {2022-03-24},
  abstract = {Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to \$O\textbackslash left(n\^\{1.5\}d\textbackslash right)\$ from \$O\textbackslash left(n\^2d\textbackslash right)\$ for sequence length \$n\$ and hidden dimension \$d\$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\CCWVVGQH\\Roy et al_2020_Efficient Content-Based Sparse Attention with Routing Transformers.pdf;D\:\\Zotero\\storage\\UDCPH42L\\2003.html}
}

@article{ruwaMoodawareVisualQuestion2019,
  title = {Mood-Aware Visual Question Answering},
  author = {Ruwa, Nelson and Mao, Qirong and Wang, Liangjun and Gou, Jianping and Dong, Ming},
  date = {2019-02},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {330},
  pages = {305--316},
  issn = {09252312},
  doi = {10.1016/j.neucom.2018.11.049},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218313808},
  urldate = {2022-02-07},
  langid = {english},
  file = {D\:\\Zotero\\storage\\CWJPJX56\\Ruwa 等。 - 2019 - Mood-aware visual question answering.pdf}
}

@unpublished{sadhuVideoQuestionAnswering2021,
  title = {Video {{Question Answering}} with {{Phrases}} via {{Semantic Roles}}},
  author = {Sadhu, Arka and Chen, Kan and Nevatia, Ram},
  date = {2021-04-08},
  eprint = {2104.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.03762},
  urldate = {2021-09-10},
  abstract = {Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models' application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We further perform extensive analysis and ablative studies to guide future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,VideoQA},
  file = {D\:\\Zotero\\storage\\9H5KZPV9\\Sadhu et al_2021_Video Question Answering with Phrases via Semantic Roles.pdf;D\:\\Zotero\\storage\\TF943WVZ\\2104.html}
}

@unpublished{sampatCLEVRHYPChallenge2021,
  title = {{{CLEVR}}\_{{HYP}}: {{A Challenge Dataset}} and {{Baselines}} for {{Visual Question Answering}} with {{Hypothetical Actions}} over {{Images}}},
  shorttitle = {{{CLEVR}}\_{{HYP}}},
  author = {Sampat, Shailaja Keyur and Kumar, Akshay and Yang, Yezhou and Baral, Chitta},
  date = {2021-04-13},
  eprint = {2104.05981},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.05981},
  urldate = {2021-09-10},
  abstract = {Most existing research on visual question answering (VQA) is limited to information explicitly present in an image or a video. In this paper, we take visual understanding to a higher level where systems are challenged to answer questions that involve mentally simulating the hypothetical consequences of performing specific actions in a given scenario. Towards that end, we formulate a vision-language question answering task based on the CLEVR (Johnson et. al., 2017) dataset. We then modify the best existing VQA methods and propose baseline solvers for this task. Finally, we motivate the development of better vision-language models by providing insights about the capability of diverse architectures to perform joint reasoning over image-text modality. Our dataset setup scripts and codes will be made publicly available at https://github.com/shailaja183/clevr\_hyp.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Dataset},
  file = {D\:\\Zotero\\storage\\YR5QCMVM\\Sampat et al_2021_CLEVR_HYP.pdf;D\:\\Zotero\\storage\\U528G9FV\\2104.html}
}

@unpublished{samsamiDistributedDeepReinforcement2020,
  title = {Distributed {{Deep Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Distributed {{Deep Reinforcement Learning}}},
  author = {Samsami, Mohammad Reza and Alimadad, Hossein},
  date = {2020-11-22},
  eprint = {2011.11012},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.11012},
  urldate = {2022-01-18},
  abstract = {Deep reinforcement learning (DRL) is a very active research area. However, several technical and scientific issues require to be addressed, amongst which we can mention data inefficiency, exploration-exploitation trade-off, and multi-task learning. Therefore, distributed modifications of DRL were introduced; agents that could be run on many machines simultaneously. In this article, we provide a survey of the role of the distributed approaches in DRL. We overview the state of the field, by studying the key research works that have a significant impact on how we can use distributed methods in DRL. We choose to overview these papers, from the perspective of distributed learning, and not the aspect of innovations in reinforcement learning algorithms. Also, we evaluate these methods on different tasks and compare their performance with each other and with single actor and learner agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\ML5882ZE\\Samsami_Alimadad_2020_Distributed Deep Reinforcement Learning.pdf;D\:\\Zotero\\storage\\GLBCGXRM\\2011.html}
}

@inproceedings{saqurMultimodalGraphNetworks2020,
  title = {Multimodal {{Graph Networks}} for {{Compositional Generalization}} in {{Visual Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Saqur, Raeid and Narasimhan, Karthik},
  date = {2020},
  volume = {33},
  pages = {3070--3081},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/1fd6c4e41e2c6a6b092eb13ee72bce95-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Compositional generalization is a key challenge in grounding natural language to visual perception. While deep learning models have achieved great success in multimodal tasks like visual question answering, recent studies have shown that they fail to generalize to new inputs that are simply an unseen combination of those seen in the training distribution. In this paper, we propose to tackle this challenge by employing neural factor graphs to induce a tighter coupling between concepts in different modalities (e.g. images and text). Graph representations are inherently compositional in nature and allow us to capture entities, attributes and relations in a scalable manner. Our model first creates a multimodal graph, processes it with a graph neural network to induce a factor correspondence matrix, and then outputs a symbolic program to predict answers to questions. Empirically, our model achieves close to perfect scores on a caption truth prediction problem and  state-of-the-art results on the recently introduced CLOSURE dataset, improving on the mean overall accuracy across seven compositional templates by 4.77\textbackslash\% over previous approaches.},
  file = {D\:\\Zotero\\storage\\ZEWI6MU6\\Saqur_Narasimhan_2020_Multimodal Graph Networks for Compositional Generalization in Visual Question.pdf}
}

@inproceedings{saqurMultimodalGraphNetworks2020a,
  title = {Multimodal {{Graph Networks}} for {{Compositional Generalization}} in {{Visual Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Saqur, Raeid and Narasimhan, Karthik},
  date = {2020},
  volume = {33},
  pages = {3070--3081},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/1fd6c4e41e2c6a6b092eb13ee72bce95-Abstract.html},
  urldate = {2021-09-10},
  keywords = {Graph},
  file = {D\:\\Zotero\\storage\\VFIWUHFN\\Saqur_Narasimhan_2020_Multimodal Graph Networks for Compositional Generalization in Visual Question.pdf}
}

@online{SceneTextDetection,
  title = {Scene Text Detection and Object Detection Difference - {{Google}} 搜索},
  url = {https://www.google.com/search?q=scene+text+detection+and+object+detection+difference&rlz=1C1CHWL_zh-CNHK985JP986&ei=6i9IYq72K5nR2roP_-Cw0A8&ved=0ahUKEwiuk53pn_X2AhWZqFYBHX8wDPoQ4dUDCA4&uact=5&oq=scene+text+detection+and+object+detection+difference&gs_lcp=Cgdnd3Mtd2l6EAM6BwgAEEcQsAM6BQghEKABOgcIIRAKEKABSgQIQRgASgQIRhgAUPcDWIYpYMIqaAJwAXgBgAGDBYgByRqSAQsxLjcuMi4yLjEuMZgBAKABAcgBCsABAQ&sclient=gws-wiz},
  urldate = {2022-04-02},
  file = {D\:\\Zotero\\storage\\J7PGUUG7\\search.html}
}

@unpublished{schwartzEnsembleMRRNDCG2021,
  title = {Ensemble of {{MRR}} and {{NDCG}} Models for {{Visual Dialog}}},
  author = {Schwartz, Idan},
  date = {2021-08-04},
  eprint = {2104.07511},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.07511},
  urldate = {2021-09-10},
  abstract = {Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as `I don't know. Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41\% vs. 71.24\%) and the NDCG state-of-the-art performance (72.16\% vs. 75.35\%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning,Visual Dialog},
  file = {D\:\\Zotero\\storage\\7LWN3Y9Q\\Schwartz_2021_Ensemble of MRR and NDCG models for Visual Dialog.pdf;D\:\\Zotero\\storage\\WYAJH5HJ\\2104.html}
}

@unpublished{schwartzEnsembleMRRNDCG2021a,
  title = {Ensemble of {{MRR}} and {{NDCG}} Models for {{Visual Dialog}}},
  author = {Schwartz, Idan},
  date = {2021-08-04},
  eprint = {2104.07511},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.07511},
  urldate = {2021-09-10},
  abstract = {Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as `I don't know. Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41\% vs. 71.24\%) and the NDCG state-of-the-art performance (72.16\% vs. 75.35\%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\G9VZV52N\\Schwartz_2021_Ensemble of MRR and NDCG models for Visual Dialog.pdf;D\:\\Zotero\\storage\\DC9KLA57\\2104.html}
}

@article{selvarajuGradCAMVisualExplanations,
  title = {Grad-{{CAM}}: {{Visual Explanations From Deep Networks}} via {{Gradient-Based Localization}}},
  author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  pages = {9},
  abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BFLILU82\\Selvaraju 等。 - Grad-CAM Visual Explanations From Deep Networks v.pdf}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017,
  title = {Grad-{{CAM}}: {{Visual Explanations From Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2017},
  pages = {618--626},
  url = {https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html},
  urldate = {2021-09-11},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  file = {D\:\\Zotero\\storage\\WI97VBVX\\Selvaraju et al_2017_Grad-CAM.pdf;D\:\\Zotero\\storage\\72QXTNU3\\Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html}
}

@inproceedings{selvarajuSQuINTingVQAModels2020,
  title = {{{SQuINTing}} at {{VQA Models}}: {{Introspecting VQA Models With Sub-Questions}}},
  shorttitle = {{{SQuINTing}} at {{VQA Models}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Selvaraju, Ramprasaath R. and Tendulkar, Purva and Parikh, Devi and Horvitz, Eric and Tulio Ribeiro, Marco and Nushi, Besmira and Kamar, Ece},
  date = {2020-06},
  pages = {10000--10008},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01002},
  url = {https://ieeexplore.ieee.org/document/9156978/},
  urldate = {2021-09-09},
  abstract = {Existing VQA datasets contain questions with varying levels of complexity. While the majority of questions in these datasets require perception for recognizing existence, properties, and spatial relationships of entities, a significant portion of questions pose challenges that correspond to reasoning tasks – tasks that can only be answered through a synthesis of perception and knowledge about the world, logic and / or reasoning. Analyzing performance across this distinction allows us to notice when existing VQA models have consistency issues – they answer the reasoning questions correctly but fail on associated low-level perception questions. For example, in Figure 1, models answer the complex reasoning question “Is the banana ripe enough to eat?” correctly, but fail on the associated perception question “Are the bananas mostly green or yellow?” indicating that the model likely answered the reasoning question correctly but for the wrong reason. We quantify the extent to which this phenomenon occurs by creating a new Reasoning split of the VQA dataset and collecting VQAintrospect, a new dataset1 which currently consists of 200K new perception questions which serve as sub questions corresponding to the set of perceptual tasks needed to effectively answer the complex reasoning questions in the Reasoning split. Our evaluation shows that state-of-the-art VQA models have comparable performance in answering perception and reasoning questions, but suffer from consistency problems. To address this shortcoming, we propose an approach called Sub-Question Importance-aware Network Tuning (SQuINT), which encourages the model to attend to the same parts of the image when answering the reasoning question and the perception sub question.We show that SQuINT improves model consistency by ∼7\%, also marginally improving performance on the Reasoning questions in VQA, while also displaying better attention maps.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {2 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\H5QDS2UJ\\Selvaraju 等。 - 2020 - SQuINTing at VQA Models Introspecting VQA Models .pdf}
}

@inproceedings{shahCycleConsistencyRobustVisual2019,
  title = {Cycle-{{Consistency}} for {{Robust Visual Question Answering}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shah, Meet and Chen, Xinlei and Rohrbach, Marcus and Parikh, Devi},
  date = {2019-06},
  pages = {6642--6651},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00681},
  url = {https://ieeexplore.ieee.org/document/8954214/},
  urldate = {2021-09-09},
  abstract = {Despite significant progress in Visual Question Answering over the years, robustness of today’s VQA models leave much to be desired. We introduce a new evaluation protocol and associated dataset (VQA-Rephrasings) and show that state-of-the-art VQA models are notoriously brittle to linguistic variations in questions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k questions spanning 40k images from the VQA v2.0 validation dataset. As a step towards improving robustness of VQA models, we propose a model-agnostic framework that exploits cycle consistency. Specifically, we train a model to not only answer a question, but also generate a question conditioned on the answer, such that the answer predicted for the generated question is the same as the ground truth answer to the original question. Without the use of additional annotations, we show that our approach is significantly more robust to linguistic variations than state-of-the-art VQA models, when evaluated on the VQA-Rephrasings dataset. In addition, our approach outperforms state-of-the-art approaches on the standard VQA and Visual Question Generation tasks on the challenging VQA v2.0 dataset.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  keywords = {Robust},
  annotation = {20 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\SRZHTBCZ\\Shah 等。 - 2019 - Cycle-Consistency for Robust Visual Question Answe.pdf}
}

@inproceedings{shaLearningAnswerEmbeddings2018,
  title = {Learning {{Answer Embeddings}} for {{Visual Question Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sha, Fei and Chao, Wei-Lun and Hu, Hexiang},
  date = {2018-06},
  pages = {5428--5436},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00569},
  url = {https://ieeexplore.ieee.org/document/8578667/},
  urldate = {2021-09-09},
  abstract = {We propose a novel probabilistic model for visual question answering (Visual QA). The key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers. The learning objective is to learn the best parameterization of those embeddings such that the correct answer has higher likelihood among all possible answers. In contrast to several existing approaches of treating Visual QA as multi-way classification, the proposed approach takes the semantic relationships (as characterized by the embeddings) among answers into consideration, instead of viewing them as independent ordinal numbers. Thus, the learned embedded function can be used to embed unseen answers (in the training dataset). These properties make the approach particularly appealing for transfer learning for open-ended Visual QA, where the source dataset on which the model is learned has limited overlapping with the target dataset in the space of answers. We have also developed large-scale optimization techniques for applying the model to datasets with a large number of answers, where the challenge is to properly normalize the proposed probabilistic models. We validate our approach on several Visual QA datasets and investigate its utility for transferring models across datasets. The empirical results have shown that the approach performs well not only on in-domain learning but also on transfer learning.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {1 citations (Crossref) [2021-09-10] SC: 0000000[s0]},
  file = {D\:\\Zotero\\storage\\IHPPZYK8\\Hu_Learning_Answer_Embeddings_CVPR_2018_paper.pdf}
}

@unpublished{shaoSAHDLSparseAttention2020,
  title = {{{SAHDL}}: {{Sparse Attention Hypergraph Regularized Dictionary Learning}}},
  shorttitle = {{{SAHDL}}},
  author = {Shao, Shuai and Xu, Rui and Wang, Yan-Jiang and Liu, Weifeng and Liu, Bao-Di},
  date = {2020-10-23},
  eprint = {2010.12416},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.12416},
  urldate = {2022-02-10},
  abstract = {In recent years, the attention mechanism contributes significantly to hypergraph based neural networks. However, these methods update the attention weights with the network propagating. That is to say, this type of attention mechanism is only suitable for deep learning-based methods while not applicable to the traditional machine learning approaches. In this paper, we propose a hypergraph based sparse attention mechanism to tackle this issue and embed it into dictionary learning. More specifically, we first construct a sparse attention hypergraph, asset attention weights to samples by employing the \$\textbackslash ell\_1\$-norm sparse regularization to mine the high-order relationship among sample features. Then, we introduce the hypergraph Laplacian operator to preserve the local structure for subspace transformation in dictionary learning. Besides, we incorporate the discriminative information into the hypergraph as the guidance to aggregate samples. Unlike previous works, our method updates attention weights independently, does not rely on the deep network. We demonstrate the efficacy of our approach on four benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\XLXL7QEJ\\Shao et al_2020_SAHDL.pdf;D\:\\Zotero\\storage\\TKJMJPFG\\2010.html}
}

@unpublished{sharifzadehImprovingVisualReasoning2021,
  title = {Improving {{Visual Reasoning}} by {{Exploiting The Knowledge}} in {{Texts}}},
  author = {Sharifzadeh, Sahand and Baharlou, Sina Moayed and Schmitt, Martin and Schütze, Hinrich and Tresp, Volker},
  date = {2021-02-09},
  eprint = {2102.04760},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.04760},
  urldate = {2021-09-26},
  abstract = {This paper presents a new framework for training image-based classifiers from a combination of texts and images with very few labels. We consider a classification framework with three modules: a backbone, a relational reasoning component, and a classification component. While the backbone can be trained from unlabeled images by self-supervised learning, we can fine-tune the relational reasoning and the classification components from external sources of knowledge instead of annotated images. By proposing a transformer-based model that creates structured knowledge from textual input, we enable the utilization of the knowledge in texts. We show that, compared to the supervised baselines with 1\% of the annotated images, we can achieve \textasciitilde 8x more accurate results in scene graph classification, \textasciitilde 3x in object classification, and \textasciitilde 1.5x in predicate classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\F2GU8V53\\Sharifzadeh et al_2021_Improving Visual Reasoning by Exploiting The Knowledge in Texts.pdf;D\:\\Zotero\\storage\\DYXJFJZQ\\2102.html}
}

@unpublished{sharmaCATSETMAT2021,
  title = {The {{CAT SET}} on the {{MAT}}: {{Cross Attention}} for {{Set Matching}} in {{Bipartite Hypergraphs}}},
  shorttitle = {The {{CAT SET}} on the {{MAT}}},
  author = {Sharma, Govind and Singh, Swyam Prakash and Devi, V. Susheela and Murty, M. Narasimha},
  date = {2021-10-30},
  eprint = {2111.00243},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.00243},
  urldate = {2022-03-27},
  abstract = {Usual relations between entities could be captured using graphs; but those of a higher-order -- more so between two different types of entities (which we term "left" and "right") -- calls for a "bipartite hypergraph". For example, given a left set of symptoms and right set of diseases, the relation between a set subset of symptoms (that a patient experiences at a given point of time) and a subset of diseases (that he/she might be diagnosed with) could be well-represented using a bipartite hyperedge. The state-of-the-art in embedding nodes of a hypergraph is based on learning the self-attention structure between node-pairs from a hyperedge. In the present work, given a bipartite hypergraph, we aim at capturing relations between node pairs from the cross-product between the left and right hyperedges, and term it a "cross-attention" (CAT) based model. More precisely, we pose "bipartite hyperedge link prediction" as a set-matching (SETMAT) problem and propose a novel neural network architecture called CATSETMAT for the same. We perform extensive experiments on multiple bipartite hypergraph datasets to show the superior performance of CATSETMAT, which we compare with multiple techniques from the state-of-the-art. Our results also elucidate information flow in self- and cross-attention scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {D\:\\Zotero\\storage\\LUPGJPM8\\Sharma et al_2021_The CAT SET on the MAT.pdf;D\:\\Zotero\\storage\\ZRL7ZNTS\\2111.html}
}

@article{sharmaFrameworkVisualQuestion2022,
  title = {A Framework for Visual Question Answering with the Integration of Scene-Text Using {{PHOCs}} and Fisher Vectors},
  author = {Sharma, Himanshu and Singh Jalal, Anand},
  date = {2022-03},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {190},
  pages = {116159},
  issn = {09574174},
  doi = {10.1016/j.eswa.2021.116159},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421014822},
  urldate = {2022-03-12},
  abstract = {Text contained in an image gives useful information about that image. Consider a warning signboard with text “high voltage”; it indicates the hazard or risk involved in the image. Thus, this semantic textual information can be very useful for better understanding of images, which is not utilized by the existing visual question answering (VQA) models. However, the presence of this textual information in images can strongly guide the VQA task. This work deal with the task of visual question answering by exploiting these textual cues together with the visual content to boost the accuracy of VQA models. In the work, a novel VQA model is proposed based on the PHOC and fisher vector based representation. Based on the PHOCs of the scene-text, we have constructed a powerful descriptor by using a Fisher Vectors. Also, the proposed model uses transformer model together with dynamic pointer networks for answer decoding process. Thus, the proposed model uses a sequence of decoding steps for answer generation instead of just assuming answer prediction as a classification problem as considered by previous works. We have shown the qualitative and quantitative results on three popular datasets: VQA 2.0, TextVQA and ST-VQA. The results show the effectiveness of the proposed model over the existing models.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\AENYSNST\\Sharma 和 Singh Jalal - 2022 - A framework for visual question answering with the.pdf}
}

@article{shengHumanAdversarialVisualQuestion,
  title = {Human-{{Adversarial Visual Question Answering}}},
  author = {Sheng, Sasha and Singh, Amanpreet and Goswami, Vedanuj and Magana, Jose Alberto Lopez and Thrush, Tristan and Galuba, Wojciech and Parikh, Devi and Kiela, Douwe},
  pages = {14},
  abstract = {Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model’s predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\S6FF8C54\\Sheng 等。 - Human-Adversarial Visual Question Answering.pdf}
}

@inproceedings{shengHumanAdversarialVisualQuestion2021,
  title = {Human-{{Adversarial Visual Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sheng, Sasha and Singh, Amanpreet and Goswami, Vedanuj and Magana, Jose and Thrush, Tristan and Galuba, Wojciech and Parikh, Devi and Kiela, Douwe},
  date = {2021},
  volume = {34},
  pages = {20346--20359},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2021/hash/aa97d584861474f4097cf13ccb5325da-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model’s predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.},
  file = {D\:\\Zotero\\storage\\ISVENHHE\\Sheng et al_2021_Human-Adversarial Visual Question Answering.pdf}
}

@unpublished{shengHumanAdversarialVisualQuestion2021a,
  title = {Human-{{Adversarial Visual Question Answering}}},
  author = {Sheng, Sasha and Singh, Amanpreet and Goswami, Vedanuj and Magana, Jose Alberto Lopez and Galuba, Wojciech and Parikh, Devi and Kiela, Douwe},
  date = {2021-06-04},
  eprint = {2106.02280},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.02280},
  urldate = {2021-08-23},
  abstract = {Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,human},
  file = {D\:\\Zotero\\storage\\C2BW4BUT\\Sheng et al_2021_Human-Adversarial Visual Question Answering.pdf;D\:\\Zotero\\storage\\I6NJTHBE\\2106.html}
}

@unpublished{shenHowMuchCan2021,
  title = {How {{Much Can CLIP Benefit Vision-and-Language Tasks}}?},
  author = {Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
  date = {2021-07-13},
  eprint = {2107.06383},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.06383},
  urldate = {2021-09-23},
  abstract = {Most existing Vision-and-Language (V\&L) models rely on pre-trained visual encoders, using a relatively small set of manually-annotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image Pre-training), trained on a massive amount of image-caption pairs, has shown a strong zero-shot capability on various vision tasks. To further study the advantage brought by CLIP, we propose to use CLIP as the visual encoder in various V\&L models in two typical scenarios: 1) plugging CLIP into task-specific fine-tuning; 2) combining CLIP with V\&L pre-training and transferring to downstream tasks. We show that CLIP significantly outperforms widely-used visual encoders trained with in-domain annotated data, such as BottomUp-TopDown. We achieve competitive or better results on diverse V\&L tasks, while establishing new state-of-the-art results on Visual Question Answering, Visual Entailment, and V\&L Navigation tasks. We release our code at https://github.com/clip-vil/CLIP-ViL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\CW5TAC76\\Shen et al_2021_How Much Can CLIP Benefit Vision-and-Language Tasks.pdf;D\:\\Zotero\\storage\\HDQL8HM5\\2107.html}
}

@unpublished{shiExplainableExplicitVisual2019,
  title = {Explainable and {{Explicit Visual Reasoning}} over {{Scene Graphs}}},
  author = {Shi, Jiaxin and Zhang, Hanwang and Li, Juanzi},
  date = {2019-03-19},
  eprint = {1812.01855},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1812.01855},
  urldate = {2021-09-09},
  abstract = {We aim to dismantle the prevalent black-box neural architectures used in complex visual reasoning tasks, into the proposed eXplainable and eXplicit Neural Modules (XNMs), which advance beyond existing neural module networks towards using scene graphs --- objects as nodes and the pairwise relationships as edges --- for explainable and explicit reasoning with structured knowledge. XNMs allow us to pay more attention to teach machines how to "think", regardless of what they "look". As we will show in the paper, by using scene graphs as an inductive bias, 1) we can design XNMs in a concise and flexible fashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce the number of parameters by 10 to 100 times, and 2) we can explicitly trace the reasoning-flow in terms of graph attentions. XNMs are so generic that they support a wide range of scene graph implementations with various qualities. For example, when the graphs are detected perfectly, XNMs achieve 100\% accuracy on both CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound for visual reasoning; when the graphs are noisily detected from real-world images, XNMs are still robust to achieve a competitive 67.5\% accuracy on VQAv2.0, surpassing the popular bag-of-objects attention models without graph structures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\QTNE7G7Y\\Shi et al_2019_Explainable and Explicit Visual Reasoning over Scene Graphs.pdf;D\:\\Zotero\\storage\\SQIA3PSJ\\1812.html}
}

@inproceedings{shihWhereLookFocus2016,
  title = {Where to {{Look}}: {{Focus Regions}} for {{Visual Question Answering}}},
  shorttitle = {Where to {{Look}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shih, Kevin J. and Singh, Saurabh and Hoiem, Derek},
  date = {2016-06},
  pages = {4613--4621},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.499},
  url = {http://ieeexplore.ieee.org/document/7780868/},
  urldate = {2021-03-11},
  abstract = {We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method maps textual queries and visual features from various regions into a shared space where they are compared for relevance with an inner product. Our method exhibits significant improvements in answering questions such as “what color,” where it is necessary to evaluate a specific location, and “what room,” where it selectively identifies informative image regions. Our model is tested on the recently released VQA [1] dataset, which features free-form human-annotated questions and answers.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\UMCE9C5N\\Shih 等。 - 2016 - Where to Look Focus Regions for Visual Question A.pdf}
}

@unpublished{shiMaskedLabelPrediction2021,
  title = {Masked {{Label Prediction}}: {{Unified Message Passing Model}} for {{Semi-Supervised Classification}}},
  shorttitle = {Masked {{Label Prediction}}},
  author = {Shi, Yunsheng and Huang, Zhengjie and Feng, Shikun and Zhong, Hui and Wang, Wenjin and Sun, Yu},
  date = {2021-05-09},
  eprint = {2009.03509},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.03509},
  urldate = {2022-03-26},
  abstract = {Graph neural network (GNN) and label propagation algorithm (LPA) are both message passing algorithms, which have achieved superior performance in semi-supervised classification. GNN performs feature propagation by a neural network to make predictions, while LPA uses label propagation across graph adjacency matrix to get results. However, there is still no effective way to directly combine these two kinds of algorithms. To address this issue, we propose a novel Unified Message Passaging Model (UniMP) that can incorporate feature and label propagation at both training and inference time. First, UniMP adopts a Graph Transformer network, taking feature embedding and label embedding as input information for propagation. Second, to train the network without overfitting in self-loop input label information, UniMP introduces a masked label prediction strategy, in which some percentage of input label information are masked at random, and then predicted. UniMP conceptually unifies feature propagation and label propagation and is empirically powerful. It obtains new state-of-the-art semi-supervised classification results in Open Graph Benchmark (OGB).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\S543Z72W\\Shi et al_2021_Masked Label Prediction.pdf;D\:\\Zotero\\storage\\UFJYFTEC\\2009.html}
}

@inproceedings{shinCustomizedImageNarrative2018,
  title = {Customized {{Image Narrative Generation}} via {{Interactive Visual Question Generation}} and {{Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shin, Andrew and Ushiku, Yoshitaka and Harada, Tatsuya},
  date = {2018-06},
  pages = {8925--8933},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00930},
  url = {https://ieeexplore.ieee.org/document/8579028/},
  urldate = {2021-09-09},
  abstract = {Image description task has been invariably examined in a static manner with qualitative presumptions held to be universally applicable, regardless of the scope or target of the description. In practice, however, different viewers may pay attention to different aspects of the image, and yield different descriptions or interpretations under various contexts. Such diversity in perspectives is difficult to derive with conventional image description techniques. In this paper, we propose a customized image narrative generation task, in which the users are interactively engaged in the generation process by providing answers to the questions. We further attempt to learn the user’s interest via repeating such interactive stages, and to automatically reflect the interest in descriptions for new images. Experimental results demonstrate that our model can generate a variety of descriptions from single image that cover a wider range of topics than conventional models, while being customizable to the target user of interaction.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {2 citations (Crossref) [2021-09-10] ZSCC: 0000000[s1]},
  file = {D\:\\Zotero\\storage\\PSAR3FRU\\Shin 等。 - 2018 - Customized Image Narrative Generation via Interact.pdf}
}

@incollection{shiQuestionTypeGuided2018,
  title = {Question {{Type Guided Attention}} in {{Visual Question Answering}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Shi, Yang and Furlanello, Tommaso and Zha, Sheng and Anandkumar, Animashree},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11208},
  pages = {158--175},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01225-0_10},
  url = {http://link.springer.com/10.1007/978-3-030-01225-0_10},
  urldate = {2021-09-10},
  abstract = {Visual Question Answering (VQA) requires integration of feature maps with drastically different structures. Image descriptors have structures at multiple spatial scales, while lexical inputs inherently follow a temporal sequence and naturally cluster into semantically different question types. A lot of previous works use complex models to extract feature representations but neglect to use high-level information summary such as question types in learning. In this work, we propose Question Type-guided Attention (QTA). It utilizes the information of question type to dynamically balance between bottom-up and top-down visual features, respectively extracted from ResNet and Faster R-CNN networks. We experiment with multiple VQA architectures with extensive input ablation studies over the TDIUC dataset and show that QTA systematically improves the performance by more than 5\% across multiple question type categories such as “Activity Recognition”, “Utility” and “Counting” on TDIUC dataset compared to the state-of-art. By adding QTA on the state-of-art model MCB, we achieve 3\% improvement in overall accuracy. Finally, we propose a multi-task extension to predict question types which generalizes QTA to applications that lack question type, with a minimal performance loss.},
  isbn = {978-3-030-01224-3 978-3-030-01225-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\8I4Q4LWZ\\Shi 等。 - 2018 - Question Type Guided Attention in Visual Question .pdf}
}

@inproceedings{shresthaAnswerThemAll2019,
  title = {Answer {{Them All}}! {{Toward Universal Visual Question Answering Models}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  date = {2019-06},
  pages = {10464--10473},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01072},
  url = {https://ieeexplore.ieee.org/document/8953399/},
  urldate = {2021-09-09},
  abstract = {Visual Question Answering (VQA) research is split into two camps: the first focuses on VQA datasets that require natural image understanding and the second focuses on synthetic datasets that test reasoning. A good VQA algorithm should be capable of both, but only a few VQA algorithms are tested in this manner. We compare five state-ofthe-art VQA algorithms across eight VQA datasets covering both domains. To make the comparison fair, all of the models are standardized as much as possible, e.g., they use the same visual features, answer vocabularies, etc. We find that methods do not generalize across the two domains. To address this problem, we propose a new VQA algorithm that rivals or exceeds the state-of-the-art for both domains.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {22 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\2BRC4L7A\\Shrestha 等。 - 2019 - Answer Them All! Toward Universal Visual Question .pdf}
}

@inproceedings{shresthaAnswerThemAll2019a,
  title = {Answer {{Them All}}! {{Toward Universal Visual Question Answering Models}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  date = {2019-06},
  pages = {10464--10473},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01072},
  url = {https://ieeexplore.ieee.org/document/8953399/},
  urldate = {2021-03-11},
  abstract = {Visual Question Answering (VQA) research is split into two camps: the first focuses on VQA datasets that require natural image understanding and the second focuses on synthetic datasets that test reasoning. A good VQA algorithm should be capable of both, but only a few VQA algorithms are tested in this manner. We compare five state-ofthe-art VQA algorithms across eight VQA datasets covering both domains. To make the comparison fair, all of the models are standardized as much as possible, e.g., they use the same visual features, answer vocabularies, etc. We find that methods do not generalize across the two domains. To address this problem, we propose a new VQA algorithm that rivals or exceeds the state-of-the-art for both domains.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\RRVZ8GHM\\Shrestha 等。 - 2019 - Answer Them All! Toward Universal Visual Question .pdf}
}

@unpublished{shresthaNegativeCaseAnalysis2020,
  title = {A Negative Case Analysis of Visual Grounding Methods for {{VQA}}},
  author = {Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  date = {2020-04-15},
  eprint = {2004.05704},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.05704},
  urldate = {2021-09-09},
  abstract = {Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements. Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,visual grounding},
  file = {D\:\\Zotero\\storage\\KJNRTJAX\\Shrestha et al_2020_A negative case analysis of visual grounding methods for VQA.pdf;D\:\\Zotero\\storage\\CAV7E34C\\2004.html}
}

@inproceedings{shuBiGTransformerIntegratingHierarchical2020,
  title = {{{BiG-Transformer}}: {{Integrating Hierarchical Features}} for {{Transformer}} via {{Bipartite Graph}}},
  shorttitle = {{{BiG-Transformer}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Shu, Xiaobo and Xue, Mengge and Li, Yanzeng and Zhang, Zhenyu and Liu, Tingwen},
  date = {2020-07},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Glasgow, United Kingdom}},
  doi = {10.1109/IJCNN48605.2020.9207132},
  url = {https://ieeexplore.ieee.org/document/9207132/},
  urldate = {2022-03-26},
  abstract = {Self-attention based models like Transformer have achieved great success on kinds of Natural Language Processing tasks. However, the traditional fixed fully-connected structure faces many challenges in practice, such as computing redundancy, fixed granularity, and inexplicable. In this paper, we present BiG-Transformer, which employs attention with bipartite-graph structure to replace the fully-connected self-attention mechanism in Transformer. Specifically, two parts of the graph are designed for integrating hierarchical semantic information, and two types of connection are proposed to fuse information from different positions. Experiments on four tasks show the BiG-Transformer achieves better performance compared to Transformer liked models and Recurrent Neural Networks.},
  eventtitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  isbn = {978-1-72816-926-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\YZUUREM3\\Shu 等。 - 2020 - BiG-Transformer Integrating Hierarchical Features.pdf}
}

@inproceedings{siCheckItAgain2021,
  title = {Check {{It Again}}:{{Progressive Visual Question Answering}} via {{Visual Entailment}}},
  shorttitle = {Check {{It Again}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Si, Qingyi and Lin, Zheng and yu Zheng, Ming and Fu, Peng and Wang, Weiping},
  date = {2021-08},
  pages = {4101--4110},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-long.317},
  url = {https://aclanthology.org/2021.acl-long.317},
  urldate = {2021-09-09},
  abstract = {While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these models tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment. Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55\% improvement.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  keywords = {Entailment},
  file = {D\:\\Zotero\\storage\\WIX99SFX\\Si et al_2021_Check It Again.pdf}
}

@inproceedings{singhStringsThingsKnowledgeEnabled2019,
  title = {From {{Strings}} to {{Things}}: {{Knowledge-Enabled VQA Model That Can Read}} and {{Reason}}},
  shorttitle = {From {{Strings}} to {{Things}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Singh, Ajeet Kumar and Mishra, Anand and Shekhar, Shashank and Chakraborty, Anirban},
  date = {2019-10},
  pages = {4601--4611},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00470},
  url = {https://ieeexplore.ieee.org/document/9010987/},
  urldate = {2021-12-08},
  abstract = {Text present in images are not merely strings, they provide useful cues about the image. Despite their utility in better image understanding, scene texts are not used in traditional visual question answering (VQA) models. In this work, we present a VQA model which can read scene texts and perform reasoning on a knowledge graph to arrive at an accurate answer. Our proposed model has three mutually interacting modules: (i) proposal module to get word and visual content proposals from the image, (ii) fusion module to fuse these proposals, question and knowledge base to mine relevant facts, and represent these facts as multi-relational graph, (iii) reasoning module to perform a novel gated graph neural network based reasoning on this graph.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  langid = {english}
}

@unpublished{singhTextOCRLargescaleEndtoend2021,
  title = {{{TextOCR}}: {{Towards}} Large-Scale End-to-End Reasoning for Arbitrary-Shaped Scene Text},
  shorttitle = {{{TextOCR}}},
  author = {Singh, Amanpreet and Pang, Guan and Toh, Mandy and Huang, Jing and Galuba, Wojciech and Hassner, Tal},
  date = {2021-05-12},
  eprint = {2105.05486},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.05486},
  urldate = {2021-12-10},
  abstract = {A crucial component for the scene text based reasoning required for TextVQA and TextCaps datasets involve detecting and recognizing text present in the images using an optical character recognition (OCR) system. The current systems are crippled by the unavailability of ground truth text annotations for these datasets as well as lack of scene text detection and recognition datasets on real images disallowing the progress in the field of OCR and evaluation of scene text based reasoning in isolation from OCR systems. In this work, we propose TextOCR, an arbitrary-shaped scene text detection and recognition with 900k annotated words collected on real images from TextVQA dataset. We show that current state-of-the-art text-recognition (OCR) models fail to perform well on TextOCR and that training on TextOCR helps achieve state-of-the-art performance on multiple other OCR datasets as well. We use a TextOCR trained OCR model to create PixelM4C model which can do scene text based reasoning on an image in an end-to-end fashion, allowing us to revisit several design choices to achieve new state-of-the-art performance on TextVQA dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\2PCLLM4C\\Singh et al_2021_TextOCR.pdf;D\:\\Zotero\\storage\\PE45RDSA\\2105.html}
}

@unpublished{singhVQAModelsThat2019,
  title = {Towards {{VQA Models That Can Read}}},
  author = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  date = {2019-05-13},
  eprint = {1904.08920},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.08920},
  urldate = {2021-12-08},
  abstract = {Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But today's VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new "TextVQA" dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason \& Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\PUXPREY5\\Singh et al_2019_Towards VQA Models That Can Read.pdf;D\:\\Zotero\\storage\\IW8GBEDJ\\1904.html}
}

@inproceedings{songCLIPModelsAre2022,
  title = {{{CLIP Models}} Are {{Few-Shot Learners}}: {{Empirical Studies}} on {{VQA}} and {{Visual Entailment}}},
  shorttitle = {{{CLIP Models}} Are {{Few-Shot Learners}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Song, Haoyu and Dong, Li and Zhang, Weinan and Liu, Ting and Wei, Furu},
  date = {2022},
  pages = {6088--6100},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.421},
  url = {https://aclanthology.org/2022.acl-long.421},
  urldate = {2022-07-06},
  abstract = {CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks. Previously, CLIP is only regarded as a powerful visual encoder. However, after being pre-trained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks. In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language. We first evaluate CLIP's zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task. Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task. We achieve competitive zero/few-shot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.},
  eventtitle = {{{ACL}} 2022},
  file = {D\:\\Zotero\\storage\\E5RSIYVK\\Song et al_2022_CLIP Models are Few-Shot Learners.pdf}
}

@unpublished{songExploringGraphstructuredPassage2018,
  title = {Exploring {{Graph-structured Passage Representation}} for {{Multi-hop Reading Comprehension}} with {{Graph Neural Networks}}},
  author = {Song, Linfeng and Wang, Zhiguo and Yu, Mo and Zhang, Yue and Florian, Radu and Gildea, Daniel},
  date = {2018-09-06},
  eprint = {1809.02040},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1809.02040},
  urldate = {2021-10-10},
  abstract = {Multi-hop reading comprehension focuses on one type of factoid question, where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Previous work approximates global evidence with local coreference information, encoding coreference chains with DAG-styled GRU layers within a gated-attention reader. However, coreference is limited in providing information for rich inference. We introduce a new method for better connecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate two recent graph neural networks, namely graph convolutional network (GCN) and graph recurrent network (GRN). Experiments on two standard datasets show that richer global information leads to better answers. Our method performs better than all published results on these datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\SCZGL4PL\\Song et al_2018_Exploring Graph-structured Passage Representation for Multi-hop Reading.pdf;D\:\\Zotero\\storage\\W75Q42QQ\\1809.html}
}

@misc{songVisionLanguagePreTrainingBoosting2022,
  title = {Vision-{{Language Pre-Training}} for {{Boosting Scene Text Detectors}}},
  author = {Song, Sibo and Wan, Jianqiang and Yang, Zhibo and Tang, Jun and Cheng, Wenqing and Bai, Xiang and Yao, Cong},
  date = {2022-04-28},
  number = {arXiv:2204.13867},
  eprint = {2204.13867},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.13867},
  urldate = {2022-07-19},
  abstract = {Recently, vision-language joint representation learning has proven to be highly effective in various scenarios. In this paper, we specifically adapt vision-language joint learning for scene text detection, a task that intrinsically involves cross-modal interaction between the two modalities: vision and language, since text is the written form of language. Concretely, we propose to learn contextualized, joint representations through vision-language pre-training, for the sake of enhancing the performance of scene text detectors. Towards this end, we devise a pre-training architecture with an image encoder, a text encoder and a cross-modal encoder, as well as three pretext tasks: image-text contrastive learning (ITC), masked language modeling (MLM) and word-in-image prediction (WIP). The pre-trained model is able to produce more informative representations with richer semantics, which could readily benefit existing scene text detectors (such as EAST and PSENet) in the down-stream text detection task. Extensive experiments on standard benchmarks demonstrate that the proposed paradigm can significantly improve the performance of various representative text detectors, outperforming previous pre-training approaches. The code and pre-trained models will be publicly released.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\6KP4FKTM\\Song et al_2022_Vision-Language Pre-Training for Boosting Scene Text Detectors.pdf;D\:\\Zotero\\storage\\KNVPU4IF\\2204.html}
}

@unpublished{stichLocalSGDConverges2019,
  title = {Local {{SGD Converges Fast}} and {{Communicates Little}}},
  author = {Stich, Sebastian U.},
  date = {2019-05-03},
  eprint = {1805.09767},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/1805.09767},
  urldate = {2022-01-20},
  abstract = {Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speedup with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits. To overcome this communication bottleneck recent works propose to reduce the communication frequency. An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while. This scheme shows promising results in practice, but eluded thorough theoretical analysis. We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speedup in the number of workers and mini-batch size. The number of communication rounds can be reduced up to a factor of T\^\{1/2\}---where T denotes the number of total steps---compared to mini-batch SGD. This also holds for asynchronous implementations. Local SGD can also be used for large scale training of deep learning models. The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.},
  archiveprefix = {arXiv},
  keywords = {90C06; 68W40; 68W10,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,F.2.1,G.1.6,Mathematics - Optimization and Control},
  file = {D\:\\Zotero\\storage\\97T3KKPA\\Stich_2019_Local SGD Converges Fast and Communicates Little.pdf;D\:\\Zotero\\storage\\YUKKYBVF\\1805.html}
}

@inproceedings{suiCausalAttentionInterpretable2022,
  title = {Causal {{Attention}} for {{Interpretable}} and {{Generalizable Graph Classification}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Sui, Yongduo and Wang, Xiang and Wu, Jiancan and Lin, Min and He, Xiangnan and Chua, Tat-Seng},
  date = {2022-08-14},
  eprint = {2112.15089},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1696--1705},
  doi = {10.1145/3534678.3539366},
  url = {http://arxiv.org/abs/2112.15089},
  urldate = {2022-10-10},
  abstract = {In graph classification, attention- and pooling-based graph neural networks (GNNs) prevail to extract the critical features from the input graph and support the prediction. They mostly follow the paradigm of “learning to attend”, which maximizes the mutual information between the attended graph and the ground-truth label. However, this paradigm makes GNN classifiers recklessly absorb all the statistical correlations between input features and labels in the training data, without distinguishing the causal and noncausal effects of features. Instead of underscoring the causal features, the attended graphs are prone to visit the noncausal features as the shortcut to predictions. Such shortcut features might easily change outside the training distribution, thereby making the GNN classifiers suffer from poor generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\NAN8T7ZI\\Sui 等。 - 2022 - Causal Attention for Interpretable and Generalizab.pdf}
}

@misc{suiyixiNiXiangGongChengZhongQuXianQuMianTeZhengTiQuYanJiu2008,
  title = {逆向工程中曲线曲面特征提取研究},
  author = {{隋亦熙}},
  date = {2008},
  publisher = {{浙江大学}}
}

@inproceedings{suLearningVisualKnowledge2018,
  title = {Learning {{Visual Knowledge Memory Networks}} for {{Visual Question Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Su, Zhou and Zhu, Chen and Dong, Yinpeng and Cai, Dongqi and Chen, Yurong and Li, Jianguo},
  date = {2018-06},
  pages = {7736--7745},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00807},
  url = {https://ieeexplore.ieee.org/document/8578905/},
  urldate = {2021-09-09},
  abstract = {Visual question answering (VQA) requires joint comprehension of images and natural language questions, where many questions can’t be directly or clearly answered from visual content but require reasoning from structured human knowledge with confirmation from visual content. This paper proposes visual knowledge memory network (VKMN) to address this issue, which seamlessly incorporates structured human knowledge and deep visual features into memory networks in an end-to-end learning framework. Comparing to existing methods for leveraging external knowledge for supporting VQA, this paper stresses more on two missing mechanisms. First is the mechanism for integrating visual contents with knowledge facts. VKMN handles this issue by embedding knowledge triples (subject, relation, target) and deep visual features jointly into the visual knowledge features. Second is the mechanism for handling multiple knowledge facts expanding from question and answer pairs. VKMN stores joint embedding using key-value pair structure in the memory networks so that it is easy to handle multiple facts. Experiments show that the proposed method achieves promising results on both VQA v1.0 and v2.0 benchmarks, while outperforms state-of-the-art methods on the knowledge-reasoning related questions.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {17 citations (Crossref) [2021-09-10] ZSCC: 0000000[s0]},
  file = {D\:\\Zotero\\storage\\PK9M2VB8\\Su 等。 - 2018 - Learning Visual Knowledge Memory Networks for Visu.pdf}
}

@incollection{sunNovelCapsuleAggregation2021,
  title = {A {{Novel Capsule Aggregation Framework}} for {{Natural Language Inference}}},
  booktitle = {Web and {{Big Data}}},
  author = {Sun, Chao and Wang, Jianzong and Yu, Fengying and Cheng, Ning and Xiao, Jing},
  editor = {U, Leong Hou and Spaniol, Marc and Sakurai, Yasushi and Chen, Junying},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12858},
  pages = {300--315},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-85896-4_24},
  url = {https://link.springer.com/10.1007/978-3-030-85896-4_24},
  urldate = {2021-10-11},
  isbn = {978-3-030-85895-7 978-3-030-85896-4},
  langid = {english}
}

@article{sunVideoQuestionAnswering2021,
  title = {Video {{Question Answering}}: A {{Survey}} of {{Models}} and {{Datasets}}},
  shorttitle = {Video {{Question Answering}}},
  author = {Sun, Guanglu and Liang, Lili and Li, Tianlin and Yu, Bo and Wu, Meng and Zhang, Bolun},
  date = {2021-01-25},
  journaltitle = {Mobile Networks and Applications},
  shortjournal = {Mobile Netw Appl},
  issn = {1383-469X, 1572-8153},
  doi = {10.1007/s11036-020-01730-0},
  url = {http://link.springer.com/10.1007/s11036-020-01730-0},
  urldate = {2021-03-11},
  abstract = {Video question answering (VideoQA) automatically answers natural language question according to the content of videos. It promotes the development of online education, scenario analysis, video content retrieving, etc. VideoQA is a challenging task because it requires a model to understand semantic information of the video and the question to generate the answer. Firstly, we propose a general framework of VideoQA which consists of a video feature extraction module, a text feature extraction module, an integration module, and an answer generation module. The integration module is the core module, including core processing model, recurrent neural networks (RNNs) encoder and feature fusion. These three sub-modules cooperate to generate the contextual representation, and the answer generation module generates the answer based on it. Then, we summarize the methods in core processing model, and introduce the ideas and applications of the methods in detail, such as encoder-decoder, attention model, and memory network and other methods. Additionally, we introduce the widely used datasets and evaluation criteria, as well as the analysis of experimental results on benchmark datasets. Finally, we discuss challenges in the field of VideoQA and provide some possible directions for future work.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\79B2X9TT\\Sun 等。 - 2021 - Video Question Answering a Survey of Models and D.pdf}
}

@unpublished{suVLBERTPretrainingGeneric2020,
  title = {{{VL-BERT}}: {{Pre-training}} of {{Generic Visual-Linguistic Representations}}},
  shorttitle = {{{VL-BERT}}},
  author = {Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  date = {2020-02-17},
  eprint = {1908.08530},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1908.08530},
  urldate = {2021-09-08},
  abstract = {We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at https://github.com/jackroos/VL-BERT.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\845T76W5\\Su 等。 - 2020 - VL-BERT Pre-training of Generic Visual-Linguistic.pdf}
}

@inproceedings{takeshitaLabelMessageLargeScale2021,
  title = {Label or {{Message}}: {{A Large-Scale Experimental Survey}} of {{Texts}} and {{Objects Co-Occurrence}}},
  shorttitle = {Label or {{Message}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Takeshita, Koki and Shioyama, Juntaro and Uchida, Seiichi},
  date = {2021-01-10},
  pages = {6227--6234},
  publisher = {{IEEE}},
  location = {{Milan, Italy}},
  doi = {10.1109/ICPR48806.2021.9412077},
  url = {https://ieeexplore.ieee.org/document/9412077/},
  urldate = {2022-03-12},
  eventtitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  isbn = {978-1-72818-808-9},
  file = {D\:\\Zotero\\storage\\QVCPH3BR\\Takeshita 等。 - 2021 - Label or Message A Large-Scale Experimental Surve.pdf}
}

@article{talmorMULTIMODALQACOMPLEXQUESTION2021,
  title = {{{MULTIMODALQA}}: {{COMPLEX QUESTION ANSWERING OVER TEXT}}, {{TABLES AND IMAGES}}},
  author = {Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan},
  date = {2021},
  pages = {12},
  abstract = {When answering complex questions, people can seamlessly combine information from visual, textual and tabular sources. While interest in models that reason over multiple pieces of evidence has surged in recent years, there has been relatively little work on question answering models that reason across multiple modalities. In this paper, we present MULTIMODALQA (MMQA): a challenging question answering dataset that requires joint reasoning over text, tables and images. We create MMQA using a new framework for generating complex multi-modal questions at scale, harvesting tables from Wikipedia, and attaching images and text paragraphs using entities that appear in each table. We then define a formal language that allows us to take questions that can be answered from a single modality, and combine them to generate cross-modal questions. Last, crowdsourcing workers take these automatically generated questions and rephrase them into more fluent language. We create 29,918 questions through this procedure, and empirically demonstrate the necessity of a multi-modal multi-hop approach to solve our task: our multi-hop model, ImplicitDecomp, achieves an average F1 of 51.7 over cross-modal questions, substantially outperforming a strong baseline that achieves 38.2 F1, but still lags significantly behind human performance, which is at 90.1 F1.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\KFNPPIUL\\Talmor 等。 - 2021 - MULTIMODALQA COMPLEX QUESTION ANSWERING OVER TEXT.pdf}
}

@article{tanakaVisualMRCMachineReading,
  title = {{{VisualMRC}}: {{Machine Reading Comprehension}} on {{Document Images}}},
  author = {Tanaka, Ryota and Nishida, Kyosuke and Yoshida, Sen},
  pages = {11},
  abstract = {Recent studies on machine reading comprehension have focused on text-level understanding but have not yet reached the level of human understanding of the visual layout and content of real-world documents. In this study, we introduce a new visual machine reading comprehension dataset, named VisualMRC, wherein given a question and a document image, a machine reads and comprehends texts in the image to answer the question in natural language. Compared with existing visual question answering (VQA) datasets that contain texts in images, VisualMRC focuses more on developing natural language understanding and generation abilities. It contains 30,000+ pairs of a question and an abstractive answer for 10,000+ document images sourced from multiple domains of webpages. We also introduce a new model that extends existing sequence-to-sequence models, pre-trained with largescale text corpora, to take into account the visual layout and content of documents. Experiments with VisualMRC show that this model outperformed the base sequence-to-sequence models and a state-of-the-art VQA model. However, its performance is still below that of humans on most automatic evaluation metrics. The dataset will facilitate research aimed at connecting vision and language understanding.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\D3H8AZPB\\Tanaka 等。 - VisualMRC Machine Reading Comprehension on Docume.pdf}
}

@unpublished{tanakaVisualMRCMachineReading2021,
  title = {{{VisualMRC}}: {{Machine Reading Comprehension}} on {{Document Images}}},
  shorttitle = {{{VisualMRC}}},
  author = {Tanaka, Ryota and Nishida, Kyosuke and Yoshida, Sen},
  date = {2021-05-10},
  eprint = {2101.11272},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.11272},
  urldate = {2021-12-08},
  abstract = {Recent studies on machine reading comprehension have focused on text-level understanding but have not yet reached the level of human understanding of the visual layout and content of real-world documents. In this study, we introduce a new visual machine reading comprehension dataset, named VisualMRC, wherein given a question and a document image, a machine reads and comprehends texts in the image to answer the question in natural language. Compared with existing visual question answering (VQA) datasets that contain texts in images, VisualMRC focuses more on developing natural language understanding and generation abilities. It contains 30,000+ pairs of a question and an abstractive answer for 10,000+ document images sourced from multiple domains of webpages. We also introduce a new model that extends existing sequence-to-sequence models, pre-trained with large-scale text corpora, to take into account the visual layout and content of documents. Experiments with VisualMRC show that this model outperformed the base sequence-to-sequence models and a state-of-the-art VQA model. However, its performance is still below that of humans on most automatic evaluation metrics. The dataset will facilitate research aimed at connecting vision and language understanding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\9M99W7MS\\Tanaka et al_2021_VisualMRC.pdf;D\:\\Zotero\\storage\\FJ9JHFJC\\2101.html}
}

@unpublished{tangLearningComposeDynamic2018,
  title = {Learning to {{Compose Dynamic Tree Structures}} for {{Visual Contexts}}},
  author = {Tang, Kaihua and Zhang, Hanwang and Wu, Baoyuan and Luo, Wenhan and Liu, Wei},
  date = {2018-12-05},
  eprint = {1812.01880},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1812.01880},
  urldate = {2021-09-09},
  abstract = {We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q\&A. Our visual context tree model, dubbed VCTree, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efficient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., "clothes" and "pants" are usually co-occur and belong to "person"; 2) the dynamic structure varies from image to image and task to task, allowing more content-/task-specific message passing among objects. To construct a VCTree, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-specific models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former's evaluation result serves as a self-critic for the latter's structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2.0 for visual Q\&A, show that VCTree outperforms state-of-the-art results while discovering interpretable visual context structures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Tree},
  file = {D\:\\Zotero\\storage\\PHFMW62G\\Tang et al_2018_Learning to Compose Dynamic Tree Structures for Visual Contexts.pdf}
}

@inproceedings{tangLearningComposeDynamic2019,
  title = {Learning to {{Compose Dynamic Tree Structures}} for {{Visual Contexts}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tang, Kaihua and Zhang, Hanwang and Wu, Baoyuan and Luo, Wenhan and Liu, Wei},
  date = {2019-06},
  pages = {6612--6621},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00678},
  url = {https://ieeexplore.ieee.org/document/8954146/},
  urldate = {2022-03-23},
  abstract = {We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q\&A. Our visual context tree model, dubbed VCTREE, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efficient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., “clothes” and “pants” are usually co-occur and belong to “person”; 2) the dynamic structure varies from image to image and task to task, allowing more content/task-specific message passing among objects. To construct a VCTREE, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-specific models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former’s evaluation result serves as a self-critic for the latter’s structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2.0 for visual Q\&A, show that VCTREE outperforms state-of-the-art results while discovering interpretable visual context structures.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\22MVNVDS\\Tang 等。 - 2019 - Learning to Compose Dynamic Tree Structures for Vi.pdf}
}

@incollection{tangSemanticEquivalentAdversarial2020,
  title = {Semantic {{Equivalent Adversarial Data Augmentation}} for {{Visual Question Answering}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Tang, Ruixue and Ma, Chao and Zhang, Wei Emma and Wu, Qi and Yang, Xiaokang},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12364},
  pages = {437--453},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58529-7_26},
  url = {https://link.springer.com/10.1007/978-3-030-58529-7_26},
  urldate = {2021-09-10},
  abstract = {Visual Question Answering (VQA) has achieved great success thanks to the fast development of deep neural networks (DNN). On the other hand, the data augmentation, as one of the major tricks for DNN, has been widely used in many computer vision tasks. However, there are few works studying the data augmentation problem for VQA and none of the existing image based augmentation schemes (such as rotation and flipping) can be directly applied to VQA due to its semantic structure – an image, question, answer triplet needs to be maintained correctly. For example, a direction related Question-Answer (QA) pair may not be true if the associated image is rotated or flipped. In this paper, instead of directly manipulating images and questions, we use generated adversarial examples for both images and questions as the augmented data. The augmented examples do not change the visual properties presented in the image as well as the semantic meaning of the question, the correctness of the image, question, answer is thus still maintained. We then use adversarial learning to train a classic VQA model (BUTD) with our augmented data. We find that we not only improve the overall performance on VQAv2, but also can withstand adversarial attack effectively, compared to the baseline model. The source code is available at https://github.com/zaynmi/seada-vqa.},
  isbn = {978-3-030-58528-0 978-3-030-58529-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\FYSUYTGL\\Tang 等。 - 2020 - Semantic Equivalent Adversarial Data Augmentation .pdf}
}

@misc{tangTVLTTextlessVisionLanguage2022,
  title = {{{TVLT}}: {{Textless Vision-Language Transformer}}},
  shorttitle = {{{TVLT}}},
  author = {Tang, Zineng and Cho, Jaemin and Nie, Yixin and Bansal, Mohit},
  date = {2022-09-28},
  number = {arXiv:2209.14156},
  eprint = {2209.14156},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.14156},
  urldate = {2022-10-11},
  abstract = {In this work, we present the Textless Vision-Language Transformer (TVLT), where homogeneous transformer blocks take raw visual and audio inputs for vision-and-language representation learning with minimal modality-specific design, and do not use text-specific modules such as tokenization or automatic speech recognition (ASR). TVLT is trained by reconstructing masked patches of continuous video frames and audio spectrograms (masked autoencoding) and contrastive modeling to align video and audio. TVLT attains performance comparable to its text-based counterpart, on various multimodal tasks, such as visual question answering, image retrieval, video retrieval, and multimodal sentiment analysis, with 28x faster inference speed and only 1/3 of the parameters. Our findings suggest the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without assuming the prior existence of text. Our code and checkpoints are available at: https://github.com/zinengtang/TVLT},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\HTXPRR9Z\\Tang et al_2022_TVLT.pdf;D\:\\Zotero\\storage\\QETFVPLM\\2209.html}
}

@inproceedings{tanSelectiveDependencyAggregation2021,
  title = {Selective {{Dependency Aggregation}} for {{Action Classification}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Multimedia}}},
  author = {Tan, Yi and Hao, Yanbin and He, Xiangnan and Wei, Yinwei and Yang, Xun},
  date = {2021-10-17},
  pages = {592--601},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  doi = {10.1145/3474085.3475218},
  url = {https://dl.acm.org/doi/10.1145/3474085.3475218},
  urldate = {2022-05-14},
  eventtitle = {{{MM}} '21: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-8651-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\RG5XAQBV\\Tan 等。 - 2021 - Selective Dependency Aggregation for Action Classi.pdf}
}

@unpublished{tanSelfsupervised3DSemantic2022,
  title = {Self-Supervised {{3D Semantic Representation Learning}} for {{Vision-and-Language Navigation}}},
  author = {Tan, Sinan and Ge, Mengmeng and Guo, Di and Liu, Huaping and Sun, Fuchun},
  date = {2022-01-26},
  number = {arXiv:2201.10788},
  eprint = {2201.10788},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.10788},
  urldate = {2022-06-02},
  abstract = {In the Vision-and-Language Navigation task, the embodied agent follows linguistic instructions and navigates to a specific goal. It is important in many practical scenarios and has attracted extensive attention from both computer vision and robotics communities. However, most existing works only use RGB images but neglect the 3D semantic information of the scene. To this end, we develop a novel self-supervised training framework to encode the voxel-level 3D semantic reconstruction into a 3D semantic representation. Specifically, a region query task is designed as the pretext task, which predicts the presence or absence of objects of a particular class in a specific 3D region. Then, we construct an LSTM-based navigation model and train it with the proposed 3D semantic representations and BERT language features on vision-language pairs. Experiments show that the proposed approach achieves success rates of 68\% and 66\% on the validation unseen and test unseen splits of the R2R dataset respectively, which are superior to most of RGB-based methods utilizing vision-language transformers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {D\:\\Zotero\\storage\\SRM5BTWU\\Tan et al_2022_Self-supervised 3D Semantic Representation Learning for Vision-and-Language.pdf;D\:\\Zotero\\storage\\8GG37A5U\\2201.html}
}

@inproceedings{teneyGraphStructuredRepresentationsVisual2017,
  title = {Graph-{{Structured Representations}} for {{Visual Question Answering}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Teney, Damien and Liu, Lingqiao and Van Den Hengel, Anton},
  date = {2017-07},
  pages = {3233--3241},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.344},
  url = {https://ieeexplore.ieee.org/document/8099827/},
  urldate = {2021-10-11},
  abstract = {This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which do not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. We show that this approach achieves significant improvements over the state-of-the-art, increasing accuracy from 71.2\% to 74.4\% on the “abstract scenes” multiple-choice benchmark, and from 34.7\% to 39.1\% for the more challenging “balanced” scenes, i.e. image pairs with fine-grained differences and opposite yes/no answers to a same question.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@unpublished{teneyGraphStructuredRepresentationsVisual2017a,
  title = {Graph-{{Structured Representations}} for {{Visual Question Answering}}},
  author = {Teney, Damien and Liu, Lingqiao and van den Hengel, Anton},
  date = {2017-03-30},
  eprint = {1609.05600},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1609.05600},
  urldate = {2021-09-26},
  abstract = {This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2\% to 74.4\% in accuracy on the "abstract scenes" multiple-choice benchmark, and from 34.7\% to 39.1\% in accuracy over pairs of "balanced" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\25Q7Y7NB\\Teney et al_2017_Graph-Structured Representations for Visual Question Answering.pdf;D\:\\Zotero\\storage\\68CSS8CR\\1609.html}
}

@unpublished{teneyTipsTricksVisual2017,
  title = {Tips and {{Tricks}} for {{Visual Question Answering}}: {{Learnings}} from the 2017 {{Challenge}}},
  shorttitle = {Tips and {{Tricks}} for {{Visual Question Answering}}},
  author = {Teney, Damien and Anderson, Peter and He, Xiaodong and van den Hengel, Anton},
  date = {2017-08-09},
  eprint = {1708.02711},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1708.02711},
  urldate = {2021-08-13},
  abstract = {This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\KFFCCICE\\Teney et al_2017_Tips and Tricks for Visual Question Answering.pdf;D\:\\Zotero\\storage\\NBYU4U8I\\1708.html}
}

@inproceedings{teneyTipsTricksVisual2018,
  title = {Tips and {{Tricks}} for {{Visual Question Answering}}: {{Learnings}} from the 2017 {{Challenge}}},
  shorttitle = {Tips and {{Tricks}} for {{Visual Question Answering}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Teney, Damien and Anderson, Peter and He, Xiaodong and van den Hengel, Anton},
  date = {2018-06},
  pages = {4223--4232},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00444},
  url = {https://ieeexplore.ieee.org/document/8578542/},
  urldate = {2021-09-09},
  abstract = {Deep Learning has had a transformative impact on Computer Vision, but for all of the success there is also a significant cost. This is that the models and procedures used are so complex and intertwined that it is often impossible to distinguish the impact of the individual design and engineering choices each model embodies. This ambiguity diverts progress in the field, and leads to a situation where developing a state-of-the-art model is as much an art as a science. As a step towards addressing this problem we present a massive exploration of the effects of the myriad architectural and hyperparameter choices that must be made in generating a state-of-the-art model. The model is of particular interest because it won the 2017 Visual Question Answering Challenge. We provide a detailed analysis of the impact of each choice on model performance, in the hope that it will inform others in developing models, but also that it might set a precedent that will accelerate scientific progress in the field.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {67 citations (Crossref) [2021-09-10] SC: None[s0]},
  file = {D\:\\Zotero\\storage\\ANGBZKZN\\Teney 等。 - 2018 - Tips and Tricks for Visual Question Answering Lea.pdf}
}

@unpublished{teneyValueOutofDistributionTesting2020,
  title = {On the {{Value}} of {{Out-of-Distribution Testing}}: {{An Example}} of {{Goodhart}}'s {{Law}}},
  shorttitle = {On the {{Value}} of {{Out-of-Distribution Testing}}},
  author = {Teney, Damien and Kafle, Kushal and Shrestha, Robik and Abbasnejad, Ehsan and Kanan, Christopher and van den Hengel, Anton},
  date = {2020-05-19},
  eprint = {2005.09241},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.09241},
  urldate = {2021-09-10},
  abstract = {Out-of-distribution (OOD) testing is increasingly popular for evaluating a machine learning system's ability to generalize beyond the biases of a training set. OOD benchmarks are designed to present a different joint distribution of data and labels between training and test time. VQA-CP has become the standard OOD benchmark for visual question answering, but we discovered three troubling practices in its current use. First, most published methods rely on explicit knowledge of the construction of the OOD splits. They often rely on ``inverting'' the distribution of labels, e.g. answering mostly 'yes' when the common training answer is 'no'. Second, the OOD test set is used for model selection. Third, a model's in-domain performance is assessed after retraining it on in-domain splits (VQA v2) that exhibit a more balanced distribution of labels. These three practices defeat the objective of evaluating generalization, and put into question the value of methods specifically designed for this dataset. We show that embarrassingly-simple methods, including one that generates answers at random, surpass the state of the art on some question types. We provide short- and long-term solutions to avoid these pitfalls and realize the benefits of OOD evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Distribution},
  file = {D\:\\Zotero\\storage\\MYEVEIW8\\Teney et al_2020_On the Value of Out-of-Distribution Testing.pdf;D\:\\Zotero\\storage\\S4ZN9SKD\\2005.html}
}

@incollection{teneyVisualQuestionAnswering2018,
  title = {Visual {{Question Answering}} as a {{Meta Learning Task}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Teney, Damien and van den Hengel, Anton},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  options = {useprefix=true},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11219},
  pages = {229--245},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01267-0_14},
  url = {http://link.springer.com/10.1007/978-3-030-01267-0_14},
  urldate = {2021-09-10},
  abstract = {The predominant approach to Visual Question Answering (VQA) demands that the model represents within its weights all of the information required to answer any question about any image. Learning this information from any real training set seems unlikely, and representing it in a reasonable number of weights doubly so. We propose instead to approach VQA as a meta learning task, thus separating the question answering method from the information required. At test time, the method is provided with a support set of example questions/answers, over which it reasons to resolve the given question. The support set is not fixed and can be extended without retraining, thereby expanding the capabilities of the model. To exploit this dynamically provided information, we adapt a state-of-the-art VQA model with two techniques from the recent meta learning literature, namely prototypical networks and meta networks. Experiments demonstrate the capability of the system to learn to produce completely novel answers (i.e. never seen during training) from examples provided at test time. In comparison to the existing state of the art, the proposed method produces qualitatively distinct results with higher recall of rare answers, and a better sample efficiency that allows training with little initial data. More importantly, it represents an important step towards vision-and-language methods that can learn and reason on-the-fly.},
  isbn = {978-3-030-01266-3 978-3-030-01267-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\L7TV732N\\Teney 和 van den Hengel - 2018 - Visual Question Answering as a Meta Learning Task.pdf}
}

@inproceedings{tianMaskPredictMultistep2021,
  title = {Mask and {{Predict}}: {{Multi-step Reasoning}} for {{Scene Graph Generation}}},
  shorttitle = {Mask and {{Predict}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Multimedia}}},
  author = {Tian, Hongshuo and Xu, Ning and Liu, An-An and Yan, Chenggang and Mao, Zhendong and Zhang, Quan and Zhang, Yongdong},
  date = {2021-10-17},
  pages = {4128--4136},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  doi = {10.1145/3474085.3475545},
  url = {https://dl.acm.org/doi/10.1145/3474085.3475545},
  urldate = {2022-05-05},
  abstract = {Scene Graph Generation (SGG) aims to parse the image as a set of semantics, containing objects and their relations. Currently, the SGG methods only stay at presenting the intuitive detection in the image, such as the triplet “logo on board". Intuitively, we humans can further refine these intuitive detections as rational descriptions like “flower painted on surfboard". However, most of existing methods always formulate SGG as a straightforward task, only limited by the manner of one-time prediction, which focuses on a singlepass pipeline and predicts all the semantic. Therefore, to handle this problem, we propose a novel multi-step reasoning manner for SGG. Concretely, we break SGG into two explicit learning stages, including intuitive training stage (ITS) and rational training stage (RTS). In the first stage, we follow the traditional SGG processing to detect objects and relationships, yielding an intuitive scene graph. In the second stage, we perform multi-step reasoning to refine the intuitive scene graph. For each step of reasoning, it consists of two kinds of operations: mask and predict. According to primary predictions and their confidences, we constantly select and mask the low-confidence predictions, which features are optimized and predicted again. After several iterations, all of intuitive semantics will gradually tend to be revised with high confidences, yielding a rational scene graph. Extensive experiments on Visual Genome prove the superiority of the proposed method. Additional ablation studies and visualization cases further validate its effectiveness.},
  eventtitle = {{{MM}} '21: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-8651-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\8DNNRFSR\\Tian 等。 - 2021 - Mask and Predict Multi-step Reasoning for Scene G.pdf}
}

@unpublished{tianWhenMultiLevelMeets2022,
  title = {When {{Multi-Level Meets Multi-Interest}}: {{A Multi-Grained Neural Model}} for {{Sequential Recommendation}}},
  shorttitle = {When {{Multi-Level Meets Multi-Interest}}},
  author = {Tian, Yu and Chang, Jianxin and Niu, Yannan and Song, Yang and Li, Chenliang},
  date = {2022-05-02},
  eprint = {2205.01286},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2205.01286},
  urldate = {2022-05-08},
  abstract = {Sequential recommendation aims at identifying the next item that is preferred by a user based on their behavioral history. Compared to conventional sequential models that leverage attention mechanisms and RNNs, recent efforts mainly follow two directions for improvement: multi-interest learning and graph convolutional aggregation. Specifically, multi-interest methods such as ComiRec and MIMN, focus on extracting different interests for a user by performing historical item clustering, while graph convolution methods including TGSRec and SURGE elect to refine user preferences based on multi-level correlations between historical items. Unfortunately, neither of them realizes that these two types of solutions can mutually complement each other, by aggregating multi-level user preference to achieve more precise multi-interest extraction for a better recommendation. To this end, in this paper, we propose a unified multi-grained neural model(named MGNM) via a combination of multi-interest learning and graph convolutional aggregation. Concretely, MGNM first learns the graph structure and information aggregation paths of the historical items for a user. It then performs graph convolution to derive item representations in an iterative fashion, in which the complex preferences at different levels can be well captured. Afterwards, a novel sequential capsule network is proposed to inject the sequential patterns into the multi-interest extraction process, leading to a more precise interest learning in a multi-grained manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\969EES5R\\Tian et al_2022_When Multi-Level Meets Multi-Interest.pdf;D\:\\Zotero\\storage\\BCUG5WK9\\2205.html}
}

@unpublished{tommasiCombiningMultipleCues2018,
  title = {Combining {{Multiple Cues}} for {{Visual Madlibs Question Answering}}},
  author = {Tommasi, Tatiana and Mallya, Arun and Plummer, Bryan and Lazebnik, Svetlana and Berg, Alexander C. and Berg, Tamara L.},
  date = {2018-02-07},
  eprint = {1611.00393},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.00393},
  urldate = {2021-09-10},
  abstract = {This paper presents an approach for answering fill-in-the-blank multiple choice questions from the Visual Madlibs dataset. Instead of generic and commonly used representations trained on the ImageNet classification task, our approach employs a combination of networks trained for specialized tasks such as scene recognition, person activity classification, and attribute prediction. We also present a method for localizing phrases from candidate answers in order to provide spatial support for feature extraction. We map each of these features, together with candidate answers, to a joint embedding space through normalized canonical correlation analysis (nCCA). Finally, we solve an optimization problem to learn to combine scores from nCCA models trained on multiple cues to select the best answer. Extensive experimental results show a significant improvement over the previous state of the art and confirm that answering questions from a wide range of types benefits from examining a variety of image cues and carefully choosing the spatial support for feature extraction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\K7C8MEA3\\Tommasi et al_2018_Combining Multiple Cues for Visual Madlibs Question Answering.pdf;D\:\\Zotero\\storage\\HM6LSWNP\\1611.html}
}

@inproceedings{tranLearningSpatiotemporalFeatures2015,
  title = {Learning {{Spatiotemporal Features}} with {{3D Convolutional Networks}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  date = {2015-12},
  pages = {4489--4497},
  publisher = {{IEEE}},
  location = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.510},
  url = {http://ieeexplore.ieee.org/document/7410867/},
  urldate = {2021-03-11},
  abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3 × 3 × 3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8\% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4673-8391-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\7PBI626A\\Tran 等。 - 2015 - Learning Spatiotemporal Features with 3D Convoluti.pdf}
}

@unpublished{trottInterpretableCountingVisual2018,
  title = {Interpretable {{Counting}} for {{Visual Question Answering}}},
  author = {Trott, Alexander and Xiong, Caiming and Socher, Richard},
  date = {2018-03-01},
  eprint = {1712.08697},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1712.08697},
  urldate = {2021-09-10},
  abstract = {Questions that require counting a variety of objects in images remain a major challenge in visual question answering (VQA). The most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image. In contrast, we treat counting as a sequential decision process and force our model to make discrete choices of what to count. Specifically, the model sequentially selects from detected objects and learns interactions between objects that influence subsequent selections. A distinction of our approach is its intuitive and interpretable output, as discrete counts are automatically grounded in the image. Furthermore, our method outperforms the state of the art architecture for VQA on multiple metrics that evaluate counting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\FLUWSTIL\\Trott et al_2018_Interpretable Counting for Visual Question Answering.pdf;D\:\\Zotero\\storage\\TPRSX7A9\\1712.html}
}

@unpublished{tsaiMultimodalRoutingImproving2020,
  title = {Multimodal {{Routing}}: {{Improving Local}} and {{Global Interpretability}} of {{Multimodal Language Analysis}}},
  shorttitle = {Multimodal {{Routing}}},
  author = {Tsai, Yao-Hung Hubert and Ma, Martin Q. and Yang, Muqiao and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  date = {2020-10-05},
  eprint = {2004.14198},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.14198},
  urldate = {2021-10-09},
  abstract = {The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability. In this paper we propose Multimodal Routing, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality features. Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\I8JHMRN9\\Tsai et al_2020_Multimodal Routing.pdf;D\:\\Zotero\\storage\\HIP7LDN3\\2004.html}
}

@unpublished{tsaiMultimodalTransformerUnaligned2019,
  title = {Multimodal {{Transformer}} for {{Unaligned Multimodal Language Sequences}}},
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J. Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  date = {2019-06-01},
  eprint = {1906.00295},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1906.00295},
  urldate = {2021-10-15},
  abstract = {Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\CMJ7LAGU\\Tsai et al_2019_Multimodal Transformer for Unaligned Multimodal Language Sequences.pdf;D\:\\Zotero\\storage\\TUAPCN55\\Gu_Tresp_2021_Interpretable Graph Capsule Networks for Object Recognition.pdf;D\:\\Zotero\\storage\\A7B3ZM6J\\1906.html}
}

@unpublished{tsaiMultimodalTransformerUnaligned2019a,
  title = {Multimodal {{Transformer}} for {{Unaligned Multimodal Language Sequences}}},
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J. Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  date = {2019-06-01},
  eprint = {1906.00295},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1906.00295},
  urldate = {2021-10-09},
  abstract = {Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\YWDTDI7B\\Tsai et al_2019_Multimodal Transformer for Unaligned Multimodal Language Sequences.pdf;D\:\\Zotero\\storage\\N2UMPJLZ\\1906.html}
}

@unpublished{tsaiVideoRelationshipReasoning2019,
  title = {Video {{Relationship Reasoning}} Using {{Gated Spatio-Temporal Energy Graph}}},
  author = {Tsai, Yao-Hung Hubert and Divvala, Santosh and Morency, Louis-Philippe and Salakhutdinov, Ruslan and Farhadi, Ali},
  date = {2019-03-27},
  eprint = {1903.10547},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1903.10547},
  urldate = {2021-09-09},
  abstract = {Visual relationship reasoning is a crucial yet challenging task for understanding rich interactions across visual concepts. For example, a relationship 'man, open, door' involves a complex relation 'open' between concrete entities 'man, door'. While much of the existing work has studied this problem in the context of still images, understanding visual relationships in videos has received limited attention. Due to their temporal nature, videos enable us to model and reason about a more comprehensive set of visual relationships, such as those requiring multiple (temporal) observations (e.g., 'man, lift up, box' vs. 'man, put down, box'), as well as relationships that are often correlated through time (e.g., 'woman, pay, money' followed by 'woman, buy, coffee'). In this paper, we construct a Conditional Random Field on a fully-connected spatio-temporal graph that exploits the statistical dependency between relational entities spatially and temporally. We introduce a novel gated energy function parametrization that learns adaptive relations conditioned on visual observations. Our model optimization is computationally efficient, and its space computation complexity is significantly amortized through our proposed parameterization. Experimental results on benchmark video datasets (ImageNet Video and Charades) demonstrate state-of-the-art performance across three standard relationship reasoning tasks: Detection, Tagging, and Recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Relationship},
  file = {D\:\\Zotero\\storage\\6IMJ8XQP\\Tsai et al_2019_Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph.pdf;D\:\\Zotero\\storage\\NF4SSQML\\1903.html}
}

@article{tuLearningBetterVisual,
  title = {Learning {{Better Visual Dialog Agents With Pretrained Visual-Linguistic Representation}}},
  author = {Tu, Tao and Ping, Qing and Thattai, Govindarajan and Tur, Gokhan and Natarajan, Prem},
  pages = {10},
  abstract = {GuessWhat?! is a visual dialog guessing game which incorporates a Questioner agent that generates a sequence of questions, while an Oracle agent answers the respective questions about a target object in an image. Based on this dialog history between the Questioner and the Oracle, a Guesser agent makes a final guess of the target object. While previous work has focused on dialogue policy optimization and visual-linguistic information fusion, most work learns the vision-linguistic encoding for the three agents solely on the GuessWhat?! dataset without shared and prior knowledge of vision-linguistic representation. To bridge these gaps, this paper proposes new Oracle, Guesser and Questioner models that take advantage of a pretrained vision-linguistic model, VilBERT. For Oracle model, we introduce a two-way background/target fusion mechanism to understand both intra and inter-object questions. For Guesser model, we introduce a state-estimator that best utilizes VilBERT’s strength in single-turn referring expression comprehension. For the Questioner, we share the stateestimator from pretrained Guesser with Questioner to guide the question generator. Experimental results show that our proposed models outperform state-of-the-art models significantly by 7\%, 10\%, 12\% for Oracle, Guesser and End-toEnd Questioner respectively.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\KXJY7NFS\\Tu 等。 - Learning Better Visual Dialog Agents With Pretrain.pdf}
}

@inproceedings{tuMultihopReadingComprehension2019,
  title = {Multi-Hop {{Reading Comprehension}} across {{Multiple Documents}} by {{Reasoning}} over {{Heterogeneous Graphs}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tu, Ming and Wang, Guangtao and Huang, Jing and Tang, Yun and He, Xiaodong and Zhou, Bowen},
  date = {2019-07},
  pages = {2704--2713},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1260},
  url = {https://aclanthology.org/P19-1260},
  urldate = {2021-10-10},
  abstract = {Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.},
  eventtitle = {{{ACL}} 2019},
  file = {D\:\\Zotero\\storage\\6IY34AAT\\Tu et al_2019_Multi-hop Reading Comprehension across Multiple Documents by Reasoning over.pdf}
}

@incollection{ueharaVisualQuestionGeneration2018,
  title = {Visual {{Question Generation}} for {{Class Acquisition}} of {{Unknown Objects}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Uehara, Kohei and Tejero-De-Pablos, Antonio and Ushiku, Yoshitaka and Harada, Tatsuya},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11216},
  pages = {492--507},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01258-8_30},
  url = {http://link.springer.com/10.1007/978-3-030-01258-8_30},
  urldate = {2021-09-10},
  isbn = {978-3-030-01257-1 978-3-030-01258-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\PAPZJDGS\\Uehara 等。 - 2018 - Visual Question Generation for Class Acquisition o.pdf}
}

@article{uroojFoundReasonMe,
  title = {Found a {{Reason}} for Me? {{Weakly-supervised Grounded Visual Question Answering}} Using {{Capsules}}},
  author = {Urooj, Aisha and Kuehne, Hilde and Duarte, Kevin and Gan, Chuang and Lobo, Niels and Shah, Mubarak},
  pages = {10},
  langid = {english},
  keywords = {Grounding},
  file = {D\:\\Zotero\\storage\\XQBWK5XF\\Urooj 等。 - Found a Reason for me Weakly-supervised Grounded .pdf}
}

@inproceedings{vatashskyVQANoQuestionsAnswers2020,
  title = {{{VQA With No Questions-Answers Training}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Vatashsky, Ben-Zion and Ullman, Shimon},
  date = {2020-06},
  pages = {10373--10383},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01039},
  url = {https://ieeexplore.ieee.org/document/9157617/},
  urldate = {2021-09-09},
  abstract = {Methods for teaching machines to answer visual questions have made significant progress in recent years, but current methods still lack important human capabilities, including integrating new visual classes and concepts in a modular manner, providing explanations for the answers and handling new domains without explicit examples. We propose a novel method that consists of two main parts: generating a question graph representation, and an answering procedure, guided by the abstract structure of the question graph to invoke an extendable set of visual estimators. Training is performed for the language part and the visual part on their own, but unlike existing schemes, the method does not require any training using images with associated questions and answers. This approach is able to handle novel domains (extended question types and new object classes, properties and relations) as long as corresponding visual estimators are available. In addition, it can provide explanations to its answers and suggest alternatives when questions are not grounded in the image. We demonstrate that this approach achieves both high performance and domain extensibility without any questions-answers training.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {1 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\QZQTTKXF\\Vatashsky 和 Ullman - 2020 - VQA With No Questions-Answers Training.pdf}
}

@unpublished{vathAccuracyConsolidatedTool2021,
  title = {Beyond {{Accuracy}}: {{A Consolidated Tool}} for {{Visual Question Answering Benchmarking}}},
  shorttitle = {Beyond {{Accuracy}}},
  author = {Väth, Dirk and Tilli, Pascal and Vu, Ngoc Thang},
  date = {2021-10-11},
  eprint = {2110.05159},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.05159},
  urldate = {2021-12-08},
  abstract = {On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of VQA. Our tool helps test generalization capabilities of models across multiple datasets, evaluating not just accuracy, but also performance in more realistic real-world scenarios such as robustness to input noise. Additionally, we include metrics that measure biases and uncertainty, to further explain model behavior. Interactive filtering facilitates discovery of problematic behavior, down to the data sample level. As proof of concept, we perform a case study on four models. We find that state-of-the-art VQA models are optimized for specific tasks or datasets, but fail to generalize even to other in-domain test sets, for example they cannot recognize text in images. Our metrics allow us to quantify which image and question embeddings provide most robustness to a model. All code is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\HDMJPWMH\\Väth et al_2021_Beyond Accuracy.pdf;D\:\\Zotero\\storage\\W8K9TGM8\\2110.html}
}

@inproceedings{vickersFactualityEfficientIntegration2021,
  title = {In {{Factuality}}: {{Efficient Integration}} of {{Relevant Facts}} for {{Visual Question Answering}}},
  shorttitle = {In {{Factuality}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Vickers, Peter and Aletras, Nikolaos and Monti, Emilio and Barrault, Loïc},
  date = {2021-08},
  pages = {468--475},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-short.60},
  url = {https://aclanthology.org/2021.acl-short.60},
  urldate = {2021-09-09},
  abstract = {Visual Question Answering (VQA) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. Current models are trained on labelled data that may be insufficient to learn complex knowledge representations. In this paper, we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (Vision+Language BERT) by integrating facts extracted from an external knowledge base. Evaluation on the KVQA dataset benchmark demonstrates that our method outperforms competitive baselines by 19\%, achieving new state-of-the-art results. We also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  file = {D\:\\Zotero\\storage\\M5GQNBPE\\Vickers et al_2021_In Factuality.pdf}
}

@online{VisualQuestionAnswering,
  title = {Visual Question Answering by Pattern Matching and Reasoning | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neucom.2021.10.016},
  url = {https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421e2f2409822222655721b8cba9150317b806a29/reader/sd/pii/S0925231221014946?token=917E1848F127DCF7ED3BEEFECA84BD9788B81E584D531B179AABCA83B1777C6B48AA2025CCC824AF354DCACAB4E77AFB&originRegion=us-east-1&originCreation=20220207024252},
  urldate = {2022-02-07},
  langid = {english}
}

@article{vongFewshotImageClassification,
  title = {Few-Shot Image Classification by Generating Natural Language Rules},
  author = {Vong, Wai Keen and Lake, Brenden M},
  pages = {7},
  abstract = {The ability to generate rules and hypotheses plays a key role in multiple aspects of human cognition including concept learning and explanation. Previous research has framed this ability as a form of inference via probabilistic program induction. However, this approach requires careful construction of the right grammar and hypothesis space for a particular task. In this work, we propose an alternative computational account of rule generation and concept learning that sidesteps some of these issues. By leveraging advances in multimodal learning and large language models, we extend the latent language framework from Andreas et al. (2017) to work in a zero-shot manner. Taking naturalistic images as input, our computational model is capable of generating candidate rules that are specified in natural language, and verifying them against the observed data. We show that our model can generate, in a zero-shot manner, plausible rules for visual concepts in two domains.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\FIVW4ZDH\\Vong 和 Lake - Few-shot image classification by generating natura.pdf}
}

@unpublished{wangChatCapsuleHierarchicalCapsule2022,
  title = {Chat-{{Capsule}}: {{A Hierarchical Capsule}} for {{Dialog-level Emotion Analysis}}},
  shorttitle = {Chat-{{Capsule}}},
  author = {Wang, Yequan and Meng, Xuying and Liu, Yiyi and Sun, Aixin and Wang, Yao and Zheng, Yinhe and Huang, Minlie},
  date = {2022-03-23},
  eprint = {2203.12254},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2203.12254},
  urldate = {2022-04-30},
  abstract = {Many studies on dialog emotion analysis focus on utterance-level emotion only. These models hence are not optimized for dialog-level emotion detection, i.e. to predict the emotion category of a dialog as a whole. More importantly, these models cannot benefit from the context provided by the whole dialog. In real-world applications, annotations to dialog could fine-grained, including both utterance-level tags (e.g. speaker type, intent category, and emotion category), and dialog-level tags (e.g. user satisfaction, and emotion curve category). In this paper, we propose a Context-based Hierarchical Attention Capsule\textasciitilde (Chat-Capsule) model, which models both utterance-level and dialog-level emotions and their interrelations. On a dialog dataset collected from customer support of an e-commerce platform, our model is also able to predict user satisfaction and emotion curve category. Emotion curve refers to the change of emotions along the development of a conversation. Experiments show that the proposed Chat-Capsule outperform state-of-the-art baselines on both benchmark dataset and proprietary dataset. Source code will be released upon acceptance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\JSNITERR\\Wang et al_2022_Chat-Capsule.pdf;D\:\\Zotero\\storage\\T28IT2UU\\2203.html}
}

@article{wangErrorRecognitionMethod2019,
  title = {An Error Recognition Method for Power Equipment Defect Records Based on Knowledge Graph Technology},
  author = {Wang, Hui-fang and Liu, Zi-quan},
  date = {2019-11},
  journaltitle = {Frontiers of Information Technology \& Electronic Engineering},
  shortjournal = {Front Inform Technol Electron Eng},
  volume = {20},
  number = {11},
  pages = {1564--1577},
  issn = {2095-9184, 2095-9230},
  doi = {10.1631/FITEE.1800260},
  url = {http://link.springer.com/10.1631/FITEE.1800260},
  urldate = {2021-03-11},
  abstract = {To recognize errors in the power equipment defect records in real time, we propose an error recognition method based on knowledge graph technology. According to the characteristics of power equipment defect records, a method for constructing a knowledge graph of power equipment defects is presented. Then, a graph search algorithm is employed to recognize different kinds of errors in defect records, based on the knowledge graph of power equipment defects. Finally, an error recognition example in terms of transformer defect records is given, by comparing the precision, recall, F1-score, accuracy, and efficiency of the proposed method with those of machine learning methods, and the factors influencing the error recognition effects of various methods are analyzed. Results show that the proposed method performs better in error recognition of defect records than machine learning methods, and can satisfy real-time requirements.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\6ML4RBVP\\Wang 和 Liu - 2019 - An error recognition method for power equipment de.pdf}
}

@unpublished{wangFVQAFactbasedVisual2017,
  title = {{{FVQA}}: {{Fact-based Visual Question Answering}}},
  shorttitle = {{{FVQA}}},
  author = {Wang, Peng and Wu, Qi and Shen, Chunhua and van den Hengel, Anton and Dick, Anthony},
  date = {2017-08-08},
  eprint = {1606.05433},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.05433},
  urldate = {2021-09-10},
  abstract = {Visual Question Answering (VQA) has attracted a lot of attention in both Computer Vision and Natural Language Processing communities, not least because it offers insight into the relationships between two important sources of information. Current datasets, and the models built upon them, have focused on questions which are answerable by direct analysis of the question and image alone. The set of such questions that require no external information to answer is interesting, but very limited. It excludes questions which require common sense, or basic factual knowledge to answer, for example. Here we introduce FVQA, a VQA dataset which requires, and supports, much deeper reasoning. FVQA only contains questions which require external information to answer. We thus extend a conventional visual question answering dataset, which contains image-question-answerg triplets, through additional image-question-answer-supporting fact tuples. The supporting fact is represented as a structural triplet, such as {$<$}Cat,CapableOf,ClimbingTrees{$>$}. We evaluate several baseline models on the FVQA dataset, and describe a novel model which is capable of reasoning about an image on the basis of supporting facts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\NPRYBAXH\\Wang et al_2017_FVQA.pdf;D\:\\Zotero\\storage\\UR3MNBV4\\1606.html}
}

@unpublished{wangGeneralValueEvidence2020,
  title = {On the {{General Value}} of {{Evidence}}, and {{Bilingual Scene-Text Visual Question Answering}}},
  author = {Wang, Xinyu and Liu, Yuliang and Shen, Chunhua and Ng, Chun Chet and Luo, Canjie and Jin, Lianwen and Chan, Chee Seng and van den Hengel, Anton and Wang, Liangwei},
  date = {2020-02-25},
  eprint = {2002.10215},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2002.10215},
  urldate = {2021-12-08},
  abstract = {Visual Question Answering (VQA) methods have made incredible progress, but suffer from a failure to generalize. This is visible in the fact that they are vulnerable to learning coincidental correlations in the data rather than deeper relations between image content and ideas expressed in language. We present a dataset that takes a step towards addressing this problem in that it contains questions expressed in two languages, and an evaluation process that co-opts a well understood image-based metric to reflect the method's ability to reason. Measuring reasoning directly encourages generalization by penalizing answers that are coincidentally correct. The dataset reflects the scene-text version of the VQA problem, and the reasoning evaluation can be seen as a text-based version of a referring expression challenge. Experiments and analysis are provided that show the value of the dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\3ERC4QTI\\Wang et al_2020_On the General Value of Evidence, and Bilingual Scene-Text Visual Question.pdf;D\:\\Zotero\\storage\\ZUVL5MFQ\\2002.html}
}

@inproceedings{wangGeneralValueEvidence2020a,
  title = {On the {{General Value}} of {{Evidence}}, and {{Bilingual Scene-Text Visual Question Answering}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Xinyu and Liu, Yuliang and Shen, Chunhua and Ng, Chun Chet and Luo, Canjie and Jin, Lianwen and Chan, Chee Seng and van den Hengel, Anton and Wang, Liangwei},
  options = {useprefix=true},
  date = {2020-06},
  pages = {10123--10132},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01014},
  url = {https://ieeexplore.ieee.org/document/9156857/},
  urldate = {2021-09-09},
  abstract = {Visual Question Answering (VQA) methods have made incredible progress, but suffer from a failure to generalize. This is visible in the fact that they are vulnerable to learning coincidental correlations in the data rather than deeper relations between image content and ideas expressed in language. We present a dataset that takes a step towards addressing this problem in that it contains questions expressed in two languages, and an evaluation process that co-opts a well understood image-based metric to reflect the method’s ability to reason. Measuring reasoning directly encourages generalization by penalizing answers that are coincidentally correct. The dataset reflects the scene-text version of the VQA problem, and the reasoning evaluation can be seen as a text-based version of a referring expression challenge. Experiments and analyses are provided that show the value of the dataset. The dataset is available at www.est-vqa.org.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {5 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\IAYYVWD5\\Wang 等。 - 2020 - On the General Value of Evidence, and Bilingual Sc.pdf}
}

@misc{wangImageForeignLanguage2022,
  title = {Image as a {{Foreign Language}}: {{BEiT Pretraining}} for {{All Vision}} and {{Vision-Language Tasks}}},
  shorttitle = {Image as a {{Foreign Language}}},
  author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and Wei, Furu},
  date = {2022-08-22},
  number = {arXiv:2208.10442},
  eprint = {2208.10442},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.10442},
  urldate = {2022-08-25},
  abstract = {A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked "language" modeling on images (Imglish), texts (English), and image-text pairs ("parallel sentences") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\8FIMITIV\\Wang et al_2022_Image as a Foreign Language.pdf;D\:\\Zotero\\storage\\R4VMITG6\\2208.html}
}

@inproceedings{wangMatchingImagesText2019,
  title = {Matching {{Images}} and {{Text}} with {{Multi-modal Tensor Fusion}} and {{Re-ranking}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Multimedia}}},
  author = {Wang, Tan and Xu, Xing and Yang, Yang and Hanjalic, Alan and Shen, Heng Tao and Song, Jingkuan},
  date = {2019-10-15},
  pages = {12--20},
  publisher = {{ACM}},
  location = {{Nice France}},
  doi = {10.1145/3343031.3350875},
  url = {https://dl.acm.org/doi/10.1145/3343031.3350875},
  urldate = {2021-03-11},
  abstract = {A major challenge in matching images and text is that they have intrinsically different data distributions and feature representations. Most existing approaches are based either on embedding or classification, the first one mapping image and text instances into a common embedding space for distance measuring, and the second one regarding image-text matching as a binary classification problem. Neither of these approaches can, however, balance the matching accuracy and model complexity well. We propose a novel framework that achieves remarkable matching performance with acceptable model complexity. Specifically, in the training stage, we propose a novel Multi-modal Tensor Fusion Network (MTFN) to explicitly learn an accurate image-text similarity function with rank-based tensor fusion rather than seeking a common embedding space for each image-text instance. Then, during testing, we deploy a generic Cross-modal Re-ranking (RR) scheme for refinement without requiring additional training procedure. Extensive experiments on two datasets demonstrate that our MTFN-RR consistently achieves the state-of-the-art matching performance with much less time complexity.},
  eventtitle = {{{MM}} '19: {{The}} 27th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-6889-6},
  langid = {english},
  keywords = {Multi_model},
  file = {D\:\\Zotero\\storage\\XKDRHJGS\\Wang 等。 - 2019 - Matching Images and Text with Multi-modal Tensor F.pdf}
}

@article{wangMovieQuestionAnswering,
  title = {Movie {{Question Answering}}: {{Remembering}} the {{Textual Cues}} for {{Layered Visual Contents}}},
  author = {Wang, Bo and Xu, Youjiang and Han, Yahong and Hong, Richang},
  pages = {8},
  abstract = {Movies provide us with a mass of visual content as well as attracting stories. Existing methods have illustrated that understanding movie stories through only visual content is still a hard problem. In this paper, for answering questions about movies, we put forward a Layered Memory Network (LMN) that represents frame-level and clip-level movie content by the Static Word Memory module and the Dynamic Subtitle Memory module, respectively. Particularly, we firstly extract words and sentences from the training movie subtitles. Then the hierarchically formed movie representations, which are learned from LMN, not only encode the correspondence between words and visual content inside frames, but also encode the temporal alignment between sentences and frames inside movie clips. We also extend our LMN model into three variant frameworks to illustrate the good extendable capabilities. We conduct extensive experiments on the MovieQA dataset. With only visual content as inputs, LMN with framelevel representation obtains a large performance improvement. When incorporating subtitles into LMN to form the clip-level representation, we achieve the state-of-the-art performance on the online evaluation task of ‘Video+Subtitles’. The good performance successfully demonstrates that the proposed framework of LMN is effective and the hierarchically formed movie representations have good potential for the applications of movie question answering.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\6J9D5SS3\\Wang 等。 - Movie Question Answering Remembering the Textual .pdf}
}

@inproceedings{wangMultihopAttentionGraph2021,
  title = {Multi-Hop {{Attention Graph Neural Networks}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Wang, Guangtao and Ying, Rex and Huang, Jing and Leskovec, Jure},
  date = {2021-08},
  pages = {3089--3096},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Montreal, Canada}},
  doi = {10.24963/ijcai.2021/425},
  url = {https://www.ijcai.org/proceedings/2021/425},
  urldate = {2021-10-11},
  abstract = {Self-attention mechanism in graph neural networks (GNNs) led to state-of-theart performance on many graph representation learning task. Currently, at every layer, attention is computed between connected pairs of nodes and depends solely on the representation of the two nodes. However, such attention mechanism does not account for nodes that are not directly connected but provide important network context, which could lead to improved predictive performance. Here we propose Multi-hop Attention Graph Neural Network (MAGNA), a principled way to incorporate multi-hop context information into attention computation, enabling long-range interactions at every layer of the GNN. To compute attention between nodes that are not directly connected, MAGNA diffuses the attention scores across the network, which increases the “receptive field” for every layer of the GNN. Unlike previous approaches, MAGNA uses a diffusion prior on attention values, to efficiently account for all paths between the pair of disconnected nodes. This helps MAGNA capture large-scale structural information in every layer, and learn more informative attention. Experimental results on node classification as well as the knowledge graph completion benchmarks show that MAGNA achieves stateof-the-art results: MAGNA achieves up to 5.7\% relative error reduction over the previous state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains the best performance on a large-scale Open Graph Benchmark dataset. On knowledge graph completion MAGNA advances state-of-the-art on WN18RR and FB15k237 across four different performance metrics.},
  eventtitle = {Thirtieth {{International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-21}}\vphantom\{\}},
  isbn = {978-0-9992411-9-6},
  langid = {english},
  file = {D\:\\Zotero\\storage\\KMH2S5NU\\Wang 等。 - 2021 - Multi-hop Attention Graph Neural Networks.pdf}
}

@inproceedings{wangReasoningAbilityScene2021,
  title = {Towards {{Reasoning Ability}} in {{Scene Text Visual Question Answering}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Multimedia}}},
  author = {Wang, Qingqing and Xiao, Liqiang and Lu, Yue and Jin, Yaohui and He, Hao},
  date = {2021-10-17},
  pages = {2281--2289},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  doi = {10.1145/3474085.3475390},
  url = {https://dl.acm.org/doi/10.1145/3474085.3475390},
  urldate = {2022-05-19},
  abstract = {Works on scene text visual question answering (TextVQA) always emphasize the importance of reasoning questions and image contents. However, we find current TextVQA models lack reasoning ability and tend to answer questions by exploiting dataset bias and language priors. Moreover, our observations indicate that recent accuracy improvement in TextVQA is mainly contributed by stronger OCR engines, better pre-training strategies and more Transformer layers, instead of newly proposed networks. In this work, towards the reasoning ability, we 1) conduct module-wise contribution analysis to quantitatively investigate how existing works improve accuracies in TextVQA; 2) design a gradient-based explainability method to explore why TextVQA models answer what they answer and find evidence for their predictions; 3) perform qualitative experiments to visually analyze models reasoning ability and explore potential reasons behind such a poor ability.},
  eventtitle = {{{MM}} '21: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-8651-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\VYKIVHUR\\Wang 等。 - 2021 - Towards Reasoning Ability in Scene Text Visual Que.pdf}
}

@unpublished{wangSimVLMSimpleVisual2021,
  title = {{{SimVLM}}: {{Simple Visual Language Model Pretraining}} with {{Weak Supervision}}},
  shorttitle = {{{SimVLM}}},
  author = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  date = {2021-08-24},
  eprint = {2108.10904},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.10904},
  urldate = {2021-08-30},
  abstract = {With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74\% vqa-score), NLVR2 (+1.17\% accuracy), SNLI-VE (+1.37\% accuracy) and image captioning tasks (+10.1\% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\Q49SABM7\\Wang 等。 - 2021 - SimVLM Simple Visual Language Model Pretraining w.pdf}
}

@unpublished{wangVDBERTUnifiedVision2020,
  title = {{{VD-BERT}}: {{A Unified Vision}} and {{Dialog Transformer}} with {{BERT}}},
  shorttitle = {{{VD-BERT}}},
  author = {Wang, Yue and Joty, Shafiq and Lyu, Michael R. and King, Irwin and Xiong, Caiming and Hoi, Steven C. H.},
  date = {2020-11-02},
  eprint = {2004.13278},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.13278},
  urldate = {2021-12-08},
  abstract = {Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\LUYCJWMV\\Wang et al_2020_VD-BERT.pdf;D\:\\Zotero\\storage\\IFMQSAKN\\2004.html}
}

@inproceedings{wangVDBERTUnifiedVision2020a,
  title = {{{VD-BERT}}: {{A Unified Vision}} and {{Dialog Transformer}} with {{BERT}}},
  shorttitle = {{{VD-BERT}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Wang, Yue and Joty, Shafiq and Lyu, Michael and King, Irwin and Xiong, Caiming and Hoi, Steven C.H.},
  date = {2020-11},
  pages = {3325--3338},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.269},
  url = {https://aclanthology.org/2020.emnlp-main.269},
  urldate = {2021-09-10},
  abstract = {Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.},
  eventtitle = {{{EMNLP}} 2020},
  keywords = {Visual Dialog},
  file = {D\:\\Zotero\\storage\\JUMLL5B3\\Wang et al_2020_VD-BERT.pdf}
}

@unpublished{wangViTAAVisualTextualAttributes2020,
  title = {{{ViTAA}}: {{Visual-Textual Attributes Alignment}} in {{Person Search}} by {{Natural Language}}},
  shorttitle = {{{ViTAA}}},
  author = {Wang, Zhe and Fang, Zhiyuan and Wang, Jun and Yang, Yezhou},
  date = {2020-07-30},
  eprint = {2005.07327},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.07327},
  urldate = {2021-10-12},
  abstract = {Person search by natural language aims at retrieving a specific person in a large-scale image pool that matches the given textual descriptions. While most of the current methods treat the task as a holistic visual and textual feature matching one, we approach it from an attribute-aligning perspective that allows grounding specific attribute phrases to the corresponding visual regions. We achieve success as well as the performance boosting by a robust feature learning that the referred identity can be accurately bundled by multiple attribute visual cues. To be concrete, our Visual-Textual Attribute Alignment model (dubbed as ViTAA) learns to disentangle the feature space of a person into subspaces corresponding to attributes using a light auxiliary attribute segmentation computing branch. It then aligns these visual features with the textual attributes parsed from the sentences by using a novel contrastive learning loss. Upon that, we validate our ViTAA framework through extensive experiments on tasks of person search by natural language and by attribute-phrase queries, on which our system achieves state-of-the-art performances. Code will be publicly available upon publication.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\DDWZJKKJ\\Wang 等。 - 2020 - ViTAA Visual-Textual Attributes Alignment in Pers.pdf;D\:\\Zotero\\storage\\WDNXK9H6\\2005.html}
}

@inproceedings{wangVoiceCoachInteractiveEvidencebased2020,
  title = {{{VoiceCoach}}: {{Interactive Evidence-based Training}} for {{Voice Modulation Skills}} in {{Public Speaking}}},
  shorttitle = {{{VoiceCoach}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Xingbo and Zeng, Haipeng and Wang, Yong and Wu, Aoyu and Sun, Zhida and Ma, Xiaojuan and Qu, Huamin},
  date = {2020-04-21},
  eprint = {2001.07876},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1--12},
  doi = {10.1145/3313831.3376726},
  url = {http://arxiv.org/abs/2001.07876},
  urldate = {2022-05-23},
  abstract = {The modulation of voice properties, such as pitch, volume, and speed, is crucial for delivering a successful public speech. However, it is challenging to master different voice modulation skills. Though many guidelines are available, they are often not practical enough to be applied in different public speaking situations, especially for novice speakers. We present VoiceCoach, an interactive evidence-based approach to facilitate the effective training of voice modulation skills. Specifically, we have analyzed the voice modulation skills from 2623 high-quality speeches (i.e., TED Talks) and use them as the benchmark dataset. Given a voice input, VoiceCoach automatically recommends good voice modulation examples from the dataset based on the similarity of both sentence structures and voice modulation skills. Immediate and quantitative visual feedback is provided to guide further improvement. The expert interviews and the user study provide support for the effectiveness and usability of VoiceCoach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\62ETM6QS\\Wang et al_2020_VoiceCoach.pdf;D\:\\Zotero\\storage\\S7FAKI6S\\2001.html}
}

@misc{wangVQAGNNReasoningMultimodal2022,
  title = {{{VQA-GNN}}: {{Reasoning}} with {{Multimodal Semantic Graph}} for {{Visual Question Answering}}},
  shorttitle = {{{VQA-GNN}}},
  author = {Wang, Yanan and Yasunaga, Michihiro and Ren, Hongyu and Wada, Shinya and Leskovec, Jure},
  date = {2022-05-23},
  number = {arXiv:2205.11501},
  eprint = {2205.11501},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.11501},
  urldate = {2022-09-06},
  abstract = {Visual understanding requires seamless integration between recognition and reasoning: beyond image-level recognition (e.g., detecting objects), systems must perform concept-level reasoning (e.g., inferring the context of objects and intents of people). However, existing methods only model the image-level features, and do not ground them and reason with background concepts such as knowledge graphs (KGs). In this work, we propose a novel visual question answering method, VQA-GNN, which unifies the image-level information and conceptual knowledge to perform joint reasoning of the scene. Specifically, given a question-image pair, we build a scene graph from the image, retrieve a relevant linguistic subgraph from ConceptNet and visual subgraph from VisualGenome, and unify these three graphs and the question into one joint graph, multimodal semantic graph. Our VQA-GNN then learns to aggregate messages and reason across different modalities captured by the multimodal semantic graph. In the evaluation on the VCR task, our method outperforms the previous scene graph-based Trans-VL models by over 4\%, and VQA-GNN-Large, our model that fuses a Trans-VL further improves the state of the art by 2\%, attaining the top of the VCR leaderboard at the time of submission. This result suggests the efficacy of our model in performing conceptual reasoning beyond image-level recognition for visual understanding. Finally, we demonstrate that our model is the first work to provide interpretability across visual and textual knowledge domains for the VQA task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\ZNIUJE4D\\Wang et al_2022_VQA-GNN.pdf;D\:\\Zotero\\storage\\DSM232YB\\2205.html}
}

@inproceedings{weiMMGCNMultimodalGraph2019,
  title = {{{MMGCN}}: {{Multi-modal Graph Convolution Network}} for {{Personalized Recommendation}} of {{Micro-video}}},
  shorttitle = {{{MMGCN}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Multimedia}}},
  author = {Wei, Yinwei and Wang, Xiang and Nie, Liqiang and He, Xiangnan and Hong, Richang and Chua, Tat-Seng},
  date = {2019-10-15},
  pages = {1437--1445},
  publisher = {{ACM}},
  location = {{Nice France}},
  doi = {10.1145/3343031.3351034},
  url = {https://dl.acm.org/doi/10.1145/3343031.3351034},
  urldate = {2022-02-10},
  eventtitle = {{{MM}} '19: {{The}} 27th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-6889-6},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BHN9K9SD\\Wei 等。 - 2019 - MMGCN Multi-modal Graph Convolution Network for P.pdf}
}

@inproceedings{weiVisualQuestionRewriting2021,
  title = {Visual {{Question Rewriting}} for {{Increasing Response Rate}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Wei, Jiayi and Li, Xilian and Zhang, Yi and Wang, Xin Eric},
  date = {2021-07-11},
  pages = {2071--2075},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3463114},
  url = {https://dl.acm.org/doi/10.1145/3404835.3463114},
  urldate = {2021-09-10},
  abstract = {When a human asks questions online, or when a conversational virtual agent asks a human questions, questions triggering emotions or with details might more likely to get responses or answers. we explore how to automatically rewrite natural language questions to improve the response rate form people. In particular, a new task of Visual Question Rewriting (VQR) task is introduced to explore how visual information can be used to improve the new question(s). A data set containing ∼4K bland\&attractive question-images triples is collected. We developed some baseline sequence to sequence models and more advanced transformer-based models, which take a bland question and a related image as input, and output a rewritten question that’s expected to be more attractive. Offline experiments and mechanical Turk based evaluations show that it’s possible to rewrite bland questions in a more detailed and attractive way to increase response rate, and images can be helpful.},
  eventtitle = {{{SIGIR}} '21: {{The}} 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\USKH5Y5M\\Wei 等。 - 2021 - Visual Question Rewriting for Increasing Response .pdf}
}

@inproceedings{wenDebiasedVisualQuestion2021,
  title = {Debiased {{Visual Question Answering}} from {{Feature}} and {{Sample Perspectives}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wen, Zhiquan and Xu, Guanghui and Tan, Mingkui and Wu, Qingyao and Wu, Qi},
  date = {2021},
  volume = {34},
  pages = {3784--3796},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2021/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Visual question answering (VQA) is designed to examine the visual-textual reasoning ability of an intelligent agent. However, recent observations show that many VQA models may only capture the biases between questions and answers in a dataset rather than showing real reasoning abilities. For example, given a question, some VQA models tend to output the answer that occurs frequently in the dataset and ignore the images. To reduce this tendency, existing methods focus on weakening the language bias. Meanwhile, only a few works also consider vision bias implicitly. However, these methods introduce additional annotations or show unsatisfactory performance. Moreover, not all biases are harmful to the models. Some “biases” learnt from datasets represent natural rules of the world and can help limit the range of answers. Thus, how to filter and remove the true negative biases in language and vision modalities remain a major challenge. In this paper, we propose a method named D-VQA to alleviate the above challenges from the feature and sample perspectives. Specifically, from the feature perspective, we build a question-to-answer and vision-to-answer branch to capture the language and vision biases, respectively. Next, we apply two unimodal bias detection modules to explicitly recognise and remove the negative biases. From the sample perspective, we construct two types of negative samples to assist the training of the models, without introducing additional annotations. Extensive experiments on the VQA-CP v2 and VQA v2 datasets demonstrate the effectiveness of our D-VQA method.},
  file = {D\:\\Zotero\\storage\\PQRXYEBW\\Wen et al_2021_Debiased Visual Question Answering from Feature and Sample Perspectives.pdf}
}

@unpublished{wengGAINGraphAttention2020,
  title = {{{GAIN}}: {{Graph Attention}} \& {{Interaction Network}} for {{Inductive Semi-Supervised Learning}} over {{Large-scale Graphs}}},
  shorttitle = {{{GAIN}}},
  author = {Weng, Yunpeng and Chen, Xu and Chen, Liang and Liu, Wei},
  date = {2020-11-02},
  eprint = {2011.01393},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.01393},
  urldate = {2022-02-05},
  abstract = {Graph Neural Networks (GNNs) have led to state-of-the-art performance on a variety of machine learning tasks such as recommendation, node classification and link prediction. Graph neural network models generate node embeddings by merging nodes features with the aggregated neighboring nodes information. Most existing GNN models exploit a single type of aggregator (e.g., mean-pooling) to aggregate neighboring nodes information, and then add or concatenate the output of aggregator to the current representation vector of the center node. However, using only a single type of aggregator is difficult to capture the different aspects of neighboring information and the simple addition or concatenation update methods limit the expressive capability of GNNs. Not only that, existing supervised or semi-supervised GNN models are trained based on the loss function of the node label, which leads to the neglect of graph structure information. In this paper, we propose a novel graph neural network architecture, Graph Attention \textbackslash\& Interaction Network (GAIN), for inductive learning on graphs. Unlike the previous GNN models that only utilize a single type of aggregation method, we use multiple types of aggregators to gather neighboring information in different aspects and integrate the outputs of these aggregators through the aggregator-level attention mechanism. Furthermore, we design a graph regularized loss to better capture the topological relationship of the nodes in the graph. Additionally, we first present the concept of graph feature interaction and propose a vector-wise explicit feature interaction mechanism to update the node embeddings. We conduct comprehensive experiments on two node-classification benchmarks and a real-world financial news dataset. The experiments demonstrate our GAIN model outperforms current state-of-the-art performances on all the tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\U5QAPMMT\\Weng et al_2020_GAIN.pdf;D\:\\Zotero\\storage\\FX95U3YG\\2011.html}
}

@article{whiteheadSeparatingSkillsConcepts,
  title = {Separating {{Skills}} and {{Concepts}} for {{Novel Visual Question Answering}}},
  author = {Whitehead, Spencer and Wu, Hui and Ji, Heng and Feris, Rogerio and Saenko, Kate and Lab, MIT-IBM Watson AI},
  pages = {14},
  langid = {english},
  file = {D\:\\Zotero\\storage\\W26VLVGE\\Whitehead 等。 - Separating Skills and Concepts for Novel Visual Qu.pdf}
}

@article{whiteheadSeparatingSkillsConceptsa,
  title = {Separating {{Skills}} and {{Concepts}} for {{Novel Visual Question Answering}}},
  author = {Whitehead, Spencer and Wu, Hui and Ji, Heng and Feris, Rogerio and Saenko, Kate},
  pages = {10},
  langid = {english},
  file = {D\:\\Zotero\\storage\\N7WWSP63\\Whitehead 等。 - Separating Skills and Concepts for Novel Visual Qu.pdf}
}

@unpublished{winterbottomModalityBiasTVQA2020,
  title = {On {{Modality Bias}} in the {{TVQA Dataset}}},
  author = {Winterbottom, Thomas and Xiao, Sarah and McLean, Alistair and Moubayed, Noura Al},
  date = {2020-12-18},
  eprint = {2012.10210},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.10210},
  urldate = {2022-04-18},
  abstract = {TVQA is a large scale video question answering (video-QA) dataset based on popular TV shows. The questions were specifically designed to require "both vision and language understanding to answer". In this work, we demonstrate an inherent bias in the dataset towards the textual subtitle modality. We infer said bias both directly and indirectly, notably finding that models trained with subtitles learn, on-average, to suppress video feature contribution. Our results demonstrate that models trained on only the visual information can answer \textasciitilde 45\% of the questions, while using only the subtitles achieves \textasciitilde 68\%. We find that a bilinear pooling based joint representation of modalities damages model performance by 9\% implying a reliance on modality specific information. We also show that TVQA fails to benefit from the RUBi modality bias reduction technique popularised in VQA. By simply improving text processing using BERT embeddings with the simple model first proposed for TVQA, we achieve state-of-the-art results (72.13\%) compared to the highly complex STAGE model (70.50\%). We recommend a multimodal evaluation framework that can highlight biases in models and isolate visual and textual reliant subsets of data. Using this framework we propose subsets of TVQA that respond exclusively to either or both modalities in order to facilitate multimodal modelling as TVQA originally intended.},
  archiveprefix = {arXiv},
  keywords = {68T99,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,I.2.10,I.2.4,I.2.7},
  file = {D\:\\Zotero\\storage\\EPPW3QIL\\Winterbottom et al_2020_On Modality Bias in the TVQA Dataset.pdf;D\:\\Zotero\\storage\\ZYQ9FXUF\\2012.html}
}

@unpublished{wuAreYouTalking2017,
  title = {Are {{You Talking}} to {{Me}}? {{Reasoned Visual Dialog Generation}} through {{Adversarial Learning}}},
  shorttitle = {Are {{You Talking}} to {{Me}}?},
  author = {Wu, Qi and Wang, Peng and Shen, Chunhua and Reid, Ian and van den Hengel, Anton},
  date = {2017-11-20},
  eprint = {1711.07613},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1711.07613},
  urldate = {2021-12-08},
  abstract = {The Visual Dialogue task requires an agent to engage in a conversation about an image with a human. It represents an extension of the Visual Question Answering task in that the agent needs to answer a question about an image, but it needs to do so in light of the previous dialogue that has taken place. The key challenge in Visual Dialogue is thus maintaining a consistent, and natural dialogue while continuing to answer questions correctly. We present a novel approach that combines Reinforcement Learning and Generative Adversarial Networks (GANs) to generate more human-like responses to questions. The GAN helps overcome the relative paucity of training data, and the tendency of the typical MLE-based approach to generate overly terse answers. Critically, the GAN is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer. This means that the discriminative model of the GAN has the task of assessing whether a candidate answer is generated by a human or not, given the provided reason. This is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning. The method also generates the state-of-the-art results on the primary benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\GH5U24JX\\Wu et al_2017_Are You Talking to Me.pdf;D\:\\Zotero\\storage\\N6JHPT69\\1711.html}
}

@article{wuChainReasoningVisual,
  title = {Chain of {{Reasoning}} for {{Visual Question Answering}}},
  author = {Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Dong, Xuan},
  pages = {11},
  abstract = {Reasoning plays an essential role in Visual Question Answering (VQA). Multi-step and dynamic reasoning is often necessary for answering complex questions. For example, a question “What is placed next to the bus on the right of the picture?” talks about a compound object “bus on the right,” which is generated by the relation {$<$}bus, on the right of, picture{$>$}. Furthermore, a new relation including this compound object {$<$}sign, next to, bus on the right{$>$} is then required to infer the answer. However, previous methods support either one-step or static reasoning, without updating relations or generating compound objects. This paper proposes a novel reasoning model for addressing these problems. A chain of reasoning (CoR) is constructed for supporting multi-step and dynamic reasoning on changed relations and objects. In detail, iteratively, the relational reasoning operations form new relations between objects, and the object refining operations generate new compound objects from relations. We achieve new state-of-the-art results on four publicly available datasets. The visualization of the chain of reasoning illustrates the progress that the CoR generates new compound objects that lead to the answer of the question step by step.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SDMWPHV2\\Wu 等。 - Chain of Reasoning for Visual Question Answering.pdf}
}

@inproceedings{wuChainReasoningVisual2018,
  title = {Chain of {{Reasoning}} for {{Visual Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Dong, Xuan},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2018/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Reasoning plays an essential role in Visual Question Answering (VQA). Multi-step and dynamic reasoning is often necessary for answering complex questions. For example, a question "What is placed next to the bus on the right of the picture?" talks about a compound object "bus on the right," which is generated by the relation . Furthermore, a new relation including this compound object  is then required to infer the answer. However, previous methods support either one-step or static reasoning, without updating relations or generating compound objects. This paper proposes a novel reasoning model for addressing these problems. A chain of reasoning (CoR) is constructed for supporting multi-step and dynamic reasoning on changed relations and objects. In detail, iteratively, the relational reasoning operations form new relations between objects, and the object refining operations generate new compound objects from relations. We achieve new state-of-the-art results on four publicly available datasets. The visualization of the chain of reasoning illustrates the progress that the CoR generates new compound objects that lead to the answer of the question step by step.},
  file = {D\:\\Zotero\\storage\\HC34SCWW\\Wu et al_2018_Chain of Reasoning for Visual Question Answering.pdf}
}

@article{wuChainReasoningVisuala,
  title = {Chain of {{Reasoning}} for {{Visual Question Answering}}},
  author = {Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Dong, Xuan},
  pages = {11},
  abstract = {Reasoning plays an essential role in Visual Question Answering (VQA). Multi-step and dynamic reasoning is often necessary for answering complex questions. For example, a question “What is placed next to the bus on the right of the picture?” talks about a compound object “bus on the right,” which is generated by the relation {$<$}bus, on the right of, picture{$>$}. Furthermore, a new relation including this compound object {$<$}sign, next to, bus on the right{$>$} is then required to infer the answer. However, previous methods support either one-step or static reasoning, without updating relations or generating compound objects. This paper proposes a novel reasoning model for addressing these problems. A chain of reasoning (CoR) is constructed for supporting multi-step and dynamic reasoning on changed relations and objects. In detail, iteratively, the relational reasoning operations form new relations between objects, and the object refining operations generate new compound objects from relations. We achieve new state-of-the-art results on four publicly available datasets. The visualization of the chain of reasoning illustrates the progress that the CoR generates new compound objects that lead to the answer of the question step by step.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\XM6J5VVP\\Wu 等。 - Chain of Reasoning for Visual Question Answering.pdf}
}

@article{wuDifferentialNetworksVisual,
  title = {Differential {{Networks}} for {{Visual Question Answering}}},
  author = {Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Li, Ruifan},
  pages = {8},
  abstract = {The task of Visual Question Answering (VQA) has emerged in recent years for its potential applications. To address the VQA task, the model should fuse feature elements from both images and questions efficiently. Existing models fuse image feature element vi and question feature element qi directly, such as an element product viqi. Those solutions largely ignore the following two key points: 1) Whether vi and qi are in the same space. 2) How to reduce the observation noises in vi and qi. We argue that two differences between those two feature elements themselves, like (vi − vj) and (qi − qj), are more probably in the same space. And the difference operation would be beneficial to reduce observation noise. To achieve this, we first propose Differential Networks (DN), a novel plug-and-play module which enables differences between pair-wise feature elements. With the tool of DN, we then propose DN based Fusion (DF), a novel model for VQA task. We achieve state-of-the-art results on four publicly available datasets. Ablation studies also show the effectiveness of difference operations in DF model.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\YHWCQFDA\\Wu 等。 - Differential Networks for Visual Question Answerin.pdf}
}

@inproceedings{wuGeneratingQuestionRelevant2019,
  title = {Generating {{Question Relevant Captions}} to {{Aid Visual Question Answering}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Jialin and Hu, Zeyuan and Mooney, Raymond},
  date = {2019-07},
  pages = {3585--3594},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1348},
  url = {https://aclanthology.org/P19-1348},
  urldate = {2021-09-10},
  abstract = {Visual question answering (VQA) and image captioning require a shared body of general knowledge connecting language and vision. We present a novel approach to better VQA performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question. The model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method. Experimental results on the VQA v2 challenge demonstrates that our approach obtains state-of-the-art VQA performance (e.g. 68.4\% in the Test-standard set using a single model) by simultaneously generating question-relevant captions.},
  eventtitle = {{{ACL}} 2019},
  file = {D\:\\Zotero\\storage\\NNJXJGPL\\Wu et al_2019_Generating Question Relevant Captions to Aid Visual Question Answering.pdf}
}

@unpublished{wuGINetGraphInteraction2020,
  title = {{{GINet}}: {{Graph Interaction Network}} for {{Scene Parsing}}},
  shorttitle = {{{GINet}}},
  author = {Wu, Tianyi and Lu, Yu and Zhu, Yu and Zhang, Chuang and Wu, Ming and Ma, Zhanyu and Guo, Guodong},
  date = {2020-09-13},
  eprint = {2009.06160},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2009.06160},
  urldate = {2022-02-05},
  abstract = {Recently, context reasoning using image regions beyond local convolution has shown great potential for scene parsing. In this work, we explore how to incorporate the linguistic knowledge to promote context reasoning over image regions by proposing a Graph Interaction unit (GI unit) and a Semantic Context Loss (SC-loss). The GI unit is capable of enhancing feature representations of convolution networks over high-level semantics and learning the semantic coherency adaptively to each sample. Specifically, the dataset-based linguistic knowledge is first incorporated in the GI unit to promote context reasoning over the visual graph, then the evolved representations of the visual graph are mapped to each local representation to enhance the discriminated capability for scene parsing. GI unit is further improved by the SC-loss to enhance the semantic representations over the exemplar-based semantic graph. We perform full ablation studies to demonstrate the effectiveness of each component in our approach. Particularly, the proposed GINet outperforms the state-of-the-art approaches on the popular benchmarks, including Pascal-Context and COCO Stuff.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\HNZXY9L6\\Wu et al_2020_GINet.pdf;D\:\\Zotero\\storage\\9YJ8J2FR\\2009.html}
}

@unpublished{wuGraphCapsuleAggregation2021,
  title = {Graph {{Capsule Aggregation}} for {{Unaligned Multimodal Sequences}}},
  author = {Wu, Jianfeng and Mai, Sijie and Hu, Haifeng},
  date = {2021-08-17},
  eprint = {2108.07543},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3462244.3479931},
  url = {http://arxiv.org/abs/2108.07543},
  urldate = {2021-10-04},
  abstract = {Humans express their opinions and emotions through multiple modalities which mainly consist of textual, acoustic and visual modalities. Prior works on multimodal sentiment analysis mostly apply Recurrent Neural Network (RNN) to model aligned multimodal sequences. However, it is unpractical to align multimodal sequences due to different sample rates for different modalities. Moreover, RNN is prone to the issues of gradient vanishing or exploding and it has limited capacity of learning long-range dependency which is the major obstacle to model unaligned multimodal sequences. In this paper, we introduce Graph Capsule Aggregation (GraphCAGE) to model unaligned multimodal sequences with graph-based neural model and Capsule Network. By converting sequence data into graph, the previously mentioned problems of RNN are avoided. In addition, the aggregation capability of Capsule Network and the graph-based structure enable our model to be interpretable and better solve the problem of long-range dependency. Experimental results suggest that GraphCAGE achieves state-of-the-art performance on two benchmark datasets with representations refined by Capsule Network and interpretation provided.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\HU4GK9CK\\Wu et al_2021_Graph Capsule Aggregation for Unaligned Multimodal Sequences.pdf;D\:\\Zotero\\storage\\958X3MMJ\\2108.html}
}

@article{wuGraphConvolutionMachine2022,
  title = {Graph {{Convolution Machine}} for {{Context-aware Recommender System}}},
  author = {Wu, Jiancan and He, Xiangnan and Wang, Xiang and Wang, Qifan and Chen, Weijian and Lian, Jianxun and Xie, Xing},
  date = {2022-12},
  journaltitle = {Frontiers of Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {16},
  number = {6},
  eprint = {2001.11402},
  eprinttype = {arxiv},
  pages = {166614},
  issn = {2095-2228, 2095-2236},
  doi = {10.1007/s11704-021-0261-8},
  url = {http://arxiv.org/abs/2001.11402},
  urldate = {2022-03-23},
  abstract = {The latest advance in recommendation shows that better user and item representations can be learned via performing graph convolutions on the user-item interaction graph. However, such finding is mostly restricted to the collaborative filtering (CF) scenario, where the interaction contexts are not available. In this work, we extend the advantages of graph convolutions to context-aware recommender system (CARS, which represents a generic type of models that can handle various side information). We propose \textbackslash textit\{Graph Convolution Machine\} (GCM), an end-to-end framework that consists of three components: an encoder, graph convolution (GC) layers, and a decoder. The encoder projects users, items, and contexts into embedding vectors, which are passed to the GC layers that refine user and item embeddings with context-aware graph convolutions on user-item graph. The decoder digests the refined embeddings to output the prediction score by considering the interactions among user, item, and context embeddings. We conduct experiments on three real-world datasets from Yelp and Amazon, validating the effectiveness of GCM and the benefits of performing graph convolutions for CARS. Our implementations are available at \textbackslash url\{https://github.com/wujcan/GCM\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\8ZTAVSBP\\Wu et al_2022_Graph Convolution Machine for Context-aware Recommender System.pdf;D\:\\Zotero\\storage\\ZB3BJGNE\\2001.html}
}

@unpublished{wuImageCaptioningVisual2016,
  title = {Image {{Captioning}} and {{Visual Question Answering Based}} on {{Attributes}} and {{External Knowledge}}},
  author = {Wu, Qi and Shen, Chunhua and van den Hengel, Anton and Wang, Peng and Dick, Anthony},
  date = {2016-12-16},
  eprint = {1603.02814},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1603.02814},
  urldate = {2021-09-10},
  abstract = {Much recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we first propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We further show that the same mechanism can be used to incorporate external knowledge, which is critically important for answering high level visual questions. Specifically, we design a visual question answering model that combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain a complete answer. Our final model achieves the best reported results on both image captioning and visual question answering on several benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Knowledge},
  file = {D\:\\Zotero\\storage\\B6ZJBDDM\\Wu et al_2016_Image Captioning and Visual Question Answering Based on Attributes and External.pdf;D\:\\Zotero\\storage\\TSI9GMP7\\1603.html}
}

@unpublished{wuLearningFairRepresentations2021,
  title = {Learning {{Fair Representations}} for {{Recommendation}}: {{A Graph-based Perspective}}},
  shorttitle = {Learning {{Fair Representations}} for {{Recommendation}}},
  author = {Wu, Le and Chen, Lei and Shao, Pengyang and Hong, Richang and Wang, Xiting and Wang, Meng},
  date = {2021-04-23},
  eprint = {2102.09140},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.09140},
  urldate = {2022-03-23},
  abstract = {As a key application of artificial intelligence, recommender systems are among the most pervasive computer aided systems to help users find potential items of interests. Recently, researchers paid considerable attention to fairness issues for artificial intelligence applications. Most of these approaches assumed independence of instances, and designed sophisticated models to eliminate the sensitive information to facilitate fairness. However, recommender systems differ greatly from these approaches as users and items naturally form a user-item bipartite graph, and are collaboratively correlated in the graph structure. In this paper, we propose a novel graph based technique for ensuring fairness of any recommendation models. Here, the fairness requirements refer to not exposing sensitive feature set in the user modeling process. Specifically, given the original embeddings from any recommendation models, we learn a composition of filters that transform each user's and each item's original embeddings into a filtered embedding space based on the sensitive feature set. For each user, this transformation is achieved under the adversarial learning of a user-centric graph, in order to obfuscate each sensitive feature between both the filtered user embedding and the sub graph structures of this user. Finally, extensive experimental results clearly show the effectiveness of our proposed model for fair recommendation. We publish the source code at https://github.com/newlei/FairGo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\ERNQWURS\\Wu et al_2021_Learning Fair Representations for Recommendation.pdf;D\:\\Zotero\\storage\\WW4YYIN8\\2102.html}
}

@article{wuMultimodalAnalysisVideo2020,
  title = {Multimodal {{Analysis}} of {{Video Collections}}: {{Visual Exploration}} of {{Presentation Techniques}} in {{TED Talks}}},
  shorttitle = {Multimodal {{Analysis}} of {{Video Collections}}},
  author = {Wu, Aoyu and Qu, Huamin},
  date = {2020-07-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  volume = {26},
  number = {7},
  pages = {2429--2442},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2018.2889081},
  url = {https://ieeexplore.ieee.org/document/8585103/},
  urldate = {2022-05-23},
  abstract = {While much research in the educational field has revealed many presentation techniques, they often overlap and are even occasionally contradictory. Exploring presentation techniques used in TED Talks could provide evidence for a practical guideline. This study aims to explore the verbal and non-verbal presentation techniques from a collection of TED Talks. However, such analysis is challenging due to the difficulties of analyzing multimodal video collections consisted of frame images, text, and metadata. This paper proposes a visual analytic system to analyze multimodal content in video collections. The system features three views at different levels: the Projection view with novel glyphs to facilitate cluster analysis regarding presentation styles; the Comparison View to present temporal distribution and concurrences of presentation techniques and support intra-cluster analysis; and the Video View to enable contextualized exploration of a video. We conduct a case study with language education experts and university students to provide anecdotal evidence about the effectiveness of our approach, and report new findings about presentation techniques in TED Talks. Quantitative feedback from a user study confirms the usefulness of our visual system for multimodal analysis of video collections.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\9B7UE3SQ\\Wu 和 Qu - 2020 - Multimodal Analysis of Video Collections Visual E.pdf}
}

@unpublished{wuMultiModalAnswerValidation2021,
  title = {Multi-{{Modal Answer Validation}} for {{Knowledge-Based VQA}}},
  author = {Wu, Jialin and Lu, Jiasen and Sabharwal, Ashish and Mottaghi, Roozbeh},
  date = {2021-12-13},
  eprint = {2103.12248},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.12248},
  urldate = {2022-04-14},
  abstract = {The problem of knowledge-based visual question answering involves answering questions that require external knowledge in addition to the content of the image. Such knowledge typically comes in various forms, including visual, textual, and commonsense knowledge. Using more knowledge sources increases the chance of retrieving more irrelevant or noisy facts, making it challenging to comprehend the facts and find the answer. To address this challenge, we propose Multi-modal Answer Validation using External knowledge (MAVEx), where the idea is to validate a set of promising answer candidates based on answer-specific knowledge retrieval. Instead of searching for the answer in a vast collection of often irrelevant facts as most existing approaches do, MAVEx aims to learn how to extract relevant knowledge from noisy sources, which knowledge source to trust for each answer candidate, and how to validate the candidate using that source. Our multi-modal setting is the first to leverage external visual knowledge (images searched using Google), in addition to textual knowledge in the form of Wikipedia sentences and ConceptNet concepts. Our experiments with OK-VQA, a challenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new state-of-the-art results. Our code is available at https://github.com/jialinwu17/MAVEX},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\M6KYNH67\\Wu et al_2021_Multi-Modal Answer Validation for Knowledge-Based VQA.pdf;D\:\\Zotero\\storage\\ASVZIJ8J\\2103.html}
}

@article{wuMultimodalAttentionFusion2022,
  title = {A Multimodal Attention Fusion Network with a Dynamic Vocabulary for {{TextVQA}}},
  author = {Wu, Jiajia and Du, Jun and Wang, Fengren and Yang, Chen and Jiang, Xinzhe and Hu, Jinshui and Yin, Bing and Zhang, Jianshu and Dai, Lirong},
  date = {2022-02},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {122},
  pages = {108214},
  issn = {00313203},
  doi = {10.1016/j.patcog.2021.108214},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321003952},
  urldate = {2022-03-24},
  langid = {english},
  file = {D\:\\Zotero\\storage\\9RGMBN4T\\Wu 等。 - 2022 - A multimodal attention fusion network with a dynam.pdf}
}

@article{wuMultimodalAttentionFusion2022a,
  title = {A Multimodal Attention Fusion Network with a Dynamic Vocabulary for {{TextVQA}}},
  author = {Wu, Jiajia and Du, Jun and Wang, Fengren and Yang, Chen and Jiang, Xinzhe and Hu, Jinshui and Yin, Bing and Zhang, Jianshu and Dai, Lirong},
  date = {2022-02},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {122},
  pages = {108214},
  issn = {00313203},
  doi = {10.1016/j.patcog.2021.108214},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321003952},
  urldate = {2022-02-08},
  langid = {english},
  file = {D\:\\Zotero\\storage\\U9GN6HSG\\Wu 等。 - 2022 - A multimodal attention fusion network with a dynam.pdf}
}

@unpublished{wuRepresentingLongRangeContext2022,
  title = {Representing {{Long-Range Context}} for {{Graph Neural Networks}} with {{Global Attention}}},
  author = {Wu, Zhanghao and Jain, Paras and Wright, Matthew A. and Mirhoseini, Azalia and Gonzalez, Joseph E. and Stoica, Ion},
  date = {2022-01-21},
  eprint = {2201.08821},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.08821},
  urldate = {2022-03-26},
  abstract = {Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel "readout" mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\37UHSCHF\\Wu et al_2022_Representing Long-Range Context for Graph Neural Networks with Global Attention.pdf;D\:\\Zotero\\storage\\563A3ZTL\\2201.html}
}

@misc{wuRFMaskSimpleBaseline2022,
  title = {{{RFMask}}: {{A Simple Baseline}} for {{Human Silhouette Segmentation}} with {{Radio Signals}}},
  shorttitle = {{{RFMask}}},
  author = {Wu, Zhi and Zhang, Dongheng and Xie, Chunyang and Yu, Cong and Chen, Jinbo and Hu, Yang and Chen, Yan},
  date = {2022-01-25},
  number = {arXiv:2201.10175},
  eprint = {2201.10175},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.10175},
  urldate = {2022-07-26},
  abstract = {Human silhouette segmentation, which is originally defined in computer vision, has achieved promising results for understanding human activities. However, the physical limitation makes existing systems based on optical cameras suffer from severe performance degradation under low illumination, smoke, and/or opaque obstruction conditions. To overcome such limitations, in this paper, we propose to utilize the radio signals, which can traverse obstacles and are unaffected by the lighting conditions to achieve silhouette segmentation. The proposed RFMask framework is composed of three modules. It first transforms RF signals captured by millimeter wave radar on two planes into spatial domain and suppress interference with the signal processing module. Then, it locates human reflections on RF frames and extract features from surrounding signals with human detection module. Finally, the extracted features from RF frames are aggregated with an attention based mask generation module. To verify our proposed framework, we collect a dataset containing 804,760 radio frames and 402,380 camera frames with human activities under various scenes. Experimental results show that the proposed framework can achieve impressive human silhouette segmentation even under the challenging scenarios(such as low light and occlusion scenarios) where traditional optical-camera-based methods fail. To the best of our knowledge, this is the first investigation towards segmenting human silhouette based on millimeter wave signals. We hope that our work can serve as a baseline and inspire further research that perform vision tasks with radio signals. The dataset and codes will be made in public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\TF5F9B5Q\\Wu et al_2022_RFMask.pdf;D\:\\Zotero\\storage\\UFMZQAEJ\\2201.html}
}

@inproceedings{wuSelfCriticalReasoningRobust2019,
  title = {Self-{{Critical Reasoning}} for {{Robust Visual Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wu, Jialin and Mooney, Raymond},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2019/hash/33b879e7ab79f56af1e88359f9314a10-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Visual Question Answering (VQA) deep-learning systems tend to capture superficial statistical correlations in the training data because of strong language priors and fail to generalize to test data with a significantly different question-answer (QA) distribution. To address this issue, we introduce a self-critical training objective that ensures that visual explanations of correct answers match the most influential image regions more than other competitive answer candidates. The influential regions are either determined from human visual/textual explanations or automatically from just significant words in the question and answer. We evaluate our approach on the VQA generalization task using the VQA-CP dataset, achieving a new state-of-the-art i.e. 49.5\textbackslash\% using textual explanations and 48.5\textbackslash\% using automatically},
  file = {D\:\\Zotero\\storage\\EVEZBYMD\\Wu_Mooney_2019_Self-Critical Reasoning for Robust Visual Question Answering.pdf}
}

@inproceedings{wuSelfCriticalReasoningRobust2019a,
  title = {Self-{{Critical Reasoning}} for {{Robust Visual Question Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wu, Jialin and Mooney, Raymond},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2019/hash/33b879e7ab79f56af1e88359f9314a10-Abstract.html},
  urldate = {2021-09-10},
  keywords = {Reasoning},
  file = {D\:\\Zotero\\storage\\3YEWH6LR\\Wu_Mooney_2019_Self-Critical Reasoning for Robust Visual Question Answering.pdf}
}

@unpublished{wuVisualQuestionAnswering2016,
  title = {Visual {{Question Answering}}: {{A Survey}} of {{Methods}} and {{Datasets}}},
  shorttitle = {Visual {{Question Answering}}},
  author = {Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
  date = {2016-07-20},
  eprint = {1607.05910},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1607.05910},
  urldate = {2021-03-09},
  abstract = {Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\XZ82FJHQ\\Wu 等。 - 2016 - Visual Question Answering A Survey of Methods and.pdf}
}

@article{xiaKnowledgeEnhancedHierarchicalGraph,
  title = {Knowledge-{{Enhanced Hierarchical Graph Transformer Network}} for {{Multi-Behavior Recommendation}}},
  author = {Xia, Lianghao and Huang, Chao and Xu, Yong and Dai, Peng and Zhang, Xiyue and Yang, Hongsheng and Pei, Jian and Bo, Liefeng},
  pages = {8},
  abstract = {Accurate user and item embedding learning is crucial for modern recommender systems. However, most existing recommendation techniques have thus far focused on modeling users’ preferences over singular type of user-item interactions. Many practical recommendation scenarios involve multi-typed user interactive behaviors (e.g., page view, addto-favorite and purchase), which presents unique challenges that cannot be handled by current recommendation solutions. In particular: i) complex inter-dependencies across different types of user behaviors; ii) the incorporation of knowledgeaware item relations into the multi-behavior recommendation framework; iii) dynamic characteristics of multityped user-item interactions. To tackle these challenges, this work proposes a Knowledge-Enhanced Hierarchical Graph Transformer Network (KHGT), to investigate multi-typed interactive patterns between users and items in recommender systems. Specifically, KHGT is built upon a graph-structured neural architecture to i) capture type-specific behavior characteristics; ii) explicitly discriminate which types of user-item interactions are more important in assisting the forecasting task on the target behavior. Additionally, we further integrate the graph attention layer with the temporal encoding strategy, to empower the learned embeddings be reflective of both dedicated multiplex user-item and item-item relations, as well as the underlying interaction dynamics. Extensive experiments conducted on three real-world datasets show that KHGT consistently outperforms many state-of-the-art recommendation methods across various evaluation settings. Our implementation code is available in https://github.com/akaxlh/KHGT.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\X9JS7IKW\\Xia 等。 - Knowledge-Enhanced Hierarchical Graph Transformer .pdf}
}

@online{xianhuatalentZoteroJiChuCaoZuoBiEndnoteGengHaoYongDeWenXianGuanLiRuanJianBiLiBiLi,
  title = {{{Zotero基础操作}}，{{比Endnote更好用的文献管理软件}}\_哔哩哔哩\_bilibili},
  author = {显华Talent},
  url = {https://www.bilibili.com/video/BV1ZE411p7qT/},
  urldate = {2021-08-05},
  abstract = {Zotero 是一款非常好用的文献管理软件，本视频介绍Zotero的基础操作包括：安装与注册、文献导入、添加标签、添加笔记、添加插件、数据管理与同步、参考文献引用。Zotero进阶操作，请看：https://www.bilibili.com/video/BV1c54y1d7mx/Zotero官网：https://www.zotero.org/},
  file = {D\:\\Zotero\\storage\\UFI2U7LG\\BV1ZE411p7qT.html}
}

@unpublished{xingLearningHierarchicalGraph2021,
  title = {Learning {{Hierarchical Graph Neural Networks}} for {{Image Clustering}}},
  author = {Xing, Yifan and He, Tong and Xiao, Tianjun and Wang, Yongxin and Xiong, Yuanjun and Xia, Wei and Wipf, David and Zhang, Zheng and Soatto, Stefano},
  date = {2021-07-17},
  eprint = {2107.01319},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.01319},
  urldate = {2021-10-10},
  abstract = {We propose a hierarchical graph neural network (GNN) model that learns how to cluster a set of images into an unknown number of identities using a training set of images annotated with labels belonging to a disjoint set of identities. Our hierarchical GNN uses a novel approach to merge connected components predicted at each level of the hierarchy to form a new graph at the next level. Unlike fully unsupervised hierarchical clustering, the choice of grouping and complexity criteria stems naturally from supervision in the training set. The resulting method, Hi-LANDER, achieves an average of 54\% improvement in F-score and 8\% increase in Normalized Mutual Information (NMI) relative to current GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based methods rely on separate models to predict linkage probabilities and node densities as intermediate steps of the clustering process. In contrast, our unified framework achieves a seven-fold decrease in computational cost. We release our training and inference code at https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\F76MX55H\\Xing et al_2021_Learning Hierarchical Graph Neural Networks for Image Clustering.pdf;D\:\\Zotero\\storage\\469LRAAC\\2107.html}
}

@article{xinyiCAPSULEGRAPHNEURAL2019,
  title = {{{CAPSULE GRAPH NEURAL NETWORK}}},
  author = {Xinyi, Zhang and Chen, Lihui},
  date = {2019},
  pages = {16},
  abstract = {The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representations may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\MR6NM92Z\\Xinyi 和 Chen - 2019 - CAPSULE GRAPH NEURAL NETWORK.pdf}
}

@unpublished{xiongMGAVQAMultiGranularityAlignment2022,
  title = {{{MGA-VQA}}: {{Multi-Granularity Alignment}} for {{Visual Question Answering}}},
  shorttitle = {{{MGA-VQA}}},
  author = {Xiong, Peixi and Shen, Yilin and Jin, Hongxia},
  date = {2022-01-25},
  eprint = {2201.10656},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.10656},
  urldate = {2022-03-15},
  abstract = {Learning to answer visual questions is a challenging task since the multi-modal inputs are within two feature spaces. Moreover, reasoning in visual question answering requires the model to understand both image and question, and align them in the same space, rather than simply memorize statistics about the question-answer pairs. Thus, it is essential to find component connections between different modalities and within each modality to achieve better attention. Previous works learned attention weights directly on the features. However, the improvement is limited since these two modality features are in two domains: image features are highly diverse, lacking structure and grammatical rules as language, and natural language features have a higher probability of missing detailed information. To better learn the attention between visual and text, we focus on how to construct input stratification and embed structural information to improve the alignment between different level components. We propose Multi-Granularity Alignment architecture for Visual Question Answering task (MGA-VQA), which learns intra- and inter-modality correlations by multi-granularity alignment, and outputs the final result by the decision fusion module. In contrast to previous works, our model splits alignment into different levels to achieve learning better correlations without needing additional data and annotations. The experiments on the VQA-v2 and GQA datasets demonstrate that our model significantly outperforms non-pretrained state-of-the-art methods on both datasets without extra pretraining data and annotations. Moreover, it even achieves better results over the pre-trained methods on GQA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\RZW4IBD7\\Xiong et al_2022_MGA-VQA.pdf;D\:\\Zotero\\storage\\IKFDIQ8I\\2201.html}
}

@unpublished{xiongMGAVQAMultiGranularityAlignment2022a,
  title = {{{MGA-VQA}}: {{Multi-Granularity Alignment}} for {{Visual Question Answering}}},
  shorttitle = {{{MGA-VQA}}},
  author = {Xiong, Peixi and Shen, Yilin and Jin, Hongxia},
  date = {2022-01-25},
  eprint = {2201.10656},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.10656},
  urldate = {2022-02-10},
  abstract = {Learning to answer visual questions is a challenging task since the multi-modal inputs are within two feature spaces. Moreover, reasoning in visual question answering requires the model to understand both image and question, and align them in the same space, rather than simply memorize statistics about the question-answer pairs. Thus, it is essential to find component connections between different modalities and within each modality to achieve better attention. Previous works learned attention weights directly on the features. However, the improvement is limited since these two modality features are in two domains: image features are highly diverse, lacking structure and grammatical rules as language, and natural language features have a higher probability of missing detailed information. To better learn the attention between visual and text, we focus on how to construct input stratification and embed structural information to improve the alignment between different level components. We propose Multi-Granularity Alignment architecture for Visual Question Answering task (MGA-VQA), which learns intra- and inter-modality correlations by multi-granularity alignment, and outputs the final result by the decision fusion module. In contrast to previous works, our model splits alignment into different levels to achieve learning better correlations without needing additional data and annotations. The experiments on the VQA-v2 and GQA datasets demonstrate that our model significantly outperforms non-pretrained state-of-the-art methods on both datasets without extra pretraining data and annotations. Moreover, it even achieves better results over the pre-trained methods on GQA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\TQC3L5U7\\Xiong et al_2022_MGA-VQA.pdf;D\:\\Zotero\\storage\\UIW5T5JK\\2201.html}
}

@unpublished{xiongSAVQAStructuredAlignment2022,
  title = {{{SA-VQA}}: {{Structured Alignment}} of {{Visual}} and {{Semantic Representations}} for {{Visual Question Answering}}},
  shorttitle = {{{SA-VQA}}},
  author = {Xiong, Peixi and You, Quanzeng and Yu, Pei and Liu, Zicheng and Wu, Ying},
  date = {2022-01-25},
  eprint = {2201.10654},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2201.10654},
  urldate = {2022-02-05},
  abstract = {Visual Question Answering (VQA) attracts much attention from both industry and academia. As a multi-modality task, it is challenging since it requires not only visual and textual understanding, but also the ability to align cross-modality representations. Previous approaches extensively employ entity-level alignments, such as the correlations between the visual regions and their semantic labels, or the interactions across question words and object features. These attempts aim to improve the cross-modality representations, while ignoring their internal relations. Instead, we propose to apply structured alignments, which work with graph representation of visual and textual content, aiming to capture the deep connections between the visual and textual modalities. Nevertheless, it is nontrivial to represent and integrate graphs for structured alignments. In this work, we attempt to solve this issue by first converting different modality entities into sequential nodes and the adjacency graph, then incorporating them for structured alignments. As demonstrated in our experimental results, such a structured alignment improves reasoning performance. In addition, our model also exhibits better interpretability for each generated answer. The proposed model, without any pretraining, outperforms the state-of-the-art methods on GQA dataset, and beats the non-pretrained state-of-the-art methods on VQA-v2 dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\3FZLQL6N\\Xiong et al_2022_SA-VQA.pdf;D\:\\Zotero\\storage\\2XKD8PID\\2201.html}
}

@inproceedings{xiongTAStudentVQAMultiAgents2020,
  title = {{{TA-Student VQA}}: {{Multi-Agents Training}} by {{Self-Questioning}}},
  shorttitle = {{{TA-Student VQA}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xiong, Peixi and Wu, Ying},
  date = {2020-06},
  pages = {10062--10072},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01008},
  url = {https://ieeexplore.ieee.org/document/9157453/},
  urldate = {2021-09-09},
  abstract = {There are two main challenges in Visual Question Answering (VQA). The first one is that each model obtains its strengths and shortcomings when applied to several questions; what is more, the “ceiling effect” for specific questions is difficult to overcome with simple consecutive training. The second challenge is that even the state-of-the-art dataset is of large scale, questions targeted at a single image are off in format and lack diversity in content. We introduce our self-questioning model with multi-agent training: TA-student VQA. This framework differs from standard VQA algorithms by involving question-generating mechanisms and collaborative learning between question-answering agents. Thus, TA-student VQA overcomes the limitation of the content diversity and format variation of questions and improves the overall performance of multiple question-answering agents. We evaluate our model on VQA-v2 [1], which outperforms algorithms without such mechanisms. In addition, TA-student VQA achieves a greater model capacity, allowing it to answer more generated questions in addition to those in the annotated datasets.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {0 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\5UW4E5VJ\\Xiong 和 Wu - 2020 - TA-Student VQA Multi-Agents Training by Self-Ques.pdf}
}

@inproceedings{xiongVisualQueryAnswering2019,
  title = {Visual {{Query Answering}} by {{Entity-Attribute Graph Matching}} and {{Reasoning}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xiong, Peixi and Zhan, Huayi and Wang, Xin and Sinha, Baivab and Wu, Ying},
  date = {2019-06},
  pages = {8349--8358},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00855},
  url = {https://ieeexplore.ieee.org/document/8954248/},
  urldate = {2021-09-09},
  abstract = {Visual Query Answering (VQA) is of great significance in offering people convenience: one can raise a question for details of objects, or high-level understanding about the scene, over an image. This paper proposes a novel method to address the VQA problem. In contrast to prior works, our method that targets single scene VQA, replies on graphbased techniques and involves reasoning. In a nutshell, our approach is centered on three graphs. The first graph, referred to as inference graph GI , is constructed via learning over labeled data. The other two graphs, referred to as query graph Q and entity-attribute graph GEA, are generated from natural language query Qnl and image Img, that are issued from users, respectively. As GEA often does not take sufficient information to answer Q, we develop techniques to infer missing information of GEA with GI . Based on GEA and Q, we provide techniques to find matches of Q in GEA, as the answer of Qnl in Img. Unlike commonly used VQA methods that are based on end-to-end neural networks, our graph-based method shows well-designed reasoning capability, and thus is highly interpretable. We also create a dataset on soccer match (Soccer-VQA)1 with rich annotations. The experimental results show that our approach outperforms the state-of-the-art method and has high potential for future investigation.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {6 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\FGCGX5J3\\Xiong 等。 - 2019 - Visual Query Answering by Entity-Attribute Graph M.pdf}
}

@unpublished{xuAccurateTextbasedImage2021,
  title = {Towards {{Accurate Text-based Image Captioning}} with {{Content Diversity Exploration}}},
  author = {Xu, Guanghui and Niu, Shuaicheng and Tan, Mingkui and Luo, Yucheng and Du, Qing and Wu, Qi},
  date = {2021-04-23},
  eprint = {2105.03236},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.03236},
  urldate = {2022-03-29},
  abstract = {Text-based image captioning (TextCap) which aims to read and reason images with texts is crucial for a machine to understand a detailed and complex scene environment, considering that texts are omnipresent in daily life. This task, however, is very challenging because an image often contains complex texts and visual information that is hard to be described comprehensively. Existing methods attempt to extend the traditional image captioning methods to solve this task, which focus on describing the overall scene of images by one global caption. This is infeasible because the complex text and visual information cannot be described well within one caption. To resolve this difficulty, we seek to generate multiple captions that accurately describe different parts of an image in detail. To achieve this purpose, there are three key challenges: 1) it is hard to decide which parts of the texts of images to copy or paraphrase; 2) it is non-trivial to capture the complex relationship between diverse texts in an image; 3) how to generate multiple captions with diverse content is still an open problem. To conquer these, we propose a novel Anchor-Captioner method. Specifically, we first find the important tokens which are supposed to be paid more attention to and consider them as anchors. Then, for each chosen anchor, we group its relevant texts to construct the corresponding anchor-centred graph (ACG). Last, based on different ACGs, we conduct multi-view caption generation to improve the content diversity of generated captions. Experimental results show that our method not only achieves SOTA performance but also generates diverse captions to describe images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\G63L99V5\\Xu et al_2021_Towards Accurate Text-based Image Captioning with Content Diversity Exploration.pdf;D\:\\Zotero\\storage\\M9A8KGVM\\[REPE]2022_ICLR_Graph neural networks with learnable structural.pdf;D\:\\Zotero\\storage\\ZRKRUBSM\\2105.html}
}

@misc{xuCLIPDiffusionLMApplyDiffusion2022,
  title = {{{CLIP-Diffusion-LM}}: {{Apply Diffusion Model}} on {{Image Captioning}}},
  shorttitle = {{{CLIP-Diffusion-LM}}},
  author = {Xu, Shitong},
  date = {2022-10-10},
  number = {arXiv:2210.04559},
  eprint = {2210.04559},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.04559},
  urldate = {2022-10-11},
  abstract = {Image captioning task has been extensively researched by previous work. However, limited experiments focus on generating captions based on non-autoregressive text decoder. Inspired by the recent success of the denoising diffusion model on image synthesis tasks, we apply denoising diffusion probabilistic models to text generation in image captioning tasks. We show that our CLIP-Diffusion-LM is capable of generating image captions using significantly fewer inference steps than autoregressive models. On the Flickr8k dataset, the model achieves 0.1876 BLEU-4 score. By training on the combined Flickr8k and Flickr30k dataset, our model achieves 0.2470 BLEU-4 score. Our code is available at https://github.com/xu-shitong/diffusion-image-captioning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\748WNEDN\\Xu_2022_CLIP-Diffusion-LM.pdf;D\:\\Zotero\\storage\\NYS3DZ3G\\2210.html}
}

@inproceedings{xuE2EVLPEndtoEndVisionLanguage2021,
  title = {{{E2E-VLP}}: {{End-to-End Vision-Language Pre-training Enhanced}} by {{Visual Learning}}},
  shorttitle = {{{E2E-VLP}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Xu, Haiyang and Yan, Ming and Li, Chenliang and Bi, Bin and Huang, Songfang and Xiao, Wenming and Huang, Fei},
  date = {2021-08},
  pages = {503--513},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-long.42},
  url = {https://aclanthology.org/2021.acl-long.42},
  urldate = {2021-08-16},
  abstract = {Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  file = {D\:\\Zotero\\storage\\LPJAZ7IH\\Xu et al_2021_E2E-VLP.pdf}
}

@misc{xuFourierbasedFrameworkDomain2021,
  title = {A {{Fourier-based Framework}} for {{Domain Generalization}}},
  author = {Xu, Qinwei and Zhang, Ruipeng and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
  date = {2021-05-24},
  number = {arXiv:2105.11120},
  eprint = {2105.11120},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.11120},
  urldate = {2022-06-07},
  abstract = {Modern deep neural networks suffer from performance degradation when evaluated on testing data under different distributions from training data. Domain generalization aims at tackling this problem by learning transferable knowledge from multiple source domains in order to generalize to unseen target domains. This paper introduces a novel Fourier-based perspective for domain generalization. The main assumption is that the Fourier phase information contains high-level semantics and is not easily affected by domain shifts. To force the model to capture phase information, we develop a novel Fourier-based data augmentation strategy called amplitude mix which linearly interpolates between the amplitude spectrums of two images. A dual-formed consistency loss called co-teacher regularization is further introduced between the predictions induced from original and augmented images. Extensive experiments on three benchmarks have demonstrated that the proposed method is able to achieve state-of-the-arts performance for domain generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\8ZSPW2EY\\Xu 等。 - 2021 - A Fourier-based Framework for Domain Generalizatio.pdf}
}

@misc{xuShowAttendTell2016,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  date = {2016-04-19},
  number = {arXiv:1502.03044},
  eprint = {1502.03044},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1502.03044},
  urldate = {2022-09-25},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\7WIJKCQS\\Xu et al_2016_Show, Attend and Tell.pdf;D\:\\Zotero\\storage\\SGTD7I3J\\1502.html}
}

@inproceedings{xuVideoQuestionAnswering2017,
  title = {Video {{Question Answering}} via {{Gradually Refined Attention}} over {{Appearance}} and {{Motion}}},
  booktitle = {Proceedings of the 25th {{ACM}} International Conference on {{Multimedia}}},
  author = {Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  date = {2017-10-23},
  pages = {1645--1653},
  publisher = {{ACM}},
  location = {{Mountain View California USA}},
  doi = {10.1145/3123266.3123427},
  url = {https://dl.acm.org/doi/10.1145/3123266.3123427},
  urldate = {2021-03-11},
  abstract = {Recently image question answering (ImageQA) has gained lots of attention in the research community. However, as its natural extension, video question answering (VideoQA) is less explored. Although both tasks look similar, VideoQA is more challenging mainly because of the complexity and diversity of videos. As such, simply extending the ImageQA methods to videos is insufficient and suboptimal. Particularly, working with the video needs to model its inherent temporal structure and analyze the diverse information it contains. In this paper, we consider exploiting the appearance and motion information resided in the video with a novel attention mechanism. More specifically, we propose an end-to-end model which gradually refines its attention over the appearance and motion features of the video using the question as guidance. The question is processed word by word until the model generates the final optimized attention. The weighted representation of the video, as well as other contextual information, are used to generate the answer. Extensive experiments show the advantages of our model compared to other baseline models. We also demonstrate the effectiveness of our model by analyzing the refined attention weights during the question answering procedure.},
  eventtitle = {{{MM}} '17: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-4906-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\FWCG5A9X\\Xu 等。 - 2017 - Video Question Answering via Gradually Refined Att.pdf}
}

@unpublished{yagciogluRecipeQAChallengeDataset2018,
  title = {{{RecipeQA}}: {{A Challenge Dataset}} for {{Multimodal Comprehension}} of {{Cooking Recipes}}},
  shorttitle = {{{RecipeQA}}},
  author = {Yagcioglu, Semih and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli},
  date = {2018-09-04},
  eprint = {1809.00812},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1809.00812},
  urldate = {2022-03-07},
  abstract = {Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems. The data and leaderboard are available at http://hucvl.github.io/recipeqa.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\ST52L6M8\\Yagcioglu et al_2018_RecipeQA.pdf;D\:\\Zotero\\storage\\CV6V2S8X\\1809.html}
}

@unpublished{yanAchievingHumanParity2021,
  title = {Achieving {{Human Parity}} on {{Visual Question Answering}}},
  author = {Yan, Ming and Xu, Haiyang and Li, Chenliang and Tian, Junfeng and Bi, Bin and Wang, Wei and Chen, Weihua and Xu, Xianzhe and Wang, Fan and Cao, Zheng and Zhang, Zhicheng and Zhang, Qiyu and Zhang, Ji and Huang, Songfang and Huang, Fei and Si, Luo and Jin, Rong},
  date = {2021-11-19},
  eprint = {2111.08896},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.08896},
  urldate = {2021-12-08},
  abstract = {The Visual Question Answering (VQA) task utilizes both visual image and language analysis to answer a textual question with respect to an image. It has been a popular research topic with an increasing number of real-world applications in the last decade. This paper describes our recent research of AliceMind-MMU (ALIbaba's Collection of Encoder-decoders from Machine IntelligeNce lab of Damo academy - MultiMedia Understanding) that obtains similar or even slightly better results than human being does on VQA. This is achieved by systematically improving the VQA pipeline including: (1) pre-training with comprehensive visual and textual feature representation; (2) effective cross-modal interaction with learning to attend; and (3) A novel knowledge mining framework with specialized expert modules for the complex VQA task. Treating different types of visual questions with corresponding expertise needed plays an important role in boosting the performance of our VQA architecture up to the human level. An extensive set of experiments and analysis are conducted to demonstrate the effectiveness of the new research work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\FYIGB8DV\\Yan et al_2021_Achieving Human Parity on Visual Question Answering.pdf;D\:\\Zotero\\storage\\76A2VU94\\2111.html}
}

@unpublished{yanAchievingHumanParity2021a,
  title = {Achieving {{Human Parity}} on {{Visual Question Answering}}},
  author = {Yan, Ming and Xu, Haiyang and Li, Chenliang and Tian, Junfeng and Bi, Bin and Wang, Wei and Chen, Weihua and Xu, Xianzhe and Wang, Fan and Cao, Zheng and Zhang, Zhicheng and Zhang, Qiyu and Zhang, Ji and Huang, Songfang and Huang, Fei and Si, Luo and Jin, Rong},
  date = {2021-11-19},
  eprint = {2111.08896},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.08896},
  urldate = {2021-11-23},
  abstract = {The Visual Question Answering (VQA) task utilizes both visual image and language analysis to answer a textual question with respect to an image. It has been a popular research topic with an increasing number of real-world applications in the last decade. This paper describes our recent research of AliceMind-MMU (ALIbaba's Collection of Encoder-decoders from Machine IntelligeNce lab of Damo academy - MultiMedia Understanding) that obtains similar or even slightly better results than human being does on VQA. This is achieved by systematically improving the VQA pipeline including: (1) pre-training with comprehensive visual and textual feature representation; (2) effective cross-modal interaction with learning to attend; and (3) A novel knowledge mining framework with specialized expert modules for the complex VQA task. Treating different types of visual questions with corresponding expertise needed plays an important role in boosting the performance of our VQA architecture up to the human level. An extensive set of experiments and analysis are conducted to demonstrate the effectiveness of the new research work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\I25T3EPF\\Yan et al_2021_Achieving Human Parity on Visual Question Answering.pdf;D\:\\Zotero\\storage\\NWF75PM5\\2111.html}
}

@inproceedings{yangAutoEncodingSceneGraphs2019,
  title = {Auto-{{Encoding Scene Graphs}} for {{Image Captioning}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Xu and Tang, Kaihua and Zhang, Hanwang and Cai, Jianfei},
  date = {2019-06},
  pages = {10677--10686},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01094},
  url = {https://ieeexplore.ieee.org/document/8953305/},
  urldate = {2021-09-26},
  abstract = {We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language inductive bias into the encoderdecoder image captioning framework for more human-like captions. Intuitively, we humans use the inductive bias to compose collocations and contextual inference in discourse. For example, when we see the relation “person on bike”, it is natural to replace “on” with “ride” and infer “person riding bike on a road” even the “road” is not evident. Therefore, exploiting such bias as a language prior is expected to help the conventional encoder-decoder models less likely overfit to the dataset bias and focus on reasoning. Specifically, we use the scene graph — a directed graph (G) where an object node is connected by adjective nodes and relationship nodes — to represent the complex structural layout of both image (I) and sentence (S). In the textual domain, we use SGAE to learn a dictionary (D) that helps to reconstruct sentences in the S → G → D → S pipeline, where D encodes the desired language prior; in the vision-language domain, we use the shared D to guide the encoder-decoder in the I → G → D → S pipeline. Thanks to the scene graph representation and shared dictionary, the inductive bias is transferred across domains in principle. We validate the effectiveness of SGAE on the challenging MS-COCO image captioning benchmark, e.g., our SGAE-based single-model achieves a new state-of-theart 127.8 CIDEr-D on the Karpathy split, and a competitive 125.5 CIDEr-D (c40) on the official server even compared to other ensemble models. Code has been made available at: https://github.com/yangxuntu/SGAE.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english}
}

@inproceedings{yangBERTRepresentationsVideo2020,
  title = {{{BERT Representations}} for {{Video Question Answering}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Yang, Zekun and Garcia, Noa and Chu, Chenhui and Otani, Mayu and Nakashima, Yuta and Takemura, Haruo},
  date = {2020-03},
  pages = {1545--1554},
  publisher = {{IEEE}},
  location = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093596},
  url = {https://ieeexplore.ieee.org/document/9093596/},
  urldate = {2021-09-10},
  abstract = {Visual question answering (VQA) aims at answering questions about the visual content of an image or a video. Currently, most work on VQA is focused on image-based question answering, and less attention has been paid into answering questions about videos. However, VQA in video presents some unique challenges that are worth studying: it not only requires to model a sequence of visual features over time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a sequential modelling technique based on Transformers, to encode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments, we exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two well-known video VQA datasets: TVQA and Pororo.},
  eventtitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-72816-553-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\HFQZSEIX\\Yang 等。 - 2020 - BERT Representations for Video Question Answering.pdf}
}

@inproceedings{yangDeconfoundedVideoMoment2021,
  title = {Deconfounded {{Video Moment Retrieval}} with {{Causal Intervention}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Yang, Xun and Feng, Fuli and Ji, Wei and Wang, Meng and Chua, Tat-Seng},
  date = {2021-07-11},
  pages = {1--10},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3462823},
  url = {https://dl.acm.org/doi/10.1145/3404835.3462823},
  urldate = {2022-05-14},
  abstract = {We tackle the task of video moment retrieval (VMR), which aims to localize a specific moment in a video according to a textual query. Existing methods primarily model the matching relationship between query and moment by complex cross-modal interactions. Despite their effectiveness, current models mostly exploit dataset biases while ignoring the video content, thus leading to poor generalizability. We argue that the issue is caused by the hidden confounder in VMR, i.e., temporal location of moments, that spuriously correlates the model input and prediction. How to design robust matching models against the temporal location biases is crucial but, as far as we know, has not been studied yet for VMR.},
  eventtitle = {{{SIGIR}} '21: {{The}} 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\QVSMZ69V\\3404835.pdf}
}

@article{yangEmpiricalStudyGPT32022,
  title = {An {{Empirical Study}} of {{GPT-3}} for {{Few-Shot Knowledge-Based VQA}}},
  author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {36},
  number = {3},
  pages = {3081--3089},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i3.20215},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20215},
  urldate = {2022-07-06},
  abstract = {Knowledge-based visual question answering (VQA) involves answering questions that require external knowledge not present in the image. Existing methods first retrieve knowledge from external resources, then reason over the selected knowledge, the input image, and question for answer prediction. However, this two-step approach could lead to mismatches that potentially limit the VQA performance. For example, the retrieved knowledge might be noisy and irrelevant to the question, and the re-embedded knowledge features during reasoning might deviate from their original meanings in the knowledge base (KB). To address this challenge, we propose PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by GPT-3’s power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge. Specifically, we first convert the image into captions (or tags) that GPT-3 can understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just providing a few in-context VQA examples. We further boost performance by carefully investigating: (i) what text formats best describe the image content, and (ii) how in-context examples can be better selected and used. PICa unlocks the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa surpasses the supervised state of the art by an absolute +8.6 points on the OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent few-shot performance.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2YJZ3WFQ\\Yang 等。 - 2022 - An Empirical Study of GPT-3 for Few-Shot Knowledge.pdf}
}

@unpublished{yangGraphRelationTransformer2021,
  title = {Graph {{Relation Transformer}}: {{Incorporating}} Pairwise Object Features into the {{Transformer}} Architecture},
  shorttitle = {Graph {{Relation Transformer}}},
  author = {Yang, Michael and Anantharaman, Aditya and Kitowski, Zachary and Robert, Derik Clive},
  date = {2021-11-11},
  eprint = {2111.06075},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.06075},
  urldate = {2022-03-26},
  abstract = {Previous studies such as VizWiz find that Visual Question Answering (VQA) systems that can read and reason about text in images are useful in application areas such as assisting visually-impaired people. TextVQA is a VQA dataset geared towards this problem, where the questions require answering systems to read and reason about visual objects and text objects in images. One key challenge in TextVQA is the design of a system that effectively reasons not only about visual and text objects individually, but also about the spatial relationships between these objects. This motivates the use of 'edge features', that is, information about the relationship between each pair of objects. Some current TextVQA models address this problem but either only use categories of relations (rather than edge feature vectors) or do not use edge features within the Transformer architectures. In order to overcome these shortcomings, we propose a Graph Relation Transformer (GRT), which uses edge information in addition to node information for graph attention computation in the Transformer. We find that, without using any other optimizations, the proposed GRT method outperforms the accuracy of the M4C baseline model by 0.65\% on the val set and 0.57\% on the test set. Qualitatively, we observe that the GRT has superior spatial reasoning ability to M4C.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\66UPIUFE\\Yang et al_2021_Graph Relation Transformer.pdf;D\:\\Zotero\\storage\\86J8FY9D\\2111.html}
}

@unpublished{yangGraphRelationTransformer2021a,
  title = {Graph {{Relation Transformer}}: {{Incorporating}} Pairwise Object Features into the {{Transformer}} Architecture},
  shorttitle = {Graph {{Relation Transformer}}},
  author = {Yang, Michael and Anantharaman, Aditya and Kitowski, Zachary and Robert, Derik Clive},
  date = {2021-11-11},
  eprint = {2111.06075},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.06075},
  urldate = {2021-12-10},
  abstract = {Previous studies such as VizWiz find that Visual Question Answering (VQA) systems that can read and reason about text in images are useful in application areas such as assisting visually-impaired people. TextVQA is a VQA dataset geared towards this problem, where the questions require answering systems to read and reason about visual objects and text objects in images. One key challenge in TextVQA is the design of a system that effectively reasons not only about visual and text objects individually, but also about the spatial relationships between these objects. This motivates the use of 'edge features', that is, information about the relationship between each pair of objects. Some current TextVQA models address this problem but either only use categories of relations (rather than edge feature vectors) or do not use edge features within the Transformer architectures. In order to overcome these shortcomings, we propose a Graph Relation Transformer (GRT), which uses edge information in addition to node information for graph attention computation in the Transformer. We find that, without using any other optimizations, the proposed GRT method outperforms the accuracy of the M4C baseline model by 0.65\% on the val set and 0.57\% on the test set. Qualitatively, we observe that the GRT has superior spatial reasoning ability to M4C.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\FG6SHU3G\\Yang et al_2021_Graph Relation Transformer.pdf;D\:\\Zotero\\storage\\J8EDC3MJ\\2111.html}
}

@unpublished{yangHierarchicalGraphCapsule2021,
  title = {Hierarchical {{Graph Capsule Network}}},
  author = {Yang, Jinyu and Zhao, Peilin and Rong, Yu and Yan, Chaochao and Li, Chunyuan and Ma, Hehuan and Huang, Junzhou},
  date = {2021-03-27},
  eprint = {2012.08734},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.08734},
  urldate = {2021-10-10},
  abstract = {Graph Neural Networks (GNNs) draw their strength from explicitly modeling the topological information of structured data. However, existing GNNs suffer from limited capability in capturing the hierarchical graph representation which plays an important role in graph classification. In this paper, we innovatively propose hierarchical graph capsule network (HGCN) that can jointly learn node embeddings and extract graph hierarchies. Specifically, disentangled graph capsules are established by identifying heterogeneous factors underlying each node, such that their instantiation parameters represent different properties of the same entity. To learn the hierarchical representation, HGCN characterizes the part-whole relationship between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the structure information among the parts. Experimental studies demonstrate the effectiveness of HGCN and the contribution of each component.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {D\:\\Zotero\\storage\\7GW4HCI4\\Yang et al_2021_Hierarchical Graph Capsule Network.pdf;D\:\\Zotero\\storage\\A7EZ6W9U\\2012.html}
}

@unpublished{yangItTimeAnalog2021,
  title = {It's {{About Time}}: {{Analog Clock Reading}} in the {{Wild}}},
  shorttitle = {It's {{About Time}}},
  author = {Yang, Charig and Xie, Weidi and Zisserman, Andrew},
  date = {2021-12-06},
  eprint = {2111.09162},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.09162},
  urldate = {2022-03-13},
  abstract = {In this paper, we present a framework for reading analog clocks in natural images or videos. Specifically, we make the following contributions: First, we create a scalable pipeline for generating synthetic clocks, significantly reducing the requirements for the labour-intensive annotations; Second, we introduce a clock recognition architecture based on spatial transformer networks (STN), which is trained end-to-end for clock alignment and recognition. We show that the model trained on the proposed synthetic dataset generalises towards real clocks with good accuracy, advocating a Sim2Real training regime; Third, to further reduce the gap between simulation and real data, we leverage the special property of time, i.e. uniformity, to generate reliable pseudo-labels on real unlabelled clock videos, and show that training on these videos offers further improvements while still requiring zero manual annotations. Lastly, we introduce three benchmark datasets based on COCO, Open Images, and The Clock movie, totalling 4,472 images with clocks, with full annotations for time, accurate to the minute.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\9PEY3W9V\\Yang et al_2021_It's About Time.pdf;D\:\\Zotero\\storage\\94Z7JI5H\\2111.html}
}

@unpublished{yangMakingHistoryMatter2019,
  title = {Making {{History Matter}}: {{History-Advantage Sequence Training}} for {{Visual Dialog}}},
  shorttitle = {Making {{History Matter}}},
  author = {Yang, Tianhao and Zha, Zheng-Jun and Zhang, Hanwang},
  date = {2019-04-17},
  eprint = {1902.09326},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.09326},
  urldate = {2021-12-08},
  abstract = {We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q\&A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage-a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9\&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\DXD79VD3\\Yang et al_2019_Making History Matter.pdf;D\:\\Zotero\\storage\\77A78LQM\\1902.html}
}

@unpublished{yangMTAGModalTemporalAttention2021,
  title = {{{MTAG}}: {{Modal-Temporal Attention Graph}} for {{Unaligned Human Multimodal Language Sequences}}},
  shorttitle = {{{MTAG}}},
  author = {Yang, Jianing and Wang, Yongxin and Yi, Ruitao and Zhu, Yuying and Rehman, Azaan and Zadeh, Amir and Poria, Soujanya and Morency, Louis-Philippe},
  date = {2021-04-28},
  eprint = {2010.11985},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.11985},
  urldate = {2021-10-15},
  abstract = {Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-of-the-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\NPXIQH3H\\Yang et al_2021_MTAG.pdf;D\:\\Zotero\\storage\\A377RCP5\\2010.html}
}

@article{yangMultiScaleStructureAwareNetwork2021,
  title = {Multi-{{Scale Structure-Aware Network}} for {{Weakly Supervised Temporal Action Detection}}},
  author = {Yang, Wenfei and Zhang, Tianzhu and Mao, Zhendong and Zhang, Yongdong and Tian, Qi and Wu, Feng},
  date = {2021},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {5848--5861},
  issn = {1941-0042},
  doi = {10.1109/TIP.2021.3089361},
  abstract = {Weakly supervised temporal action detection has better scalability and practicability than fully supervised action detection in reality deployment. However, it is difficult to learn a robust model without temporal action boundary annotations. In this paper, we propose an en-to-end Multi-Scale Structure-Aware Network (MSA-Net) for weakly supervised temporal action detection by exploring both the global structure information of a video and the local structure information of actions. The proposed SA-Net enjoys several merits. First, to localize actions with different durations, each video is encoded into feature representations with different temporal scales. Second, based on the multi-scale feature representation, the proposed model has designed two effective structure modeling mechanisms including global structure modeling and local structure modeling, which can effectively learn discriminative structure aware representations for robust and complete action detection. To the best of our knowledge, this is the first work to fully explore the global and local structure information in a unified deep model for weakly supervised action detection. And extensive experimental results on two benchmark datasets demonstrate that the proposed MSA-Net performs favorably against state-of-the-art methods.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {action detection,Feature extraction,Graph neural networks,GSM,Image segmentation,multi-scale,Noise measurement,Proposals,Scalability,structure-aware,Weakly supervised},
  file = {D\:\\Zotero\\storage\\NEX48ESI\\Yang et al_2021_Multi-Scale Structure-Aware Network for Weakly Supervised Temporal Action.pdf;D\:\\Zotero\\storage\\PGIDHJN4\\9461631.html}
}

@inproceedings{yangQuestionAwareTubeSwitchNetwork2019,
  title = {Question-{{Aware Tube-Switch Network}} for {{Video Question Answering}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Multimedia}}},
  author = {Yang, Tianhao and Zha, Zheng-Jun and Xie, Hongtao and Wang, Meng and Zhang, Hanwang},
  date = {2019-10-15},
  pages = {1184--1192},
  publisher = {{ACM}},
  location = {{Nice France}},
  doi = {10.1145/3343031.3350969},
  url = {https://dl.acm.org/doi/10.1145/3343031.3350969},
  urldate = {2021-03-11},
  abstract = {Video Question \& Answering (VideoQA), a task to answer questions in videos, involves rich spatio-temporal content (e.g., appearance and motion) and requires multi-hop reasoning process. However, existing methods usually deal with appearance and motion separately and fail to synchronize the attentions on appearance and motion features, neglecting two key properties of video QA: (1) appearance and motion features are usually concomitant and complementary to each other at time slice level. Some questions rely on joint representations of both kinds of features at some point in the video; (2) appearance and motion have different importance in multi-step reasoning. In this paper, we propose a novel QuestionAware Tube-Switch Network (TSN) for video question answering which contains (1) a Mix module to synchronously combine the appearance and motion representation at time slice level, achieving fine-grained temporal alignment and correspondence between appearance and motion at every time slice and (2) a Switch module to adaptively choose appearance or motion tube as primary at each reasoning step, guiding the multi-hop reasoning process. To end-to-end train TSN, we utilize the Gumbel-Softmax strategy to account for the discrete tube-switch process. Extensive experimental results on two benchmarks: MSVD-QA and MSRVTT-QA, have demonstrated that the proposed TSN consistently outperforms state-of-the-art on all metrics.},
  eventtitle = {{{MM}} '19: {{The}} 27th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-6889-6},
  langid = {english},
  file = {D\:\\Zotero\\storage\\UAURQWN6\\Yang 等。 - 2019 - Question-Aware Tube-Switch Network for Video Quest.pdf}
}

@article{yangTAPTextAwarePreTraining,
  title = {{{TAP}}: {{Text-Aware Pre-Training}} for {{Text-VQA}} and {{Text-Caption}}},
  author = {Yang, Zhengyuan and Lu, Yijuan and Wang, Jianfeng and Yin, Xi and Florencio, Dinei and Wang, Lijuan and Zhang, Cha and Zhang, Lei and Luo, Jiebo},
  pages = {11},
  langid = {english},
  file = {D\:\\Zotero\\storage\\59XHKU6K\\Yang 等。 - TAP Text-Aware Pre-Training for Text-VQA and Text.pdf}
}

@unpublished{yangTAPTextAwarePretraining2020,
  title = {{{TAP}}: {{Text-Aware Pre-training}} for {{Text-VQA}} and {{Text-Caption}}},
  shorttitle = {{{TAP}}},
  author = {Yang, Zhengyuan and Lu, Yijuan and Wang, Jianfeng and Yin, Xi and Florencio, Dinei and Wang, Lijuan and Zhang, Cha and Zhang, Lei and Luo, Jiebo},
  date = {2020-12-08},
  eprint = {2012.04638},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.04638},
  urldate = {2021-12-08},
  abstract = {In this paper, we propose Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks. These two tasks aim at reading and understanding scene text in images for question answering and image caption generation, respectively. In contrast to the conventional vision-language pre-training that fails to capture scene text and its relationship with the visual and text modalities, TAP explicitly incorporates scene text (generated from OCR engines) in pre-training. With three pre-training tasks, including masked language modeling (MLM), image-text (contrastive) matching (ITM), and relative (spatial) position prediction (RPP), TAP effectively helps the model learn a better aligned representation among the three modalities: text word, visual object, and scene text. Due to this aligned representation learning, even pre-trained on the same downstream task dataset, TAP already boosts the absolute accuracy on the TextVQA dataset by +5.4\%, compared with a non-TAP baseline. To further improve the performance, we build a large-scale dataset based on the Conceptual Caption dataset, named OCR-CC, which contains 1.4 million scene text-related image-text pairs. Pre-trained on this OCR-CC dataset, our approach outperforms the state of the art by large margins on multiple tasks, i.e., +8.3\% accuracy on TextVQA, +8.6\% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\3TBMLKJA\\Yang et al_2020_TAP.pdf;D\:\\Zotero\\storage\\ZFLCCLV6\\2012.html}
}

@incollection{yangTRRNetTieredRelation2020,
  title = {{{TRRNet}}: {{Tiered Relation Reasoning}} for {{Compositional Visual Question Answering}}},
  shorttitle = {{{TRRNet}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Yang, Xiaofeng and Lin, Guosheng and Lv, Fengmao and Liu, Fayao},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12366},
  pages = {414--430},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58589-1_25},
  url = {https://link.springer.com/10.1007/978-3-030-58589-1_25},
  urldate = {2021-09-10},
  abstract = {Compositional visual question answering requires reasoning over both semantic and geometry object relations. We propose a novel tiered reasoning method that dynamically selects object level candidates based on language representations and generates robust pairwise relations within the selected candidate objects. The proposed tiered relation reasoning method can be compatible with the majority of the existing visual reasoning frameworks, leading to significant performance improvement with very little extra computational cost. Moreover, we propose a policy network that decides the appropriate reasoning steps based on question complexity and current reasoning status. In experiments, our model achieves state-of-the-art performance on two VQA datasets.},
  isbn = {978-3-030-58588-4 978-3-030-58589-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\HZBN2IEU\\Yang 等。 - 2020 - TRRNet Tiered Relation Reasoning for Compositiona.pdf}
}

@article{yangVideoMomentRetrieval2022,
  title = {Video {{Moment Retrieval With Cross-Modal Neural Architecture Search}}},
  author = {Yang, Xun and Wang, Shanshan and Dong, Jian and Dong, Jianfeng and Wang, Meng and Chua, Tat-Seng},
  date = {2022},
  journaltitle = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
  volume = {31},
  pages = {13},
  abstract = {The task of video moment retrieval (VMR) is to retrieve the specific video moment from an untrimmed video, according to a textual query. It is a challenging task that requires effective modeling of complex cross-modal matching relationship. Recent efforts primarily model the cross-modal interactions by hand-crafted network architectures. Despite their effectiveness, they rely heavily on expert experience to select architectures and have numerous hyperparameters that need to be carefully tuned, which significantly limit their applications in real-world scenarios. How to design flexible architectures for modeling cross-modal interactions with less manual effort is crucial for the task of VMR but has received limited attention so far. To address this issue, we present a novel VMR approach that automatically searches for an optimal architecture to learn cross-modal matching relationship. Specifically, we develop a cross-modal architecture searching method. It first searches for repeatable cell network architectures based on a directed acyclic graph, which performs operation sampling over a customized task-specific operation set. Then, we adaptively modulate the edge importance in the graph by a query-aware attention network, which performs edge sampling softly in the searched cell. Different from existing neural architecture search methods, our approach can effectively exploit the query information to reach query-conditioned architectures for modeling cross modal matching. Extensive experiments on three benchmark datasets show that our approach can not only significantly outperform the state-of-the-art methods but also run more efficiently and robustly than manually crafted network architectures.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SRMMUUFY\\Yang 等。 - 2022 - Video Moment Retrieval With Cross-Modal Neural Arc.pdf}
}

@article{yangWhenCausalIntervention2019,
  title = {When {{Causal Intervention Meets Adversarial Examples}} and {{Image Masking}} for {{Deep Neural Networks}}},
  author = {Yang, Chao-Han Huck and Liu, Yi-Chieh and Chen, Pin-Yu and Ma, Xiaoli and Tsai, Yi-Chang James},
  date = {2019-09},
  journaltitle = {2019 IEEE International Conference on Image Processing (ICIP)},
  eprint = {1902.03380},
  eprinttype = {arxiv},
  pages = {3811--3815},
  doi = {10.1109/ICIP.2019.8803554},
  url = {http://arxiv.org/abs/1902.03380},
  urldate = {2022-05-09},
  abstract = {Discovering and exploiting the causality in deep neural networks (DNNs) are crucial challenges for understanding and reasoning causal effects (CE) on an explainable visual model. "Intervention" has been widely used for recognizing a causal relation ontologically. In this paper, we propose a causal inference framework for visual reasoning via do-calculus. To study the intervention effects on pixel-level features for causal reasoning, we introduce pixel-wise masking and adversarial perturbation. In our framework, CE is calculated using features in a latent space and perturbed prediction from a DNN-based model. We further provide the first look into the characteristics of discovered CE of adversarially perturbed images generated by gradient-based methods \textbackslash footnote\{\textasciitilde\textasciitilde https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg\}. Experimental results show that CE is a competitive and robust index for understanding DNNs when compared with conventional methods such as class-activation mappings (CAMs) on the Chest X-Ray-14 dataset for human-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds promises for detecting adversarial examples as it possesses distinct characteristics in the presence of adversarial perturbations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  file = {D\:\\Zotero\\storage\\L48SWSSZ\\Yang et al_2019_When Causal Intervention Meets Adversarial Examples and Image Masking for Deep.pdf;D\:\\Zotero\\storage\\9MUC6BSX\\1902.html}
}

@article{yanJieHeZiDiXiangShangZhuYiLiJiZhiHeJiYiWangLuoDeShiJueWenDaMoXing2020,
  title = {结合自底向上注意力机制和记忆网络的视觉问答模型},
  author = {闫, 茹玉 and 刘, 学亮},
  date = {2020},
  journaltitle = {中国图象图形学报},
  volume = {25},
  number = {05},
  pages = {993--1006},
  issn = {1006-8961},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=ZGTB202005013&v=},
  abstract = {目的现有大多数视觉问答模型均采用自上而下的视觉注意力机制,对图像内容无加权统一处理,无法更好地表征图像信息,且因为缺乏长期记忆模块,无法对信息进行长时间记忆存储,在推理答案过程中会造成有效信息丢失,从而预测出错误答案。为此,提出一种结合自底向上注意力机制和记忆网络的视觉问答模型,通过增强对图像内容的表示和记忆,提高视觉问答的准确率。方法预训练一个目标检测模型提取图像中的目标和显著性区域作为图像特征,联合问题表示输入到记忆网络,记忆网络根据问题检索输入图像特征中的有用信息,并结合输入图像信息和问题表示进行多次迭代、更新,以生成最终的信息表示,最后融合记忆网络记忆的最终信息和问题表示,推测出正确答案。结果在公开的大规模数据集VQA (visual question answering) v2. 0上与现有主流算法进行比较实验和消融实验,结果表明,提出的模型在视觉问答任务中的准确率有显著提升,总体准确率为64. 0\%。与MCB(multimodal compact bilinear)算法相比,总体准确率提升了1. 7\%;与性能较好的VQA machine算法相比,总体准确率提升了1\%,其中回答是/否、计数和其他类型问题的准确率分别提升了1. 1\%、3. 4\%和0. 6\%。整体性能优于其他对比算法,验证了提出算法的有效性。结论本文提出的结合自底向上注意力机制和记忆网络的视觉问答模型,更符合人类的视觉注意力机制,并且在推理答案的过程中减少了信息丢失,有效提升了视觉问答的准确率。},
  langid = {chinese},
  keywords = {attention mechanism,bottom-up,memory network,multi-classification,multimodal fusion,多分类 visual question answering(VQA),多模态融合,注意力机制,视觉问答,自底向上,记忆网络},
  annotation = {1 citations(CNKI)[2021-09-06]{$<$}北大核心, CSCD{$>$}}
}

@unpublished{yanPrimitiveRepresentationLearning2021,
  title = {Primitive {{Representation Learning}} for {{Scene Text Recognition}}},
  author = {Yan, Ruijie and Peng, Liangrui and Xiao, Shanyu and Yao, Gang},
  date = {2021-05-10},
  eprint = {2105.04286},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.04286},
  urldate = {2022-04-02},
  abstract = {Scene text recognition is a challenging task due to diverse variations of text instances in natural scene images. Conventional methods based on CNN-RNN-CTC or encoder-decoder with attention mechanism may not fully investigate stable and efficient feature representations for multi-oriented scene texts. In this paper, we propose a primitive representation learning method that aims to exploit intrinsic representations of scene text images. We model elements in feature maps as the nodes of an undirected graph. A pooling aggregator and a weighted aggregator are proposed to learn primitive representations, which are transformed into high-level visual text representations by graph convolutional networks. A Primitive REpresentation learning Network (PREN) is constructed to use the visual text representations for parallel decoding. Furthermore, by integrating visual text representations into an encoder-decoder model with the 2D attention mechanism, we propose a framework called PREN2D to alleviate the misalignment problem in attention-based methods. Experimental results on both English and Chinese scene text recognition tasks demonstrate that PREN keeps a balance between accuracy and efficiency, while PREN2D achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\HAK92SMQ\\Yan et al_2021_Primitive Representation Learning for Scene Text Recognition.pdf;D\:\\Zotero\\storage\\V8HYAZQI\\2105.html}
}

@online{yanruyuJieHeZiDiXiangShangZhuYiLiJiZhiHeJiYiWangLuoDeShiJueWenDaMoXing2020,
  title = {结合自底向上注意力机制和记忆网络的视觉问答模型},
  author = {闫茹玉 and Ruyu, Yan and 刘学亮 and Xueliang, Liu},
  date = {2020-05-16},
  publisher = {{中国图象图形学报}},
  doi = {10.11834/jig.190366},
  url = {http://www.cjig.cn/html/jig/2020/5/20200513.htm},
  urldate = {2021-09-05},
  abstract = {{$<$}sec{$>$}目的现有大多数视觉问答模型均采用自上而下的视觉注意力机制，对图像内容无加权统一处理，无法更好地表征图像信息，且因为缺乏长期记忆模块，无法对信息进行长时间记忆存储，在推理答案过程中会造成有效信息丢失，从而预测出错误答案。为此，提出一种结合自底向上注意力机制和记忆网络的视觉问答模型，通过增强对图像内容的表示和记忆，提高视觉问答的准确率。{$<$}/sec{$>$} {$<$}sec{$>$}方法预训练一个目标检测模型提取图像中的目标和显著性区域作为图像特征，联合问题表示输入到记忆网络，记忆网络根据问题检索输入图像特征中的有用信息，并结合输入图像信息和问题表示进行多次迭代、更新，以生成最终的信息表示，最后融合记忆网络记忆的最终信息和问题表示，推测出正确答案。{$<$}/sec{$>$} {$<$}sec{$>$}结果在公开的大规模数据集VQA（visual question answering）v2.0上与现有主流算法进行比较实验和消融实验，结果表明，提出的模型在视觉问答任务中的准确率有显著提升，总体准确率为64.0\%。与MCB（multimodal compact bilinear）算法相比，总体准确率提升了1.7\%；与性能较好的VQA machine算法相比，总体准确率提升了1\%，其中回答是/否、计数和其他类型问题的准确率分别提升了1.1\%、3.4\%和0.6\%。整体性能优于其他对比算法，验证了提出算法的有效性。{$<$}/sec{$>$} {$<$}sec{$>$}结论本文提出的结合自底向上注意力机制和记忆网络的视觉问答模型，更符合人类的视觉注意力机制，并且在推理答案的过程中减少了信息丢失，有效提升了视觉问答的准确率。{$<$}/sec{$>$}},
  langid = {cn},
  file = {D\:\\Zotero\\storage\\V8KE3LX3\\20200513.html}
}

@unpublished{yaoCPTColorfulPrompt2021,
  title = {{{CPT}}: {{Colorful Prompt Tuning}} for {{Pre-trained Vision-Language Models}}},
  shorttitle = {{{CPT}}},
  author = {Yao, Yuan and Zhang, Ao and Zhang, Zhengyan and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},
  date = {2021-09-24},
  eprint = {2109.11797},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.11797},
  urldate = {2021-09-28},
  abstract = {Pre-Trained Vision-Language Models (VL-PTMs) have shown promising capabilities in grounding natural language in image data, facilitating a broad variety of cross-modal tasks. However, we note that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for quantities of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. To address the challenge, we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap. In this way, our prompt tuning approach enables strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive experimental results show that prompt tuned VL-PTMs outperform their fine-tuned counterparts by a large margin (e.g., 17.3\% absolute accuracy improvement, and 73.8\% relative standard deviation reduction on average with one shot in RefCOCO evaluation). All the data and code will be available to facilitate future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\DWE95CNC\\Yao et al_2021_CPT.pdf;D\:\\Zotero\\storage\\THXXXB2W\\2109.html}
}

@unpublished{yaoExploringVisualRelationship2018,
  title = {Exploring {{Visual Relationship}} for {{Image Captioning}}},
  author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  date = {2018-09-19},
  eprint = {1809.07041},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1809.07041},
  urldate = {2022-03-19},
  abstract = {It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically, we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1\% to 128.7\% on COCO testing set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\SM98SJPS\\Yao et al_2018_Exploring Visual Relationship for Image Captioning.pdf;D\:\\Zotero\\storage\\IMTVA5Y7\\1809.html}
}

@inproceedings{yaoHeterogeneousGraphTransformer2020,
  title = {Heterogeneous {{Graph Transformer}} for {{Graph-to-Sequence Learning}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Yao, Shaowei and Wang, Tianming and Wan, Xiaojun},
  date = {2020},
  pages = {7145--7154},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.640},
  url = {https://aclanthology.org/2020.acl-main.640},
  urldate = {2022-03-26},
  abstract = {The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes. Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation.},
  eventtitle = {{{ACL}} 2020},
  file = {D\:\\Zotero\\storage\\CYI2BA8W\\Yao et al_2020_Heterogeneous Graph Transformer for Graph-to-Sequence Learning.pdf}
}

@unpublished{ye3DQuestionAnswering2021,
  title = {{{3D Question Answering}}},
  author = {Ye, Shuquan and Chen, Dongdong and Han, Songfang and Liao, Jing},
  date = {2021-12-15},
  number = {arXiv:2112.08359},
  eprint = {2112.08359},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.08359},
  urldate = {2022-06-02},
  abstract = {Visual Question Answering (VQA) has witnessed tremendous progress in recent years. However, most efforts only focus on the 2D image question answering tasks. In this paper, we present the first attempt at extending VQA to the 3D domain, which can facilitate artificial intelligence's perception of 3D real-world scenarios. Different from image based VQA, 3D Question Answering (3DQA) takes the color point cloud as input and requires both appearance and 3D geometry comprehension ability to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework \textbackslash textbf\{``3DQA-TR"\}, which consists of two encoders for exploiting the appearance and geometry information, respectively. The multi-modal information of appearance, geometry, and the linguistic question can finally attend to each other via a 3D-Linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset \textbackslash textbf\{``ScanQA"\}, which builds on the ScanNet dataset and contains \$\textbackslash sim\$6K questions, \$\textbackslash sim\$30K answers for \$806\$ scenes. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over existing VQA frameworks, and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate the research in this direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\98NMCMZV\\Ye et al_2021_3D Question Answering.pdf;D\:\\Zotero\\storage\\G98YYXRC\\2112.html}
}

@unpublished{yiNeuralSymbolicVQADisentangling2019,
  title = {Neural-{{Symbolic VQA}}: {{Disentangling Reasoning}} from {{Vision}} and {{Language Understanding}}},
  shorttitle = {Neural-{{Symbolic VQA}}},
  author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Joshua B.},
  date = {2019-01-14},
  eprint = {1810.02338},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.02338},
  urldate = {2021-10-08},
  abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\TXH63S98\\Yi et al_2019_Neural-Symbolic VQA.pdf;D\:\\Zotero\\storage\\A4FBBB7U\\1810.html}
}

@unpublished{yiNeuralSymbolicVQADisentangling2019a,
  title = {Neural-{{Symbolic VQA}}: {{Disentangling Reasoning}} from {{Vision}} and {{Language Understanding}}},
  shorttitle = {Neural-{{Symbolic VQA}}},
  author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Joshua B.},
  date = {2019-01-14},
  eprint = {1810.02338},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.02338},
  urldate = {2021-10-08},
  abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\QNV5UNB8\\Yi et al_2019_Neural-Symbolic VQA.pdf;D\:\\Zotero\\storage\\8CKHL37B\\1810.html}
}

@unpublished{yingTransformersReallyPerform2021,
  title = {Do {{Transformers Really Perform Bad}} for {{Graph Representation}}?},
  author = {Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  date = {2021-11-23},
  eprint = {2106.05234},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.05234},
  urldate = {2022-03-26},
  abstract = {The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\A3V45DIM\\Ying et al_2021_Do Transformers Really Perform Bad for Graph Representation.pdf;D\:\\Zotero\\storage\\7A9W33ER\\2106.html}
}

@unpublished{yuAccurateSceneText2020,
  title = {Towards {{Accurate Scene Text Recognition}} with {{Semantic Reasoning Networks}}},
  author = {Yu, Deli and Li, Xuan and Zhang, Chengquan and Han, Junyu and Liu, Jingtuo and Ding, Errui},
  date = {2020-03-27},
  eprint = {2003.12294},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.12294},
  urldate = {2022-04-02},
  abstract = {Scene text image contains two levels of contents: visual texture and semantic information. Although the previous scene text recognition methods have made great progress over the past few years, the research on mining semantic information to assist text recognition attracts less attention, only RNN-like structures are explored to implicitly model semantic information. However, we observe that RNN based methods have some obvious shortcomings, such as time-dependent decoding manner and one-way serial transmission of semantic context, which greatly limit the help of semantic information and the computation efficiency. To mitigate these limitations, we propose a novel end-to-end trainable framework named semantic reasoning network (SRN) for accurate scene text recognition, where a global semantic reasoning module (GSRM) is introduced to capture global semantic context through multi-way parallel transmission. The state-of-the-art results on 7 public benchmarks, including regular text, irregular text and non-Latin long text, verify the effectiveness and robustness of the proposed method. In addition, the speed of SRN has significant advantages over the RNN based methods, demonstrating its value in practical use.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\KRP2PGPK\\Yu et al_2020_Towards Accurate Scene Text Recognition with Semantic Reasoning Networks.pdf;D\:\\Zotero\\storage\\K4Z4QKED\\2003.html}
}

@unpublished{yuanLanguageBiasVisual2021,
  title = {Language Bias in {{Visual Question Answering}}: {{A Survey}} and {{Taxonomy}}},
  shorttitle = {Language Bias in {{Visual Question Answering}}},
  author = {Yuan, Desen},
  date = {2021-11-16},
  eprint = {2111.08531},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.08531},
  urldate = {2021-12-08},
  abstract = {Visual question answering (VQA) is a challenging task, which has attracted more and more attention in the field of computer vision and natural language processing. However, the current visual question answering has the problem of language bias, which reduces the robustness of the model and has an adverse impact on the practical application of visual question answering. In this paper, we conduct a comprehensive review and analysis of this field for the first time, and classify the existing methods according to three categories, including enhancing visual information, weakening language priors, data enhancement and training strategies. At the same time, the relevant representative methods are introduced, summarized and analyzed in turn. The causes of language bias are revealed and classified. Secondly, this paper introduces the datasets mainly used for testing, and reports the experimental results of various existing methods. Finally, we discuss the possible future research directions in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\4UJK4UK8\\Yuan_2021_Language bias in Visual Question Answering.pdf;D\:\\Zotero\\storage\\UACITV8Z\\2111.html}
}

@unpublished{yuanLanguageBiasVisual2021a,
  title = {Language Bias in {{Visual Question Answering}}: {{A Survey}} and {{Taxonomy}}},
  shorttitle = {Language Bias in {{Visual Question Answering}}},
  author = {Yuan, Desen},
  date = {2021-11-16},
  eprint = {2111.08531},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.08531},
  urldate = {2021-11-23},
  abstract = {Visual question answering (VQA) is a challenging task, which has attracted more and more attention in the field of computer vision and natural language processing. However, the current visual question answering has the problem of language bias, which reduces the robustness of the model and has an adverse impact on the practical application of visual question answering. In this paper, we conduct a comprehensive review and analysis of this field for the first time, and classify the existing methods according to three categories, including enhancing visual information, weakening language priors, data enhancement and training strategies. At the same time, the relevant representative methods are introduced, summarized and analyzed in turn. The causes of language bias are revealed and classified. Secondly, this paper introduces the datasets mainly used for testing, and reports the experimental results of various existing methods. Finally, we discuss the possible future research directions in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\YDIEDB7X\\Yuan_2021_Language bias in Visual Question Answering.pdf;D\:\\Zotero\\storage\\A4C7ZHJQ\\2111.html}
}

@article{yuanPerceptionMattersDetecting,
  title = {Perception {{Matters}}: {{Detecting Perception Failures}} of {{VQA Models Using Metamorphic Testing}}},
  author = {Yuan, Yuanyuan and Wang, Shuai and Jiang, Mingyue and Chen, Tsong Yueh},
  pages = {10},
  abstract = {Visual question answering (VQA) takes an image and a natural-language question as input and returns a naturallanguage answer. To date, VQA models are primarily assessed by their accuracy on high-level reasoning questions. Nevertheless, Given that perception tasks (e.g., recognizing objects) are the building blocks in the compositional process required by high-level reasoning, there is a demanding need to gain insights into how much of a problem lowlevel perception is. Inspired by the principles of software metamorphic testing, we introduce MetaVQA, a modelagnostic framework for benchmarking perception capability of VQA models. Given an image i, MetaVQA is able to synthesize a low-level perception question q. It then jointly transforms (i, q) to one or a set of sub-questions and subimages. MetaVQA checks whether the answer to (i, q) satisfies metamorphic relationships (MRs), denoting perception consistency, with the composed answers of transformed questions and images. Violating MRs denotes a failure of answering perception questions. MetaVQA successfully detects over 4.9 million perception failures made by popular VQA models with metamorphic testing. The state-of-the-art VQA models (e.g., the champion of VQA 2020 Challenge) suffer from perception consistency problems. In contrast, the Oscar VQA models, by using anchor points to align questions and images, show generally better consistency in perception tasks. We hope MetaVQA will revitalize interest in enhancing the low-level perceptual abilities of VQA models, a cornerstone of high-level reasoning.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\6GX2LP6G\\Yuan 等。 - Perception Matters Detecting Perception Failures .pdf}
}

@inproceedings{yuanSpeechLensVisualAnalytics2019,
  title = {{{SpeechLens}}: {{A Visual Analytics Approach}} for {{Exploring Speech Strategies}} with {{Textural}} and {{Acoustic Features}}},
  shorttitle = {{{SpeechLens}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
  author = {Yuan, Linping and Chen, Yuanzhe and Fu, Siwei and Wu, Aoyu and Qu, Huamin},
  date = {2019-02},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Kyoto, Japan}},
  doi = {10.1109/BIGCOMP.2019.8679261},
  url = {https://ieeexplore.ieee.org/document/8679261/},
  urldate = {2022-05-24},
  abstract = {Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple levelof-details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
  isbn = {978-1-5386-7789-6},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SKIKZASL\\Yuan 等。 - 2019 - SpeechLens A Visual Analytics Approach for Explor.pdf}
}

@inproceedings{yuDeepModularCoAttention2019,
  title = {Deep {{Modular Co-Attention Networks}} for {{Visual Question Answering}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  date = {2019-06},
  pages = {6274--6283},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00644},
  url = {https://ieeexplore.ieee.org/document/8953581/},
  urldate = {2021-09-09},
  abstract = {Visual Question Answering (VQA) requires a finegrained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effective ‘co-attention’ model to associate key words in questions with key objects in images is central to VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, and deep co-attention models show little improvement over their shallow counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and images, as well as the question-guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN’s effectiveness. Experimental results demonstrate that MCAN significantly outperforms the previous state-of-the-art. Our best single model delivers 70.63\% overall accuracy on the test-dev set.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {78 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\CPVMLDC6\\Yu 等。 - 2019 - Deep Modular Co-Attention Networks for Visual Ques.pdf}
}

@unpublished{yuDeepModularCoAttention2019a,
  title = {Deep {{Modular Co-Attention Networks}} for {{Visual Question Answering}}},
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  date = {2019-06-25},
  eprint = {1906.10770},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1906.10770},
  urldate = {2021-08-04},
  abstract = {Visual Question Answering (VQA) requires a finegrained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effective ‘co-attention’ model to associate key words in questions with key objects in images is central to VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, and deep co-attention models show little improvement over their shallow counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and images, as well as the guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN’s effectiveness. Experimental results demonstrate that MCAN significantly outperforms the previous state-ofthe-art. Our best single model delivers 70.63\% overall accuracy on the test-dev set. Code is available at https://github.com/MILVLG/mcan-vqa.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\VHDBECFV\\Yu 等。 - 2019 - Deep Modular Co-Attention Networks for Visual Ques.pdf}
}

@inproceedings{yuDeepModularCoAttention2019b,
  title = {Deep {{Modular Co-Attention Networks}} for {{Visual Question Answering}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  date = {2019-06},
  pages = {6274--6283},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00644},
  url = {https://ieeexplore.ieee.org/document/8953581/},
  urldate = {2021-03-11},
  abstract = {Visual Question Answering (VQA) requires a finegrained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effective ‘co-attention’ model to associate key words in questions with key objects in images is central to VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, and deep co-attention models show little improvement over their shallow counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and images, as well as the question-guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN’s effectiveness. Experimental results demonstrate that MCAN significantly outperforms the previous state-of-the-art. Our best single model delivers 70.63\% overall accuracy on the test-dev set.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\NM5QPRHP\\Yu 等。 - 2019 - Deep Modular Co-Attention Networks for Visual Ques.pdf}
}

@inproceedings{yuDeepMultimodalNeural2020,
  title = {Deep {{Multimodal Neural Architecture Search}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Yu, Zhou and Cui, Yuhao and Yu, Jun and Wang, Meng and Tao, Dacheng and Tian, Qi},
  date = {2020-10-12},
  pages = {3743--3752},
  publisher = {{ACM}},
  location = {{Seattle WA USA}},
  doi = {10.1145/3394171.3413977},
  url = {https://dl.acm.org/doi/10.1145/3394171.3413977},
  urldate = {2021-03-11},
  abstract = {Designing effective neural networks is fundamentally important in deep multimodal learning. Most existing works focus on a single task and design neural architectures manually, which are highly task-specific and hard to generalize to different tasks. In this paper, we devise a generalized deep multimodal neural architecture search (MMnas) framework for various multimodal learning tasks. Given multimodal input, we first define a set of primitive operations, and then construct a deep encoder-decoder based unified backbone, where each encoder or decoder block corresponds to an operation searched from a predefined operation pool. On top of the unified backbone, we attach task-specific heads to tackle different multimodal learning tasks. By using a gradientbased NAS algorithm, the optimal architectures for different tasks are learned efficiently. Extensive ablation studies, comprehensive analysis, and comparative experimental results show that the obtained MMnasNet significantly outperforms existing state-ofthe-art approaches across three multimodal learning tasks (over five datasets), including visual question answering, image-text matching, and visual grounding.},
  eventtitle = {{{MM}} '20: {{The}} 28th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-7988-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\DIJNHBJW\\Yu 等。 - 2020 - Deep Multimodal Neural Architecture Search.pdf}
}

@unpublished{yuICEGANIdentityawareCapsuleEnhanced2021,
  title = {{{ICE-GAN}}: {{Identity-aware}} and {{Capsule-Enhanced GAN}} with {{Graph-based Reasoning}} for {{Micro-Expression Recognition}} and {{Synthesis}}},
  shorttitle = {{{ICE-GAN}}},
  author = {Yu, Jianhui and Zhang, Chaoyi and Song, Yang and Cai, Weidong},
  date = {2021-04-26},
  eprint = {2005.04370},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.04370},
  urldate = {2021-10-08},
  abstract = {Micro-expressions are reflections of people's true feelings and motives, which attract an increasing number of researchers into the study of automatic facial micro-expression recognition. The short detection window, the subtle facial muscle movements, and the limited training samples make micro-expression recognition challenging. To this end, we propose a novel Identity-aware and Capsule-Enhanced Generative Adversarial Network with graph-based reasoning (ICE-GAN), introducing micro-expression synthesis as an auxiliary task to assist recognition. The generator produces synthetic faces with controllable micro-expressions and identity-aware features, whose long-ranged dependencies are captured through the graph reasoning module (GRM), and the discriminator detects the image authenticity and expression classes. Our ICE-GAN was evaluated on Micro-Expression Grand Challenge 2019 (MEGC2019) with a significant improvement (12.9\%) over the winner and surpassed other state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\B42PQZWY\\Yu et al_2021_ICE-GAN.pdf;D\:\\Zotero\\storage\\JUVBNZMP\\2005.html}
}

@unpublished{yuICEGANIdentityawareCapsuleEnhanced2021a,
  title = {{{ICE-GAN}}: {{Identity-aware}} and {{Capsule-Enhanced GAN}} with {{Graph-based Reasoning}} for {{Micro-Expression Recognition}} and {{Synthesis}}},
  shorttitle = {{{ICE-GAN}}},
  author = {Yu, Jianhui and Zhang, Chaoyi and Song, Yang and Cai, Weidong},
  date = {2021-04-26},
  eprint = {2005.04370},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.04370},
  urldate = {2021-10-04},
  abstract = {Micro-expressions are reflections of people's true feelings and motives, which attract an increasing number of researchers into the study of automatic facial micro-expression recognition. The short detection window, the subtle facial muscle movements, and the limited training samples make micro-expression recognition challenging. To this end, we propose a novel Identity-aware and Capsule-Enhanced Generative Adversarial Network with graph-based reasoning (ICE-GAN), introducing micro-expression synthesis as an auxiliary task to assist recognition. The generator produces synthetic faces with controllable micro-expressions and identity-aware features, whose long-ranged dependencies are captured through the graph reasoning module (GRM), and the discriminator detects the image authenticity and expression classes. Our ICE-GAN was evaluated on Micro-Expression Grand Challenge 2019 (MEGC2019) with a significant improvement (12.9\%) over the winner and surpassed other state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\UPIEX4U2\\Yu et al_2021_ICE-GAN.pdf;D\:\\Zotero\\storage\\L45IRV5G\\2005.html}
}

@incollection{yuJointSequenceFusion2018,
  title = {A {{Joint Sequence Fusion Model}} for {{Video Question Answering}} and {{Retrieval}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Yu, Youngjae and Kim, Jongseok and Kim, Gunhee},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11211},
  pages = {487--503},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01234-2_29},
  url = {http://link.springer.com/10.1007/978-3-030-01234-2_29},
  urldate = {2021-09-10},
  abstract = {We present an approach named JSFusion (Joint Sequence Fusion) that can measure semantic similarity between any pairs of multimodal sequence data (e.g. a video clip and a language sentence). Our multimodal matching network consists of two key components. First, the Joint Semantic Tensor composes a dense pairwise representation of two sequence data into a 3D tensor. Then, the Convolutional Hierarchical Decoder computes their similarity score by discovering hidden hierarchical matches between the two sequence modalities. Both modules leverage hierarchical attention mechanisms that learn to promote well-matched representation patterns while prune out misaligned ones in a bottom-up manner. Although the JSFusion is a universal model to be applicable to any multimodal sequence data, this work focuses on video-language tasks including multimodal retrieval and video QA. We evaluate the JSFusion model in three retrieval and VQA tasks in LSMDC, for which our model achieves the best performance reported so far. We also perform multiple-choice and movie retrieval tasks for the MSR-VTT dataset, on which our approach outperforms many state-of-the-art methods.},
  isbn = {978-3-030-01233-5 978-3-030-01234-2},
  langid = {english},
  keywords = {Retrieval,VideoQA},
  file = {D\:\\Zotero\\storage\\YFYTVCEB\\Yu 等。 - 2018 - A Joint Sequence Fusion Model for Video Question A.pdf}
}

@inproceedings{yuLearningSelfdrivenSiamese2021,
  title = {Learning from {{Inside}}: {{Self-driven Siamese Sampling}} and {{Reasoning}} for {{Video Question Answering}}},
  shorttitle = {Learning from {{Inside}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yu, Weijiang and Zheng, Haoteng and Li, Mengfei and Ji, Lei and Wu, Lijun and Xiao, Nong and Duan, Nan},
  date = {2021},
  volume = {34},
  pages = {26462--26474},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2021/hash/dea184826614d3f4c608731389ed0c74-Abstract.html},
  urldate = {2022-04-25},
  abstract = {Recent advances in the video question answering (i.e., VideoQA) task have achieved strong success by following the paradigm of fine-tuning each clip-text pair independently on the pretrained transformer-based model via supervised learning. Intuitively, multiple samples (i.e., clips) should be interdependent to capture similar visual and key semantic information in the same video. To consider the interdependent knowledge between contextual clips into the network inference, we propose a Siamese Sampling and Reasoning (SiaSamRea) approach, which consists of a siamese sampling mechanism to generate sparse and similar clips (i.e., siamese clips) from the same video, and a novel reasoning strategy for integrating the interdependent knowledge between contextual clips into the network. The reasoning strategy contains two modules: (1) siamese knowledge generation to learn the inter-relationship among clips; (2) siamese knowledge reasoning to produce the refined soft label by propagating the weights of inter-relationship to the predicted candidates of all clips. Finally, our SiaSamRea can endow the current multimodal reasoning paradigm with the ability of learning from inside via the guidance of soft labels. Extensive experiments demonstrate our SiaSamRea achieves state-of-the-art performance on five VideoQA benchmarks, e.g., a significant +2.1\% gain on MSRVTT-QA, +2.9\% on MSVD-QA, +1.0\% on ActivityNet-QA, +1.8\% on How2QA and +4.3\% (action) on TGIF-QA.},
  file = {D\:\\Zotero\\storage\\TV4HAQDK\\Yu et al_2021_Learning from Inside.pdf}
}

@inproceedings{yuMultimodalFactorizedBilinear2017,
  title = {Multi-Modal {{Factorized Bilinear Pooling}} with {{Co-attention Learning}} for {{Visual Question Answering}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Yu, Zhou and Yu, Jun and Fan, Jianping and Tao, Dacheng},
  date = {2017-10},
  pages = {1839--1848},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.202},
  url = {http://ieeexplore.ieee.org/document/8237464/},
  urldate = {2021-03-11},
  abstract = {Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both the visual content of images and the textual content of questions. The approaches used to represent the images and questions in a fine-grained manner and questions and to fuse these multimodal features play key roles in performance. Bilinear pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional representations and high computational complexity may seriously limit their applicability in practice. For multimodal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efficiently and effectively combine multi-modal features, which results in superior performance for VQA compared with other bilinear pooling approaches. For fine-grained image and question representation, we develop a ‘co-attention’ mechanism using an end-to-end deep network architecture to jointly learn both the image and question attentions. Combining the proposed MFB approach with co-attention learning in a new network architecture provides a unified model for VQA. Our experimental results demonstrate that the single MFB with co-attention model achieves new state-of-theart performance on the real-world VQA dataset. Code available at https://github.com/yuzcccc/mfb.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\3D5XZDZQ\\Yu 等。 - 2017 - Multi-modal Factorized Bilinear Pooling with Co-at.pdf}
}

@unpublished{yunDoesVisionandLanguagePretraining2021,
  title = {Does {{Vision-and-Language Pretraining Improve Lexical Grounding}}?},
  author = {Yun, Tian and Sun, Chen and Pavlick, Ellie},
  date = {2021-09-21},
  eprint = {2109.10246},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.10246},
  urldate = {2021-09-24},
  abstract = {Linguistic representations derived from text alone have been criticized for their lack of grounding, i.e., connecting words to their meanings in the physical world. Vision-and-Language (VL) models, trained jointly on text and image or video data, have been offered as a response to such criticisms. However, while VL pretraining has shown success on multimodal tasks such as visual question answering, it is not yet known how the internal linguistic representations themselves compare to their text-only counterparts. This paper compares the semantic representations learned via VL vs. text-only pretraining for two recent VL models using a suite of analyses (clustering, probing, and performance on a commonsense question answering task) in a language-only setting. We find that the multimodal models fail to significantly outperform the text-only variants, suggesting that future work is required if multimodal pretraining is to be pursued as a means of improving NLP in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\Q87VGFGT\\Yun et al_2021_Does Vision-and-Language Pretraining Improve Lexical Grounding.pdf;D\:\\Zotero\\storage\\AMJWKA3U\\2109.html}
}

@misc{yuRFGANRFBasedHuman2021,
  title = {{{RFGAN}}: {{RF-Based Human Synthesis}}},
  shorttitle = {{{RFGAN}}},
  author = {Yu, Cong and Wu, Zhi and Zhang, Dongheng and Lu, Zhi and Hu, Yang and Chen, Yan},
  date = {2021-12-07},
  number = {arXiv:2112.03727},
  eprint = {2112.03727},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.03727},
  urldate = {2022-07-26},
  abstract = {This paper demonstrates human synthesis based on the Radio Frequency (RF) signals, which leverages the fact that RF signals can record human movements with the signal reflections off the human body. Different from existing RF sensing works that can only perceive humans roughly, this paper aims to generate fine-grained optical human images by introducing a novel cross-modal RFGAN model. Specifically, we first build a radio system equipped with horizontal and vertical antenna arrays to transceive RF signals. Since the reflected RF signals are processed as obscure signal projection heatmaps on the horizontal and vertical planes, we design a RF-Extractor with RNN in RFGAN for RF heatmap encoding and combining to obtain the human activity information. Then we inject the information extracted by the RF-Extractor and RNN as the condition into GAN using the proposed RF-based adaptive normalizations. Finally, we train the whole model in an end-to-end manner. To evaluate our proposed model, we create two cross-modal datasets (RF-Walk \& RF-Activity) that contain thousands of optical human activity frames and corresponding RF signals. Experimental results show that the RFGAN can generate target human activity frames using RF signals. To the best of our knowledge, this is the first work to generate optical images based on RF signals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\AL5VS9R5\\Yu et al_2021_RFGAN.pdf;D\:\\Zotero\\storage\\3JDVZMJR\\2112.html}
}

@inproceedings{zadehSocialIQQuestionAnswering2019,
  title = {Social-{{IQ}}: {{A Question Answering Benchmark}} for {{Artificial Social Intelligence}}},
  shorttitle = {Social-{{IQ}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zadeh, Amir and Chan, Michael and Liang, Paul Pu and Tong, Edmund and Morency, Louis-Philippe},
  date = {2019-06},
  pages = {8799--8809},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00901},
  url = {https://ieeexplore.ieee.org/document/8953344/},
  urldate = {2021-10-15},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english}
}

@inproceedings{zadehSocialIQQuestionAnswering2019a,
  title = {Social-{{IQ}}: {{A Question Answering Benchmark}} for {{Artificial Social Intelligence}}},
  shorttitle = {Social-{{IQ}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zadeh, Amir and Chan, Michael and Liang, Paul Pu and Tong, Edmund and Morency, Louis-Philippe},
  date = {2019-06},
  pages = {8799--8809},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00901},
  url = {https://ieeexplore.ieee.org/document/8953344/},
  urldate = {2021-09-09},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {6 citations (Crossref) [2021-09-10] 00000},
  file = {D\:\\Zotero\\storage\\5A3T95NA\\Zadeh 等。 - 2019 - Social-IQ A Question Answering Benchmark for Arti.pdf}
}

@unpublished{zaoukNeuralbasedModelingPerformance2021,
  title = {Neural-Based {{Modeling}} for {{Performance Tuning}} of {{Spark Data Analytics}}},
  author = {Zaouk, Khaled and Song, Fei and Lyu, Chenghao and Diao, Yanlei},
  date = {2021-01-20},
  eprint = {2101.08167},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.08167},
  urldate = {2021-12-25},
  abstract = {Cloud data analytics has become an integral part of enterprise business operations for data-driven insight discovery. Performance modeling of cloud data analytics is crucial for performance tuning and other critical operations in the cloud. Traditional modeling techniques fail to adapt to the high degree of diversity in workloads and system behaviors in this domain. In this paper, we bring recent Deep Learning techniques to bear on the process of automated performance modeling of cloud data analytics, with a focus on Spark data analytics as representative workloads. At the core of our work is the notion of learning workload embeddings (with a set of desired properties) to represent fundamental computational characteristics of different jobs, which enable performance prediction when used together with job configurations that control resource allocation and other system knobs. Our work provides an in-depth study of different modeling choices that suit our requirements. Results of extensive experiments reveal the strengths and limitations of different modeling methods, as well as superior performance of our best performing method over a state-of-the-art modeling tool for cloud analytics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\IW5D63LT\\Zaouk et al_2021_Neural-based Modeling for Performance Tuning of Spark Data Analytics.pdf;D\:\\Zotero\\storage\\H7PNRAE6\\2101.html}
}

@article{zengEmoCoVisualAnalysis2019,
  title = {{{EmoCo}}: {{Visual Analysis}} of {{Emotion Coherence}} in {{Presentation Videos}}},
  shorttitle = {{{EmoCo}}},
  author = {Zeng, Haipeng and Wang, Xingbo and Wu, Aoyu and Wang, Yong and Li, Quan and Endert, Alex and Qu, Huamin},
  date = {2019},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  eprint = {1907.12918},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1--1},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2019.2934656},
  url = {http://arxiv.org/abs/1907.12918},
  urldate = {2022-05-23},
  abstract = {Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\4DALSUSX\\Zeng et al_2019_EmoCo.pdf;D\:\\Zotero\\storage\\FC99NJNA\\1907.html}
}

@article{zengGestureLensVisualAnalysis2022,
  title = {{{GestureLens}}: {{Visual Analysis}} of {{Gestures}} in {{Presentation Videos}}},
  shorttitle = {{{GestureLens}}},
  author = {Zeng, Haipeng and Wang, Xingbo and Wang, Yong and Wu, Aoyu and Pong, Ting Chuen and Qu, Huamin},
  date = {2022},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  eprint = {2204.08894},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1--1},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2022.3169175},
  url = {http://arxiv.org/abs/2204.08894},
  urldate = {2022-05-23},
  abstract = {Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. Manually checking and exploring gesture usage in the presentation videos is often tedious and time-consuming. There lacks an efficient method to help users conduct gesture exploration, which is challenging due to the intrinsically temporal evolution of gestures and their complex correlation to speech content. In this paper, we propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Specifically, the exploration view enables users to obtain a quick overview of the spatial and temporal distributions of gestures. The dynamic hand movements are firstly aggregated through a heatmap in the gesture space for uncovering spatial patterns, and then decomposed into two mutually perpendicular timelines for revealing temporal patterns. The relation view allows users to explicitly explore the correlation between speech content and gestures by enabling linked analysis and intuitive glyph designs. The video view and dynamic view show the context and overall dynamic movement of the selected gestures, respectively. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multimedia,H.5.1,H.5.2,J.4,K.3.0},
  file = {D\:\\Zotero\\storage\\AV4MQMGB\\Zeng et al_2022_GestureLens.pdf;D\:\\Zotero\\storage\\XZMJ5F67\\2204.html}
}

@article{zengLeveragingVideoDescriptions,
  title = {Leveraging {{Video Descriptions}} to {{Learn Video Question Answering}}},
  author = {Zeng, Kuo-Hao and Chen, Tseng-Hung and Chuang, Ching-Yao and Liao, Yuan-Hong and Niebles, Juan Carlos and Sun, Min},
  pages = {7},
  abstract = {We propose a scalable approach to learn video-based question answering (QA): to answer a free-form natural language question about the contents of a video. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended from MN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), and SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\XXE9VNPX\\Zeng 等。 - Leveraging Video Descriptions to Learn Video Quest.pdf}
}

@inproceedings{zengOCRVQAInvolving2021,
  title = {Beyond {{OCR}} + {{VQA}}: {{Involving OCR}} into the {{Flow}} for {{Robust}} and {{Accurate TextVQA}}},
  shorttitle = {Beyond {{OCR}} + {{VQA}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Multimedia}}},
  author = {Zeng, Gangyan and Zhang, Yuan and Zhou, Yu and Yang, Xiaomeng},
  date = {2021-10-17},
  pages = {376--385},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  doi = {10.1145/3474085.3475606},
  url = {https://dl.acm.org/doi/10.1145/3474085.3475606},
  urldate = {2022-05-16},
  abstract = {Text-based visual question answering (TextVQA) requires analyzing both the visual contents and texts in an image to answer a question, which is more practical than general visual question answering (VQA). Existing efforts tend to regard optical character recognition (OCR) as a pre-processing and then combine it with a VQA framework. It makes the performance of multimodal reasoning and question answering highly depend on the accuracy of OCR. In this work, we address this issue with two perspectives. First, we take advantages of multimodal cues to complete the semantic information of texts. A visually enhanced text embedding is proposed to enable understanding of texts without accurately recognizing them. Second, we further leverage rich contextual information to modify the answer texts even if the OCR module does not correctly recognize them. In addition, the visual objects are endued with semantic representations to enable objects in the same semantic space as OCR tokens. Equipped with these techniques, the cumulative error propagation caused by poor OCR performance is effectively suppressed. Extensive experiments on TextVQA and ST-VQA datasets demonstrate that our approach achieves the state-of-the-art performance in terms of accuracy and robustness.},
  eventtitle = {{{MM}} '21: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-8651-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\GFG8QRQC\\Zeng 等。 - 2021 - Beyond OCR + VQA Involving OCR into the Flow for .pdf}
}

@thesis{zengVisualAnalysisHuman2020,
  type = {phdthesis},
  title = {Visual Analysis of Human Behaviors in Classroom and Public Speech Videos},
  author = {Zeng, Haipeng},
  date = {2020},
  pages = {991012880063403412},
  institution = {{The Hong Kong University of Science and Technology}},
  location = {{Clear Water Bay, Kowloon, Hong Kong}},
  doi = {10.14711/thesis-991012880063403412},
  url = {http://lbezone.ust.hk/bib/991012880063403412},
  urldate = {2022-05-23},
  langid = {english},
  file = {D\:\\Zotero\\storage\\3GMB92VW\\Zeng - 2020 - Visual analysis of human behaviors in classroom an.pdf}
}

@unpublished{zhangAttentionBasedCapsuleNetworks2018,
  title = {Attention-{{Based Capsule Networks}} with {{Dynamic Routing}} for {{Relation Extraction}}},
  author = {Zhang, Ningyu and Deng, Shumin and Sun, Zhanlin and Chen, Xi and Zhang, Wei and Chen, Huajun},
  date = {2018-12-29},
  eprint = {1812.11321},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1812.11321},
  urldate = {2021-10-09},
  abstract = {A capsule is a group of neurons, whose activity vector represents the instantiation parameters of a specific type of entity. In this paper, we explore the capsule networks used for relation extraction in a multi-instance multi-label learning framework and propose a novel neural approach based on capsule networks with attention mechanisms. We evaluate our method with different benchmarks, and it is demonstrated that our method improves the precision of the predicted relations. Particularly, we show that capsule networks improve multiple entity pairs relation extraction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\storage\\KP3DPNEW\\Zhang et al_2018_Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction.pdf;D\:\\Zotero\\storage\\JRFK4XG5\\1812.html}
}

@article{zhangContextEnrichedNeuralNetwork,
  title = {A {{Context-Enriched Neural Network Method}} for {{Recognizing Lexical Entailment}}},
  author = {Zhang, Kun and Chen, Enhong and Liu, Qi and Liu, Chuanren and Lv, Guangyi},
  pages = {7},
  abstract = {Recognizing lexical entailment (RLE) always plays an important role in inference of natural language, i.e., identifying whether one word entails another, for example, fox entails animal. In the literature, automatically recognizing lexical entailment for word pairs deeply relies on words’ contextual representations. However, as a “prototype” vector, a single representation cannot reveal multifaceted aspects of the words due to their homonymy and polysemy. In this paper, we propose a supervised Context-Enriched Neural Network (CENN) method for recognizing lexical entailment. To be specific, we first utilize multiple embedding vectors from different contexts to represent the input word pairs. Then, through different combination methods and attention mechanism, we integrate different embedding vectors and optimize their weights to predict whether there are entailment relations in word pairs. Moreover, our proposed framework is flexible and open to handle different word contexts and entailment perspectives in the text corpus. Extensive experiments on five datasets show that our approach significantly improves the performance of automatic RLE in comparison with several state-of-the-art methods.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\IC7A48LQ\\Zhang 等。 - A Context-Enriched Neural Network Method for Recog.pdf}
}

@article{zhangDomainRobustVQADiverse,
  title = {Domain-{{Robust VQA With Diverse Datasets}} and {{Methods}} but {{No Target Labels}}},
  author = {Zhang, Mingda and Maidment, Tristan and Diab, Ahmad and Kovashka, Adriana and Hwa, Rebecca},
  pages = {11},
  abstract = {The observation that computer vision methods overfit to dataset specifics has inspired diverse attempts to make object recognition models robust to domain shifts. However, similar work on domain-robust visual question answering methods is very limited. Domain adaptation for VQA differs from adaptation for object recognition due to additional complexity: VQA models handle multimodal inputs, methods contain multiple steps with diverse modules resulting in complex optimization, and answer spaces in different datasets are vastly different. To tackle these challenges, we first quantify domain shifts between popular VQA datasets, in both visual and textual space. To disentangle shifts between datasets arising from different modalities, we also construct synthetic shifts in the image and question domains separately. Second, we test the robustness of different families of VQA methods (classic two-stream, transformer, and neuro-symbolic methods) to these shifts. Third, we test the applicability of existing domain adaptation methods and devise a new one to bridge VQA domain gaps, adjusted to specific VQA models. To emulate the setting of real-world generalization, we focus on unsupervised domain adaptation and the open-ended classification task formulation.},
  langid = {english},
  keywords = {Domain,Robust},
  file = {D\:\\Zotero\\storage\\3W7TEU2I\\Zhang 等。 - Domain-Robust VQA With Diverse Datasets and Method.pdf}
}

@article{zhangDRrNetDynamicReRead2019,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SM8UFFM2\\4734-Article Text-7773-2-10-20190729.pdf}
}

@article{zhangDRrNetDynamicReRead2019a,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english}
}

@article{zhangDRrNetDynamicReRead2019b,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english}
}

@article{zhangDRrNetDynamicReRead2019c,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english}
}

@article{zhangDRrNetDynamicReRead2019d,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english}
}

@article{zhangDRrNetDynamicReRead2019e,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english}
}

@article{zhangDRrNetDynamicReRead2019f,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english}
}

@article{zhangDRrNetDynamicReRead2019g,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english}
}

@article{zhangDRrNetDynamicReRead2019h,
  title = {{{DRr-Net}}: {{Dynamic Re-Read Network}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{DRr-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wang, Linyuan and Wu, Le and Chen, Enhong and Wu, Fangzhao and Xie, Xing},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7442--7449},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017442},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4734},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks such as Natural Language Inference (NLI) and Paraphrase Identification (PI). Among all matching methods, attention mechanism plays an important role in capturing the semantic relations and properly aligning the elements of two sentences. Previous methods utilized attention mechanism to select important parts of sentences at one time. However, the important parts of the sentence during semantic matching are dynamically changing with the degree of sentence understanding. Selecting the important parts at one time may be insufficient for semantic understanding. To this end, we propose a Dynamic Re-read Network (DRr-Net) approach for sentence semantic matching, which is able to pay close attention to a small region of sentences at each step and re-read the important words for better sentence semantic understanding. To be specific, we first employ Attention Stack-GRU (ASG) unit to model the original sentence repeatedly and preserve all the information from bottom-most word embedding input to up-most recurrent output. Second, we utilize Dynamic Re-read (DRr) unit to pay close attention to one important word at one time with the consideration of learned information and re-read the important words for better sentence semantic understanding. Extensive experiments on three sentence matching benchmark datasets demonstrate that DRr-Net has the ability to model sentence semantic more precisely and significantly improve the performance of sentence semantic matching. In addition, it is very interesting that some of finding in our experiments are consistent with the findings of psychological research.},
  langid = {english}
}

@unpublished{zhangEmpiricalStudyLeveraging2019,
  title = {An {{Empirical Study}} on {{Leveraging Scene Graphs}} for {{Visual Question Answering}}},
  author = {Zhang, Cheng and Chao, Wei-Lun and Xuan, Dong},
  date = {2019-07-28},
  eprint = {1907.12133},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.12133},
  urldate = {2021-10-09},
  abstract = {Visual question answering (Visual QA) has attracted significant attention these years. While a variety of algorithms have been proposed, most of them are built upon different combinations of image and language features as well as multi-modal attention and fusion. In this paper, we investigate an alternative approach inspired by conventional QA systems that operate on knowledge graphs. Specifically, we investigate the use of scene graphs derived from images for Visual QA: an image is abstractly represented by a graph with nodes corresponding to object entities and edges to object relationships. We adapt the recently proposed graph network (GN) to encode the scene graph and perform structured reasoning according to the input question. Our empirical studies demonstrate that scene graphs can already capture essential information of images and graph networks have the potential to outperform state-of-the-art Visual QA algorithms but with a much cleaner architecture. By analyzing the features generated by GNs we can further interpret the reasoning process, suggesting a promising direction towards explainable Visual QA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\L9SBQJUS\\Zhang et al_2019_An Empirical Study on Leveraging Scene Graphs for Visual Question Answering.pdf;D\:\\Zotero\\storage\\WJ269VFH\\1907.html}
}

@unpublished{zhangEmpiricalStudyLeveraging2019a,
  title = {An {{Empirical Study}} on {{Leveraging Scene Graphs}} for {{Visual Question Answering}}},
  author = {Zhang, Cheng and Chao, Wei-Lun and Xuan, Dong},
  date = {2019-07-28},
  eprint = {1907.12133},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.12133},
  urldate = {2021-09-26},
  abstract = {Visual question answering (Visual QA) has attracted significant attention these years. While a variety of algorithms have been proposed, most of them are built upon different combinations of image and language features as well as multi-modal attention and fusion. In this paper, we investigate an alternative approach inspired by conventional QA systems that operate on knowledge graphs. Specifically, we investigate the use of scene graphs derived from images for Visual QA: an image is abstractly represented by a graph with nodes corresponding to object entities and edges to object relationships. We adapt the recently proposed graph network (GN) to encode the scene graph and perform structured reasoning according to the input question. Our empirical studies demonstrate that scene graphs can already capture essential information of images and graph networks have the potential to outperform state-of-the-art Visual QA algorithms but with a much cleaner architecture. By analyzing the features generated by GNs we can further interpret the reasoning process, suggesting a promising direction towards explainable Visual QA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\ZRPFZHWL\\Zhang et al_2019_An Empirical Study on Leveraging Scene Graphs for Visual Question Answering.pdf;D\:\\Zotero\\storage\\INUAPH99\\1907.html}
}

@inproceedings{zhangGenerativeVisualDialogue2019,
  title = {Generative {{Visual Dialogue System}} via {{Weighted Likelihood Estimation}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhang, Heming and Ghosh, Shalini and Heck, Larry and Walsh, Stephen and Zhang, Junting and Zhang, Jie and Kuo, C.-C. Jay},
  date = {2019-08},
  pages = {1025--1031},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Macao, China}},
  doi = {10.24963/ijcai.2019/144},
  url = {https://www.ijcai.org/proceedings/2019/144},
  urldate = {2021-03-11},
  abstract = {The key challenge of generative Visual Dialogue (VD) systems is to respond to human queries with informative answers in natural and contiguous conversation flow. Traditional Maximum Likelihood Estimation-based methods only learn from positive responses but ignore the negative responses, and consequently tend to yield safe or generic responses. To address this issue, we propose a novel training scheme in conjunction with weighted likelihood estimation method. Furthermore, an adaptive multi-modal reasoning module is designed, to accommodate various dialogue scenarios automatically and select relevant information accordingly. The experimental results on the VisDial benchmark demonstrate the superiority of our proposed algorithm over other state-of-the-art approaches, with an improvement of 5.81\% on recall@10.},
  eventtitle = {Twenty-{{Eighth International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-19}}\vphantom\{\}},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\53NNY8IT\\Zhang 等。 - 2019 - Generative Visual Dialogue System via Weighted Lik.pdf}
}

@incollection{zhangGoalOrientedVisualQuestion2018,
  title = {Goal-{{Oriented Visual Question Generation}} via {{Intermediate Rewards}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Zhang, Junjie and Wu, Qi and Shen, Chunhua and Zhang, Jian and Lu, Jianfeng and van den Hengel, Anton},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  options = {useprefix=true},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11209},
  pages = {189--204},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01228-1_12},
  url = {http://link.springer.com/10.1007/978-3-030-01228-1_12},
  urldate = {2021-09-10},
  abstract = {Despite significant progress in a variety of vision-and-language problems, developing a method capable of asking intelligent, goal-oriented questions about images is proven to be an inscrutable challenge. Towards this end, we propose a Deep Reinforcement Learning framework based on three new intermediate rewards, namely goal-achieved, progressive and informativeness that encourage the generation of succinct questions, which in turn uncover valuable information towards the overall goal. By directly optimizing for questions that work quickly towards fulfilling the overall goal, we avoid the tendency of existing methods to generate long series of inane queries that add little value. We evaluate our model on the GuessWhat?! dataset and show that the resulting questions can help a standard ‘Guesser’ identify a specific object in an image at a much higher success rate.},
  isbn = {978-3-030-01227-4 978-3-030-01228-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\374SIZXA\\Zhang 等。 - 2018 - Goal-Oriented Visual Question Generation via Inter.pdf}
}

@unpublished{zhangGraphBertOnlyAttention2020,
  title = {Graph-{{Bert}}: {{Only Attention}} Is {{Needed}} for {{Learning Graph Representations}}},
  shorttitle = {Graph-{{Bert}}},
  author = {Zhang, Jiawei and Zhang, Haopeng and Xia, Congying and Sun, Li},
  date = {2020-01-22},
  eprint = {2001.05140},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.05140},
  urldate = {2022-03-26},
  abstract = {The dominant graph neural networks (GNNs) over-rely on the graph links, several serious performance problems with which have been witnessed already, e.g., suspended animation problem and over-smoothing problem. What's more, the inherently inter-connected nature precludes parallelization within the graph, which becomes critical for large-sized graph, as memory constraints limit batching across the nodes. In this paper, we will introduce a new graph neural network, namely GRAPH-BERT (Graph based BERT), solely based on the attention mechanism without any graph convolution or aggregation operators. Instead of feeding GRAPH-BERT with the complete large input graph, we propose to train GRAPH-BERT with sampled linkless subgraphs within their local contexts. GRAPH-BERT can be learned effectively in a standalone mode. Meanwhile, a pre-trained GRAPH-BERT can also be transferred to other application tasks directly or with necessary fine-tuning if any supervised label information or certain application oriented objective is available. We have tested the effectiveness of GRAPH-BERT on several graph benchmark datasets. Based the pre-trained GRAPH-BERT with the node attribute reconstruction and structure recovery tasks, we further fine-tune GRAPH-BERT on node classification and graph clustering tasks specifically. The experimental results have demonstrated that GRAPH-BERT can out-perform the existing GNNs in both the learning effectiveness and efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\8IFPJ46C\\Zhang et al_2020_Graph-Bert.pdf;D\:\\Zotero\\storage\\DXNPSAWE\\2001.html}
}

@inproceedings{zhangHighresolutionSpontaneous3D2013,
  title = {A High-Resolution Spontaneous {{3D}} Dynamic Facial Expression Database},
  booktitle = {2013 10th {{IEEE International Conference}} and {{Workshops}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Zhang, Xing and Yin, Lijun and Cohn, Jeffrey F. and Canavan, Shaun and Reale, Michael and Horowitz, Andy and {Peng Liu}},
  date = {2013-04},
  pages = {1--6},
  publisher = {{IEEE}},
  location = {{Shanghai, China}},
  doi = {10.1109/FG.2013.6553788},
  url = {http://ieeexplore.ieee.org/document/6553788/},
  urldate = {2022-10-04},
  abstract = {Facial expression is central to human experience. Its efficient and valid measurement is a challenge that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Because posed and un-posed (aka “spontaneous”) facial expressions differ along several dimensions including complexity and timing, well-annotated video of un-posed facial behavior is needed. Moreover, because the face is a three-dimensional deformable object, 2D video may be insufficient, and therefore 3D video archives are needed. We present a newly developed 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both personspecific and generic approaches. The work promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action.},
  eventtitle = {2013 10th {{IEEE International Conference}} on {{Automatic Face}} \& {{Gesture Recognition}} ({{FG}} 2013)},
  isbn = {978-1-4673-5546-9 978-1-4673-5545-2 978-1-4673-5544-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\66Q56TZN\\Zhang 等。 - 2013 - A high-resolution spontaneous 3D dynamic facial ex.pdf}
}

@unpublished{zhangLadRaNetLocallyAwareDynamic2021,
  title = {{{LadRa-Net}}: {{Locally-Aware Dynamic Re-read Attention Net}} for {{Sentence Semantic Matching}}},
  shorttitle = {{{LadRa-Net}}},
  author = {Zhang, Kun and Lv, Guangyi and Wu, Le and Chen, Enhong and Liu, Qi and Wang, Meng},
  date = {2021-08-05},
  eprint = {2108.02915},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.02915},
  urldate = {2021-09-12},
  abstract = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks, such as Natural Language Inference (NLI), Paraphrase Identification (PI), and so on. Much recent progress has been made in this area, especially attention-based methods and pre-trained language model based methods. However, most of these methods focus on all the important parts in sentences in a static way and only emphasize how important the words are to the query, inhibiting the ability of attention mechanism. In order to overcome this problem and boost the performance of attention mechanism, we propose a novel dynamic re-read attention, which can pay close attention to one small region of sentences at each step and re-read the important parts for better sentence representations. Based on this attention variation, we develop a novel Dynamic Re-read Network (DRr-Net) for sentence semantic matching. Moreover, selecting one small region in dynamic re-read attention seems insufficient for sentence semantics, and employing pre-trained language models as input encoders will introduce incomplete and fragile representation problems. To this end, we extend DRrNet to Locally-Aware Dynamic Re-read Attention Net (LadRa-Net), in which local structure of sentences is employed to alleviate the shortcoming of Byte-Pair Encoding (BPE) in pre-trained language models and boost the performance of dynamic reread attention. Extensive experiments on two popular sentence semantic matching tasks demonstrate that DRr-Net can significantly improve the performance of sentence semantic matching. Meanwhile, LadRa-Net is able to achieve better performance by considering the local structures of sentences. In addition, it is exceedingly interesting that some discoveries in our experiments are consistent with some findings of psychological research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,zhang},
  file = {D\:\\Zotero\\storage\\YJHH5NJG\\Zhang et al_2021_LadRa-Net.pdf;D\:\\Zotero\\storage\\AMGX43QN\\2108.html}
}

@article{zhangLEARNINGCOUNTOBJECTS2018,
  title = {{{LEARNING TO COUNT OBJECTS IN NATURAL IMAGES FOR VISUAL QUESTION ANSWERING}}},
  author = {Zhang, Yan and Hare, Jonathon},
  date = {2018},
  pages = {17},
  abstract = {Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-theart accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6\%.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\H9Z69T5I\\Zhang 和 Hare - 2018 - LEARNING TO COUNT OBJECTS IN NATURAL IMAGES FOR VI.pdf}
}

@unpublished{zhangMAGICMultimodalRelAtional2021,
  title = {{{MAGIC}}: {{Multimodal relAtional Graph adversarIal inferenCe}} for {{Diverse}} and {{Unpaired Text-based Image Captioning}}},
  shorttitle = {{{MAGIC}}},
  author = {Zhang, Wenqiao and Shi, Haochen and Guo, Jiannan and Zhang, Shengyu and Cai, Qingpeng and Li, Juncheng and Luo, Sihui and Zhuang, Yueting},
  date = {2021-12-13},
  eprint = {2112.06558},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.06558},
  urldate = {2022-02-10},
  abstract = {Text-based image captioning (TextCap) requires simultaneous comprehension of visual content and reading the text of images to generate a natural language description. Although a task can teach machines to understand the complex human environment further given that text is omnipresent in our daily surroundings, it poses additional challenges in normal captioning. A text-based image intuitively contains abundant and complex multimodal relational content, that is, image details can be described diversely from multiview rather than a single caption. Certainly, we can introduce additional paired training data to show the diversity of images' descriptions, this process is labor-intensive and time-consuming for TextCap pair annotations with extra texts. Based on the insight mentioned above, we investigate how to generate diverse captions that focus on different image parts using an unpaired training paradigm. We propose the Multimodal relAtional Graph adversarIal inferenCe (MAGIC) framework for diverse and unpaired TextCap. This framework can adaptively construct multiple multimodal relational graphs of images and model complex relationships among graphs to represent descriptive diversity. Moreover, a cascaded generative adversarial network is developed from modeled graphs to infer the unpaired caption generation in image-sentence feature alignment and linguistic coherence levels. We validate the effectiveness of MAGIC in generating diverse captions from different relational information items of an image. Experimental results show that MAGIC can generate very promising outcomes without using any image-caption training pairs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\NC7LNXZU\\Zhang et al_2021_MAGIC.pdf;D\:\\Zotero\\storage\\FD9VW7ZI\\2112.html}
}

@unpublished{zhangMAGICMultimodalRelAtional2022,
  title = {{{MAGIC}}: {{Multimodal relAtional Graph adversarIal inferenCe}} for {{Diverse}} and {{Unpaired Text-based Image Captioning}}},
  shorttitle = {{{MAGIC}}},
  author = {Zhang, Wenqiao and Shi, Haochen and Guo, Jiannan and Zhang, Shengyu and Cai, Qingpeng and Li, Juncheng and Luo, Sihui and Zhuang, Yueting},
  date = {2022-03-04},
  eprint = {2112.06558},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.06558},
  urldate = {2022-03-12},
  abstract = {Text-based image captioning (TextCap) requires simultaneous comprehension of visual content and reading the text of images to generate a natural language description. Although a task can teach machines to understand the complex human environment further given that text is omnipresent in our daily surroundings, it poses additional challenges in normal captioning. A text-based image intuitively contains abundant and complex multimodal relational content, that is, image details can be described diversely from multiview rather than a single caption. Certainly, we can introduce additional paired training data to show the diversity of images' descriptions, this process is labor-intensive and time-consuming for TextCap pair annotations with extra texts. Based on the insight mentioned above, we investigate how to generate diverse captions that focus on different image parts using an unpaired training paradigm. We propose the Multimodal relAtional Graph adversarIal inferenCe (MAGIC) framework for diverse and unpaired TextCap. This framework can adaptively construct multiple multimodal relational graphs of images and model complex relationships among graphs to represent descriptive diversity. Moreover, a cascaded generative adversarial network is developed from modeled graphs to infer the unpaired caption generation in image-sentence feature alignment and linguistic coherence levels. We validate the effectiveness of MAGIC in generating diverse captions from different relational information items of an image. Experimental results show that MAGIC can generate very promising outcomes without using any image-caption training pairs.},
  archiveprefix = {arXiv},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\CZETHGEZ\\Zhang et al_2022_MAGIC.pdf;D\:\\Zotero\\storage\\59T3PUW3\\2112.html}
}

@unpublished{zhangMultilabeledRelationExtraction2018,
  title = {Multi-Labeled {{Relation Extraction}} with {{Attentive Capsule Network}}},
  author = {Zhang, Xinsong and Li, Pengshuai and Jia, Weijia and Zhao, Hai},
  date = {2018-11-11},
  eprint = {1811.04354},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.04354},
  urldate = {2021-10-21},
  abstract = {To disclose overlapped multiple relations from a sentence still keeps challenging. Most current works in terms of neural models inconveniently assuming that each sentence is explicitly mapped to a relation label, cannot handle multiple relations properly as the overlapped features of the relations are either ignored or very difficult to identify. To tackle with the new issue, we propose a novel approach for multi-labeled relation extraction with capsule network which acts considerably better than current convolutional or recurrent net in identifying the highly overlapped relations within an individual sentence. To better cluster the features and precisely extract the relations, we further devise attention-based routing algorithm and sliding-margin loss function, and embed them into our capsule network. The experimental results show that the proposed approach can indeed extract the highly overlapped features and achieve significant performance improvement for relation extraction comparing to the state-of-the-art works.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\DLYVGVZS\\Zhang et al_2018_Multi-labeled Relation Extraction with Attentive Capsule Network.pdf;D\:\\Zotero\\storage\\GPZDM8SW\\1811.html}
}

@article{zhangMultiLabeledRelationExtraction2019,
  title = {Multi-{{Labeled Relation Extraction}} with {{Attentive Capsule Network}}},
  author = {Zhang, Xinsong and Li, Pengshuai and Jia, Weijia and Zhao, Hai},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {7484--7491},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017484},
  url = {https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4739},
  urldate = {2021-10-09},
  abstract = {To disclose overlapped multiple relations from a sentence still keeps challenging. Most current works in terms of neural models inconveniently assuming that each sentence is explicitly mapped to a relation label, cannot handle multiple relations properly as the overlapped features of the relations are either ignored or very difficult to identify. To tackle with the new issue, we propose a novel approach for multi-labeled relation extraction with capsule network which acts considerably better than current convolutional or recurrent net in identifying the highly overlapped relations within an individual sentence. To better cluster the features and precisely extract the relations, we further devise attention-based routing algorithm and sliding-margin loss function, and embed them into our capsule network. The experimental results show that the proposed approach can indeed extract the highly overlapped features and achieve significant performance improvement for relation extraction comparing to the state-of-the-art works.},
  langid = {english}
}

@article{zhangMultilevelImageEnhancedSentence2021,
  title = {Multilevel {{Image-Enhanced Sentence Representation Net}} for {{Natural Language Inference}}},
  author = {Zhang, Kun and Lv, Guangyi and Wu, Le and Chen, Enhong and Liu, Qi and Wu, Han and Xie, Xing and Wu, Fangzhao},
  date = {2021-06},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  shortjournal = {IEEE Trans. Syst. Man Cybern, Syst.},
  volume = {51},
  number = {6},
  pages = {3781--3795},
  issn = {2168-2216, 2168-2232},
  doi = {10.1109/TSMC.2019.2932410},
  url = {https://ieeexplore.ieee.org/document/8820068/},
  urldate = {2021-09-06},
  abstract = {Natural language inference (NLI) task requires an agent to determine the semantic relation between a premise sentence (p) and a hypothesis sentence (h), which demands sufficient understanding about sentences semantic. Due to the issues, such as polysemy, ambiguity, as well as fuzziness of sentences, intense sentence understanding is very challenging. To this end, in this article, we introduce the corresponding image of sentences as reference information for enhancing sentence semantic understanding and representing. Specifically, we first propose an image-enhanced multilevel sentence representation net (IEMLRN), that utilizes the image features from pretrained models for enhancing the sentence semantic understanding at different scales, i.e., lexical-level, phrase-level, and sentence-level. The proposed model advances the performance on NLI tasks by leveraging the pretrained global features of images. However, as these pretrained image features are optimized on specific image classification datasets, they may not show the best performance on NLI tasks. Therefore, we further propose to design an adaptive image feature generator that extracts fine-grained image labels from the corresponding sentences. After that, we extend the IEMLRN to multilevel image-enhanced sentence representation net (MIESR) by utilizing not only the coarse-grained pretrained features of images, but also the fine-grained adaptive features of images. Therefore, sentence semantic can be evaluated and enhanced more comprehensively and precisely. Extensive experiments on two benchmark datasets (SNLI, SICK) clearly show our proposed IEMLRN significantly outperform the state-of-theart baselines, and our proposed MIESR model achieves the best performance by considering not only the text but also images in an adaptive multigranularities way.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\4MIMXSH5\\Zhang 等。 - 2021 - Multilevel Image-Enhanced Sentence Representation .pdf}
}

@article{zhangMultilevelImageEnhancedSentence2021a,
  title = {Multilevel {{Image-Enhanced Sentence Representation Net}} for {{Natural Language Inference}}},
  author = {Zhang, Kun and Lv, Guangyi and Wu, Le and Chen, Enhong and Liu, Qi and Wu, Han and Xie, Xing and Wu, Fangzhao},
  date = {2021-06},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {51},
  number = {6},
  pages = {3781--3795},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2019.2932410},
  abstract = {Natural language inference (NLI) task requires an agent to determine the semantic relation between a premise sentence ( \$p\$ ) and a hypothesis sentence ( \$h\$ ), which demands sufficient understanding about sentences semantic. Due to the issues, such as polysemy, ambiguity, as well as fuzziness of sentences, intense sentence understanding is very challenging. To this end, in this article, we introduce the corresponding image of sentences as reference information for enhancing sentence semantic understanding and representing. Specifically, we first propose an image-enhanced multilevel sentence representation net (IEMLRN), that utilizes the image features from pretrained models for enhancing the sentence semantic understanding at different scales, i.e., lexical-level, phrase-level, and sentence-level. The proposed model advances the performance on NLI tasks by leveraging the pretrained global features of images. However, as these pretrained image features are optimized on specific image classification datasets, they may not show the best performance on NLI tasks. Therefore, we further propose to design an adaptive image feature generator that extracts fine-grained image labels from the corresponding sentences. After that, we extend the IEMLRN to multilevel image-enhanced sentence representation net (MIESR) by utilizing not only the coarse-grained pretrained features of images, but also the fine-grained adaptive features of images. Therefore, sentence semantic can be evaluated and enhanced more comprehensively and precisely. Extensive experiments on two benchmark datasets (SNLI, SICK) clearly show our proposed IEMLRN significantly outperform the state-of-the-art baselines, and our proposed MIESR model achieves the best performance by considering not only the text but also images in an adaptive multigranularities way.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}: {{Systems}}},
  keywords = {Feature extraction,Image-enhanced representation,Knowledge discovery,multiple level,natural language inference (NLI),Natural languages,Neural networks,Semantics,sentence semantic,Task analysis,Visualization},
  file = {D\:\\Zotero\\storage\\RJX22PJ4\\8820068.html}
}

@inproceedings{zhangPositionAugmentedTransformersEntityAligned2021,
  title = {Position-{{Augmented Transformers}} with {{Entity-Aligned Mesh}} for {{TextVQA}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Multimedia}}},
  author = {Zhang, Xuanyu and Yang, Qing},
  date = {2021-10-17},
  pages = {2519--2528},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  doi = {10.1145/3474085.3475425},
  url = {https://dl.acm.org/doi/10.1145/3474085.3475425},
  urldate = {2022-02-09},
  abstract = {In addition to visual components, many images usually contain valuable text information, which is essential for understanding the scene. Thus, we study the TextVQA task that requires reading texts in images to answer corresponding questions. However, most of previous works utilize sophisticated graph structure and manually crafted features to model the position relationship between visual entities and texts in images. And traditional multimodal transformers cannot effectively capture relative position information and original image features. To address these issues in an intuitive but effective way, we propose a novel model, position-augmented transformers with entity-aligned mesh, for the TextVQA task. Different from traditional attention mechanism in transformers, we explicitly introduce continuous relative position information of objects and OCR tokens without complex rules. Furthermore, we replace the complicated graph structure with intuitive entity-aligned mesh according to perspective mapping. In this mesh, the information of discrete entities and image patches at different positions can interact with each other. Extensive experiments on two benchmark datasets (TextVQA and ST-VQA) show that our proposed model is superior to several state-of-the-art methods.},
  eventtitle = {{{MM}} '21: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-8651-7},
  langid = {english}
}

@unpublished{zhangRAVENDatasetRelational2019,
  title = {{{RAVEN}}: {{A Dataset}} for {{Relational}} and {{Analogical Visual rEasoNing}}},
  shorttitle = {{{RAVEN}}},
  author = {Zhang, Chi and Gao, Feng and Jia, Baoxiong and Zhu, Yixin and Zhu, Song-Chun},
  date = {2019-03-07},
  eprint = {1903.02741},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1903.02741},
  urldate = {2021-09-09},
  abstract = {Dramatic progress has been witnessed in basic vision tasks involving low-level perception, such as object recognition, detection, and tracking. Unfortunately, there is still an enormous performance gap between artificial vision systems and human intelligence in terms of higher-level vision problems, especially ones involving reasoning. Earlier attempts in equipping machines with high-level reasoning have hovered around Visual Question Answering (VQA), one typical task associating vision and language understanding. In this work, we propose a new dataset, built in the context of Raven's Progressive Matrices (RPM) and aimed at lifting machine intelligence by associating vision with structural, relational, and analogical reasoning in a hierarchical representation. Unlike previous works in measuring abstract reasoning using RPM, we establish a semantic link between vision and reasoning by providing structure representation. This addition enables a new type of abstract reasoning by jointly operating on the structure representation. Machine reasoning ability using modern computer vision is evaluated in this newly proposed dataset. Additionally, we also provide human performance as a reference. Finally, we show consistent improvement across all models by incorporating a simple neural module that combines visual understanding and structure reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Relational，dataset},
  file = {D\:\\Zotero\\storage\\EZHTN29I\\Zhang et al_2019_RAVEN.pdf;D\:\\Zotero\\storage\\2SR3FG9A\\1903.html}
}

@article{zhangReviewArrangeCurriculum2021,
  title = {Review and {{Arrange}}: {{Curriculum Learning}} for {{Natural Language Understanding}}},
  shorttitle = {Review and {{Arrange}}},
  author = {Zhang, Licheng and Mao, Zhendong and Xu, Benfeng and Wang, Quan and Zhang, Yongdong},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {29},
  pages = {3307--3320},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2021.3121986},
  url = {https://ieeexplore.ieee.org/document/9583843/},
  urldate = {2022-05-05},
  abstract = {With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\X5KCX68C\\Zhang 等。 - 2021 - Review and Arrange Curriculum Learning for Natura.pdf}
}

@article{zhangShowYourFaith,
  title = {Show {{Your Faith}}: {{Cross-Modal Conﬁdence-Aware Network}} for {{Image-Text Matching}}},
  author = {Zhang, Huatian and Mao, Zhendong and Zhang, Kun and Zhang, Yongdong},
  pages = {9},
  abstract = {Image-text matching bridges vision and language, which is a crucial task in the field of multi-modal intelligence. The key challenge lies in how to measure image-text relevance accurately as matching evidence. Most existing works aggregate the local semantic similarities of matched region-word pairs as the overall relevance, and they typically assume that the matched pairs are equally reliable. However, although a region-word pair is locally matched across modalities, it may be inconsistent/unreliable from the global perspective of image-text, resulting in inaccurate relevance measurement. In this paper, we propose a novel Cross-Modal ConfidenceAware Network to infer the matching confidence that indicates the reliability of matched region-word pairs, which is combined with the local semantic similarities to refine the relevance measurement. Specifically, we first calculate the matching confidence via the relevance between the semantic of image regions and the complete described semantic in the image, with the text as a bridge. Further, to richly express the semantic of regions, we extend the region to its visual context in the image. Then, local semantic similarities are weighted with the inferred confidence to filter out unreliable matched pairs in aggregating. Comprehensive experiments show that our method achieves state-of-the-art performance on benchmarks Flickr30K and MSCOCO.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\7MANBMQ5\\Zhang 等。 - Show Your Faith Cross-Modal Conﬁdence-Aware Netwo.pdf}
}

@article{zhangUnifiedAdaptiveRelevance2022,
  title = {Unified {{Adaptive Relevance Distinguishable Attention Network}} for {{Image-Text Matching}}},
  author = {Zhang, Kun and Mao, Zhendong and Liu, Anan and Zhang, Yongdong},
  date = {2022},
  journaltitle = {IEEE Transactions on Multimedia},
  shortjournal = {IEEE Trans. Multimedia},
  pages = {1--1},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2022.3141603},
  url = {https://ieeexplore.ieee.org/document/9676463/},
  urldate = {2022-05-05},
  abstract = {Image-text matching, as a fundamental cross-modal task, bridges the gap between vision and language. The core is to accurately learn semantic alignment to find relevant shared semantics in image and text. Existing methods typically attend to all fragments with word-region similarity greater than empirical threshold zero as relevant shared semantics, e.g., via a ReLU operation that forces the negative to zero and maintains the positive. However, this fixed threshold is totally isolated with feature learning, which cannot adaptively and accurately distinguish the varying distributions of relevant and irrelevant wordregion similarity in training, inevitably limiting the semantic alignment learning. To solve this issue, we propose a novel Unified Adaptive Relevance Distinguishable Attention (UARDA) mechanism, incorporating the relevance threshold into a unified learning framework, to maximally distinguish the relevant and irrelevant distributions to obtain better semantic alignment. Specifically, our method adaptively learns the optimal relevance boundary between these two distributions to improve the model to learn more discriminative features. The explicit relevance threshold is well integrated into similarity matching, which kills two birds with one stone as: (1) excluding the disturbances of irrelevant fragment contents to aggregate precisely relevant shared semantics for boosting matching accuracy, and (2) avoiding the calculation of irrelevant fragment queries for reducing retrieval time. Experimental results on benchmarks show that UARDA can substantially and consistently outperform state-of-the-arts, with relative rSum improvements of 2\%-4\% (16.9\%-35.3\% for baseline SCAN), and reducing the retrieval time by 50\%-73\%.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\CJ867MBA\\Zhang 等。 - 2022 - Unified Adaptive Relevance Distinguishable Attenti.pdf}
}

@unpublished{zhangVinVLRevisitingVisual2021,
  title = {{{VinVL}}: {{Revisiting Visual Representations}} in {{Vision-Language Models}}},
  shorttitle = {{{VinVL}}},
  author = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  date = {2021-03-09},
  eprint = {2101.00529},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.00529},
  urldate = {2021-08-25},
  abstract = {This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model OSCAR [21], and utilize an improved approach OSCAR+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code, models and pre-extracted features are released at https://github.com/pzzhang/VinVL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\LKVE2C7T\\Zhang 等。 - 2021 - VinVL Revisiting Visual Representations in Vision.pdf}
}

@unpublished{zhangVisualInterpretabilityDeep2018,
  title = {Visual {{Interpretability}} for {{Deep Learning}}: A {{Survey}}},
  shorttitle = {Visual {{Interpretability}} for {{Deep Learning}}},
  author = {Zhang, Quanshi and Zhu, Song-Chun},
  date = {2018-02-07},
  number = {arXiv:1802.00614},
  eprint = {1802.00614},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1802.00614},
  urldate = {2022-06-09},
  abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\H72QC4DR\\Zhang_Zhu_2018_Visual Interpretability for Deep Learning.pdf;D\:\\Zotero\\storage\\Y7ICMJB6\\1802.html}
}

@inproceedings{zhanMedicalVisualQuestion2020,
  title = {Medical {{Visual Question Answering}} via {{Conditional Reasoning}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Zhan, Li-Ming and Liu, Bo and Fan, Lu and Chen, Jiaxin and Wu, Xiao-Ming},
  date = {2020-10-12},
  pages = {2345--2354},
  publisher = {{ACM}},
  location = {{Seattle WA USA}},
  doi = {10.1145/3394171.3413761},
  url = {https://dl.acm.org/doi/10.1145/3394171.3413761},
  urldate = {2021-03-11},
  abstract = {Medical visual question answering (Med-VQA) aims to accurately answer a clinical question presented with a medical image. Despite its enormous potential in healthcare industry and services, the technology is still in its infancy and is far from practical use. Med-VQA tasks are highly challenging due to the massive diversity of clinical questions and the disparity of required visual reasoning skills for different types of questions. In this paper, we propose a novel conditional reasoning framework for Med-VQA, aiming to automatically learn effective reasoning skills for various Med-VQA tasks. Particularly, we develop a question-conditioned reasoning module to guide the importance selection over multimodal fusion features. Considering the different nature of closed-ended and open-ended Med-VQA tasks, we further propose a type-conditioned reasoning module to learn a different set of reasoning skills for the two types of tasks separately. Our conditional reasoning framework can be easily applied to existing Med-VQA systems to bring performance gains. In the experiments, we build our system on top of a recent state-of-the-art Med-VQA model and evaluate it on the VQA-RAD benchmark [23]. Remarkably, our system achieves significantly increased accuracy in predicting answers to both closed-ended and open-ended questions, especially for open-ended questions, where a 10.8\% increase in absolute accuracy is obtained. The source code can be downloaded from https://github.com/awenbocc/med-vqa.},
  eventtitle = {{{MM}} '20: {{The}} 28th {{ACM International Conference}} on {{Multimedia}}},
  isbn = {978-1-4503-7988-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\T58SIWSI\\Zhan 等。 - 2020 - Medical Visual Question Answering via Conditional .pdf}
}

@article{zhanVisualQuestionAnswering2022,
  title = {Visual Question Answering by Pattern Matching and Reasoning},
  author = {Zhan, Huayi and Xiong, Peixi and Wang, Xin and Wang, Xin and Yang, Lan},
  date = {2022-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {467},
  pages = {323--336},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.10.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221014946},
  urldate = {2021-12-24},
  abstract = {Traditional techniques for visual question answering (VQA) are mostly end-to-end neural network based, which often perform poorly (e.g., low accuracy) due to lack of understanding and reasoning. To overcome the weaknesses, we propose a comprehensive approach with following key features. (1) It represents inputs, i.e., an image I and a natural language question Q nl as an entity-attribute graph and a query graph, respectively, and employs pattern matching to find answers; (2) it leverages reinforcement learning based model to identify a set of policies that are used to guide visual tasks and construct an entityattribute graph, based on Q nl; (3) it employs a novel method to parse a question Q nl and generate corresponding query graph Q ðuoÞ for pattern matching; and (4) it integrates inference scheme to further improve result accuracy, in particular, it learns a graph-structured classifier for missing value inference and a co-occurrence matrix for candidate selection. With these features, our approach can not only process visual tasks efficiently, but also answer questions with high accuracy. To evaluate the performance of our approach, we conduct empirical studies on Soccer, Visual-Genome and GQA, and show that our approach outperforms the state-of-the-art methods in result accuracy and system efficiency.},
  langid = {english}
}

@article{zhanVisualQuestionAnswering2022a,
  title = {Visual Question Answering by Pattern Matching and Reasoning},
  author = {Zhan, Huayi and Xiong, Peixi and Wang, Xin and Wang, Xin and Yang, Lan},
  date = {2022-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {467},
  pages = {323--336},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.10.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221014946},
  urldate = {2021-12-24},
  abstract = {Traditional techniques for visual question answering (VQA) are mostly end-to-end neural network based, which often perform poorly (e.g., low accuracy) due to lack of understanding and reasoning. To overcome the weaknesses, we propose a comprehensive approach with following key features. (1) It represents inputs, i.e., an image I and a natural language question Q nl as an entity-attribute graph and a query graph, respectively, and employs pattern matching to find answers; (2) it leverages reinforcement learning based model to identify a set of policies that are used to guide visual tasks and construct an entityattribute graph, based on Q nl; (3) it employs a novel method to parse a question Q nl and generate corresponding query graph Q ðuoÞ for pattern matching; and (4) it integrates inference scheme to further improve result accuracy, in particular, it learns a graph-structured classifier for missing value inference and a co-occurrence matrix for candidate selection. With these features, our approach can not only process visual tasks efficiently, but also answer questions with high accuracy. To evaluate the performance of our approach, we conduct empirical studies on Soccer, Visual-Genome and GQA, and show that our approach outperforms the state-of-the-art methods in result accuracy and system efficiency.},
  langid = {english}
}

@unpublished{zhaoGophormerEgoGraphTransformer2021,
  title = {Gophormer: {{Ego-Graph Transformer}} for {{Node Classification}}},
  shorttitle = {Gophormer},
  author = {Zhao, Jianan and Li, Chaozhuo and Wen, Qianlong and Wang, Yiqi and Liu, Yuming and Sun, Hao and Xie, Xing and Ye, Yanfang},
  date = {2021-10-25},
  eprint = {2110.13094},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.13094},
  urldate = {2022-03-26},
  abstract = {Transformers have achieved remarkable performance in a myriad of fields including natural language processing and computer vision. However, when it comes to the graph mining area, where graph neural network (GNN) has been the dominant paradigm, transformers haven't achieved competitive performance, especially on the node classification task. Existing graph transformer models typically adopt fully-connected attention mechanism on the whole input graph and thus suffer from severe scalability issues and are intractable to train in data insufficient cases. To alleviate these issues, we propose a novel Gophormer model which applies transformers on ego-graphs instead of full-graphs. Specifically, Node2Seq module is proposed to sample ego-graphs as the input of transformers, which alleviates the challenge of scalability and serves as an effective data augmentation technique to boost model performance. Moreover, different from the feature-based attention strategy in vanilla transformers, we propose a proximity-enhanced attention mechanism to capture the fine-grained structural bias. In order to handle the uncertainty introduced by the ego-graph sampling, we further propose a consistency regularization and a multi-sample inference strategy for stabilized training and testing, respectively. Extensive experiments on six benchmark datasets are conducted to demonstrate the superiority of Gophormer over existing graph transformers and popular GNNs, revealing the promising future of graph transformers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\94VQEDQA\\Zhao et al_2021_Gophormer.pdf;D\:\\Zotero\\storage\\VBJ5ZKMZ\\2110.html}
}

@misc{zhaoMAMOMaskedMultimodal2022,
  title = {{{MAMO}}: {{Masked Multimodal Modeling}} for {{Fine-Grained Vision-Language Representation Learning}}},
  shorttitle = {{{MAMO}}},
  author = {Zhao, Zijia and Guo, Longteng and He, Xingjian and Shao, Shuai and Yuan, Zehuan and Liu, Jing},
  date = {2022-10-09},
  number = {arXiv:2210.04183},
  eprint = {2210.04183},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.04183},
  urldate = {2022-10-11},
  abstract = {Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also avoids the semantic gap between high-level representations and low- or mid-level prediction targets (e.g. image pixels), thus producing semantically rich multimodal representations that perform well on both zero-shot and fine-tuned settings. Our pre-trained model (named MAMO) achieves state-of-the-art performance on various downstream vision-language tasks, including image-text retrieval, visual question answering, visual reasoning, and weakly-supervised visual grounding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\939AEAG9\\Zhao et al_2022_MAMO.pdf;D\:\\Zotero\\storage\\TELMD3D7\\2210.html}
}

@article{zhaoMultiTurnVideoQuestion2018,
  title = {Multi-{{Turn Video Question Answering}} via {{Multi-Stream Hierarchical Attention Context Network}}},
  author = {Zhao, Zhou and Jiang, Xinghua and Cai, Deng and Xiao, Jun and He, Xiaofei and Pu, Shiliang},
  date = {2018},
  pages = {3690--3696},
  url = {https://www.ijcai.org/proceedings/2018/513},
  urldate = {2021-09-10},
  abstract = {Electronic proceedings of IJCAI 2018},
  keywords = {Attention,Hierarchical,VideoQA},
  file = {D\:\\Zotero\\storage\\DQSDJJM7\\513.html}
}

@inproceedings{zhaoOpenEndedLongformVideo2018,
  title = {Open-{{Ended Long-form Video Question Answering}} via {{Adaptive Hierarchical Reinforced Networks}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhao, Zhou and Zhang, Zhu and Xiao, Shuwen and Yu, Zhou and Yu, Jun and Cai, Deng and Wu, Fei and Zhuang, Yueting},
  date = {2018-07},
  pages = {3683--3689},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/512},
  url = {https://www.ijcai.org/proceedings/2018/512},
  urldate = {2021-09-10},
  abstract = {Open-ended long-form video question answering is challenging problem in visual information retrieval, which automatically generates the natural language answer from the referenced long-form video content according to the question. However, the existing video question answering works mainly focus on the short-form video question answering, due to the lack of modeling the semantic representation of long-form video contents. In this paper, we consider the problem of long-form video question answering from the viewpoint of adaptive hierarchical reinforced encoder-decoder network learning. We propose the adaptive hierarchical encoder network to learn the joint representation of the longform video contents according to the question with adaptive video segmentation. we then develop the reinforced decoder network to generate the natural language answer for open-ended video question answering. We construct a large-scale long-form video question answering dataset. The extensive experiments show the effectiveness of our method.},
  eventtitle = {Twenty-{{Seventh International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-18}}\vphantom\{\}},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\X3Y4AGB8\\Zhao 等。 - 2018 - Open-Ended Long-form Video Question Answering via .pdf}
}

@article{zhaoSemanticallySimilaritywiseDualbranch2021,
  title = {Semantically {{Similarity-wise Dual-branch Network}} for {{Scene Graph Generation}}},
  author = {Zhao, Bowen and Mao, Zhendong and Fang, Shancheng and Zang, Wenyu and Zhang, Yongdong},
  date = {2021},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  shortjournal = {IEEE Trans. Circuits Syst. Video Technol.},
  pages = {1--1},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2021.3130197},
  url = {https://ieeexplore.ieee.org/document/9625966/},
  urldate = {2022-05-05},
  langid = {english},
  file = {D\:\\Zotero\\storage\\75AC9EGU\\Zhao 等。 - 2021 - Semantically Similarity-wise Dual-branch Network f.pdf}
}

@inproceedings{zhaoVideoQuestionAnswering2017,
  title = {Video {{Question Answering}} via {{Hierarchical Dual-Level Attention Network Learning}}},
  booktitle = {Proceedings of the 25th {{ACM}} International Conference on {{Multimedia}}},
  author = {Zhao, Zhou and Lin, Jinghao and Jiang, Xinghua and Cai, Deng and He, Xiaofei and Zhuang, Yueting},
  date = {2017-10-19},
  pages = {1050--1058},
  publisher = {{ACM}},
  location = {{Mountain View California USA}},
  doi = {10.1145/3123266.3123364},
  url = {https://dl.acm.org/doi/10.1145/3123266.3123364},
  urldate = {2021-03-11},
  abstract = {Video question answering is a challenging task in visual information retrieval, which provides the accurate answer from the referenced video contents according to the given question. However, the existing visual question answering approaches mainly tackle the problem of static image question answering, which may be ineffectively applied for video question answering directly, due to the insufficiency of modeling the video temporal dynamics. In this paper, we study the problem of video question answering from the viewpoint of hierarchical dual-level attention network learning. We obtain the object appearance and movement information in the video based on both frame-level and segment-level feature representation methods. We then develop the hierarchical duallevel attention networks to learn the question-aware video representations with word-level and question-level attention mechanisms. We next devise the question-level fusion attention mechanism for our proposed networks to learn the questionaware joint video representation for video question answering. We construct two large-scale video question answering datasets. The extensive experiments validate the effectiveness of our method.},
  eventtitle = {{{MM}} '17: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-4906-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\MFY4NDBL\\Zhao 等。 - 2017 - Video Question Answering via Hierarchical Dual-Lev.pdf}
}

@misc{zhaoVLCheckListEvaluatingPretrained2022,
  title = {{{VL-CheckList}}: {{Evaluating Pre-trained Vision-Language Models}} with {{Objects}}, {{Attributes}} and {{Relations}}},
  shorttitle = {{{VL-CheckList}}},
  author = {Zhao, Tiancheng and Zhang, Tianqi and Zhu, Mingwei and Shen, Haozhan and Lee, Kyusong and Lu, Xiaopeng and Yin, Jianwei},
  date = {2022-07-01},
  number = {arXiv:2207.00221},
  eprint = {2207.00221},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.00221},
  urldate = {2022-07-19},
  abstract = {Vision-Language Pretraining (VLP) models have recently successfully facilitated many cross-modal downstream tasks. Most existing works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method, let alone provides insights on how the community can improve the systems in the future. Inspired by the CheckList for testing natural language processing, we introduce VL-CheckList, a novel framework to understand the capabilities of VLP models. The proposed method divides the image-texting ability of a VLP model into three categories: objects, attributes, and relations, and uses a novel taxonomy to further break down these three aspects. We conduct comprehensive studies to analyze seven recently popular VLP models via the proposed framework. Results confirm the effectiveness of the proposed method by revealing fine-grained differences among the compared models that were not visible from downstream task-only evaluation. Further results show promising research direction in building better VLP models. Data and Code: https://github.com/om-ai-lab/VL-CheckList},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\JCLHNWXC\\Zhao et al_2022_VL-CheckList.pdf;D\:\\Zotero\\storage\\2D3L4ZTE\\2207.html}
}

@unpublished{zhengCrossModalityRelevanceReasoning2020,
  title = {Cross-{{Modality Relevance}} for {{Reasoning}} on {{Language}} and {{Vision}}},
  author = {Zheng, Chen and Guo, Quan and Kordjamshidi, Parisa},
  date = {2020-05-12},
  eprint = {2005.06035},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.06035},
  urldate = {2021-09-09},
  abstract = {This work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering (VQA) and natural language for visual reasoning (NLVR). We design a novel cross-modality relevance module that is used in an end-to-end framework to learn the relevance representation between components of various input modalities under the supervision of a target task, which is more generalizable to unobserved data compared to merely reshaping the original representation space. In addition to modeling the relevance between the textual entities and visual entities, we model the higher-order relevance between entity relations in the text and object relations in the image. Our proposed approach shows competitive performance on two different language and vision tasks using public benchmarks and improves the state-of-the-art published results. The learned alignments of input spaces and their relevance representations by NLVR task boost the training efficiency of VQA task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Reasoning},
  file = {D\:\\Zotero\\storage\\QBC93W3A\\Zheng et al_2020_Cross-Modality Relevance for Reasoning on Language and Vision.pdf;D\:\\Zotero\\storage\\BWMYMJTX\\2005.html}
}

@unpublished{zhengReasoningVisualDialogs2019,
  title = {Reasoning {{Visual Dialogs}} with {{Structural}} and {{Partial Observations}}},
  author = {Zheng, Zilong and Wang, Wenguan and Qi, Siyuan and Zhu, Song-Chun},
  date = {2019-05-28},
  eprint = {1904.05548},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.05548},
  urldate = {2021-12-08},
  abstract = {We propose a novel model to address the task of Visual Dialog which exhibits complex dialog structures. To obtain a reasonable answer based on the current question and the dialog history, the underlying semantic dependencies between dialog entities are essential. In this paper, we explicitly formalize this task as inference in a graphical model with partially observed nodes and unknown graph structures (relations in dialog). The given dialog entities are viewed as the observed nodes. The answer to a given question is represented by a node with missing value. We first introduce an Expectation Maximization algorithm to infer both the underlying dialog structures and the missing node values (desired answers). Based on this, we proceed to propose a differentiable graph neural network (GNN) solution that approximates this process. Experiment results on the VisDial and VisDial-Q datasets show that our model outperforms comparative methods. It is also observed that our method can infer the underlying dialog structure for better dialog reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\9PCZN2JR\\Zheng et al_2019_Reasoning Visual Dialogs with Structural and Partial Observations.pdf;D\:\\Zotero\\storage\\SUPATC6I\\1904.html}
}

@inproceedings{zhengWeblySupervisedKnowledge2020,
  title = {Webly {{Supervised Knowledge Embedding Model}} for {{Visual Reasoning}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zheng, Wenbo and Yan, Lan and Gou, Chao and Wang, Fei-Yue},
  date = {2020-06},
  pages = {12442--12451},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01246},
  url = {https://ieeexplore.ieee.org/document/9157013/},
  urldate = {2021-09-09},
  abstract = {Visual reasoning between visual image and natural language description is a long-standing challenge in computer vision. While recent approaches offer a great promise by compositionality or relational computing, most of them are oppressed by the challenge of training with datasets containing only a limited number of images with ground-truth texts. Besides, it is extremely time-consuming and difficult to build a larger dataset by annotating millions of images with text descriptions that may very likely lead to a biased model. Inspired by the majority success of webly supervised learning, we utilize readily-available web images with its noisy annotations for learning a robust representation. Our key idea is to presume on web images and corresponding tags along with fully annotated datasets in learning with knowledge embedding. We present a two-stage approach for the task that can augment knowledge through an effective embedding model with weakly supervised web data. This approach learns not only knowledge-based embeddings derived from key-value memory networks to make joint and full use of textual and visual information but also exploits the knowledge to improve the performance with knowledge-based representation learning for applying other general reasoning tasks. Experimental results on two benchmarks show that the proposed approach significantly improves performance compared with the state-ofthe-art methods and guarantees the robustness of our model against visual reasoning tasks and other reasoning tasks.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  keywords = {Knowledge,Reasoning},
  annotation = {2 citations (Crossref) [2021-09-10]},
  file = {D\:\\Zotero\\storage\\NX4WWI7S\\Zheng 等。 - 2020 - Webly Supervised Knowledge Embedding Model for Vis.pdf}
}

@article{zhiwuWuDaoWenLanChaoDaGuiMoDuoMoTai2022,
  title = {悟道 文澜：超大规模多模态 预训练模型带来了什么？},
  author = {Zhiwu, LU and Qin, JIN and Ruihua, SONG and Jirong, WEN},
  date = {2022},
  volume = {28},
  number = {2},
  pages = {8},
  abstract = {A multimodal pre-training two-tower model called WuDao-WenLan BriVL is proposed, which is trained through self-supervised learning over 650 million image-text pairs crawled from the Web. This is the largest open-sourced Chinese image-text pre-training model. Moreover, a multi-lingual pre-training single-tower model called WuDao-WenLan MLMM is also proposed. Extensive experiments show that these two models achieve the new state-of-the-art performance on multiple public benchmark datasets. In addition, experiments are conducted to discuss what very-large multimodal pre-training models bring to text encoding, text-to-image generation, and image-text re⁃ trieval, as well as in what applications WenLan can be applied in multiple fields.},
  langid = {chinese},
  file = {D\:\\Zotero\\storage\\6PSIB9VS\\Zhiwu 等。 - 2022 - 悟道 文澜：超大规模多模态 预训练模型带来了什么？.pdf}
}

@unpublished{zhongCoarsegrainFinegrainCoattention2019,
  title = {Coarse-Grain {{Fine-grain Coattention Network}} for {{Multi-evidence Question Answering}}},
  author = {Zhong, Victor and Xiong, Caiming and Keskar, Nitish Shirish and Socher, Richard},
  date = {2019-05-13},
  eprint = {1901.00603},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1901.00603},
  urldate = {2021-10-10},
  abstract = {End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6\% on the blind test set, outperforming the previous best by 3\% accuracy despite not using pretrained contextual encoders.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\CI4XMDUN\\Zhong et al_2019_Coarse-grain Fine-grain Coattention Network for Multi-evidence Question.pdf;D\:\\Zotero\\storage\\2E8A4EBF\\1901.html}
}

@misc{zhouDomainGeneralizationMixStyle2021,
  title = {Domain {{Generalization}} with {{MixStyle}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  date = {2021-04-05},
  number = {arXiv:2104.02008},
  eprint = {2104.02008},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2104.02008},
  urldate = {2022-06-07},
  abstract = {Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instancelevel feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs. sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\storage\\TG33H9CZ\\Zhou 等。 - 2021 - Domain Generalization with MixStyle.pdf}
}

@article{zhouDynamicCapsuleAttention2019,
  title = {Dynamic {{Capsule Attention}} for {{Visual Question Answering}}},
  author = {Zhou, Yiyi and Ji, Rongrong and Su, Jinsong and Sun, Xiaoshuai and Chen, Weiqiu},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {9324--9331},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33019324},
  url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4970},
  urldate = {2021-09-10},
  abstract = {In visual question answering (VQA), recent advances have well advocated the use of attention mechanism to precisely link the question to the potential answer areas. As the difficulty of the question increases, more VQA models adopt multiple attention layers to capture the deeper visual-linguistic correlation. But a negative consequence is the explosion of parameters, which makes the model vulnerable to over-fitting, especially when limited training examples are given. In this paper, we propose an extremely compact alternative to this static multi-layer architecture towards accurate yet efficient attention modeling, termed as Dynamic Capsule Attention (CapsAtt). Inspired by the recent work of Capsule Network, CapsAtt treats visual features as capsules and obtains the attention output via dynamic routing, which updates the attention weights by calculating coupling coefficients between the underlying and output capsules. Meanwhile, CapsAtt also discards redundant projection matrices to make the model much more compact. We quantify CapsAtt on three benchmark VQA datasets, i.e., COCO-QA, VQA1.0 and VQA2.0. Compared to the traditional multi-layer attention model, CapsAtt achieves significant improvements of up to 4.1\%, 5.2\% and 2.2\% on three datasets, respectively. Moreover, with much fewer parameters, our approach also yields competitive results compared to the latest VQA models. To further verify the generalization ability of CapsAtt, we also deploy it on another challenging multi-modal task of image captioning, where state-of-the-art performance is achieved with a simple network structure.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\GMIYGG2T\\Zhou 等。 - 2019 - Dynamic Capsule Attention for Visual Question Answ.pdf}
}

@article{zhouFreeVQAModels2019,
  title = {Free {{VQA Models}} from {{Knowledge Inertia}} by {{Pairwise Inconformity Learning}}},
  author = {Zhou, Yiyi and Ji, Rongrong and Su, Jinsong and Li, Xiangming and Sun, Xiaoshuai},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {9316--9323},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33019316},
  url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4969},
  urldate = {2021-09-10},
  abstract = {In this paper, we uncover the issue of knowledge inertia in visual question answering (VQA), which commonly exists in most VQA models and forces the models to mainly rely on the question content to “guess” answer, without regard to the visual information. Such an issue not only impairs the performance of VQA models, but also greatly reduces the credibility of the answer prediction. To this end, simply highlighting the visual features in the model is undoable, since the prediction is built upon the joint modeling of two modalities and largely influenced by the data distribution. In this paper, we propose a Pairwise Inconformity Learning (PIL) to tackle the issue of knowledge inertia. In particular, PIL takes full advantage of the similar image pairs with diverse answers to an identical question provided in VQA2.0 dataset. It builds a multi-modal embedding space to project pos./neg. feature pairs, upon which word vectors of answers are modeled as anchors. By doing so, PIL strengthens the importance of visual features in prediction with a novel dynamic-margin based triplet loss that efficiently increases the semantic discrepancies between pos./neg. image pairs. To verify the proposed PIL, we plug it on a baseline VQA model as well as a set of recent VQA models, and conduct extensive experiments on two benchmark datasets, i.e., VQA1.0 and VQA2.0. Experimental results show that PIL can boost the accuracy of the existing VQA models (1.56\%-2.93\% gain) with a negligible increase in parameters (0.85\%-5.4\% parameters). Qualitative results also reveal the elimination of knowledge inertia in the existing VQA models after implementing our PIL.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\Y79UKR2U\\Zhou 等。 - 2019 - Free VQA Models from Knowledge Inertia by Pairwise.pdf}
}

@misc{zhouLearningGenerateNovel2021,
  title = {Learning to {{Generate Novel Domains}} for {{Domain Generalization}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Hospedales, Timothy and Xiang, Tao},
  date = {2021-03-09},
  number = {arXiv:2007.03304},
  eprint = {2007.03304},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2007.03304},
  urldate = {2022-06-07},
  abstract = {This paper focuses on domain generalization (DG), the task of learning from multiple source domains a model that generalizes well to unseen domains. A main challenge for DG is that the available source domains often exhibit limited diversity, hampering the model’s ability to learn to generalize. We therefore employ a data generator to synthesize data from pseudo-novel domains to augment the source domains. This explicitly increases the diversity of available training domains and leads to a more generalizable model. To train the generator, we model the distribution divergence between source and synthesized pseudo-novel domains using optimal transport, and maximize the divergence. To ensure that semantics are preserved in the synthesized data, we further impose cycle-consistency and classification losses on the generator. Our method, L2A-OT (Learning to Augment by Optimal Transport) outperforms current state-of-the-art DG methods on four benchmark datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\XPXTWWBV\\Zhou 等。 - 2021 - Learning to Generate Novel Domains for Domain Gene.pdf}
}

@article{zhouUnifiedVisionLanguagePreTraining2020,
  title = {Unified {{Vision-Language Pre-Training}} for {{Image Captioning}} and {{VQA}}},
  author = {Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {07},
  pages = {13041--13049},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i07.7005},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/7005},
  urldate = {2021-09-10},
  abstract = {This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be finetuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\IY7NDULI\\Zhou 等。 - 2020 - Unified Vision-Language Pre-Training for Image Cap.pdf}
}

@unpublished{zhuDeepGraphContrastive2020,
  title = {Deep {{Graph Contrastive Representation Learning}}},
  author = {Zhu, Yanqiao and Xu, Yichen and Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang},
  date = {2020-07-13},
  eprint = {2006.04131},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.04131},
  urldate = {2022-03-20},
  abstract = {Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\storage\\IJUT6RXF\\Zhu et al_2020_Deep Graph Contrastive Representation Learning.pdf;D\:\\Zotero\\storage\\4Y6WT86J\\2006.html}
}

@article{zhuObjectdifferenceDrivedGraph2021,
  title = {Object-Difference Drived Graph Convolutional Networks for Visual Question Answering},
  author = {Zhu, Xi and Mao, Zhendong and Chen, Zhineng and Li, Yangyang and Wang, Zhaohui and Wang, Bin},
  date = {2021-05},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {80},
  number = {11},
  pages = {16247--16265},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-08790-0},
  url = {https://link.springer.com/10.1007/s11042-020-08790-0},
  urldate = {2022-05-05},
  abstract = {Visual Question Answering(VQA), an important task to evaluate the cross-modal understanding capability of an Artificial Intelligence model, has been a hot research topic in both computer vision and natural language processing communities. Recently, graphbased models have received growing interest in VQA, for its potential of modeling the relationships between objects as well as its formidable interpretability. Nonetheless, those solutions mainly define the similarity between objects as their semantical relationships, while largely ignoring the critical point that the difference between objects can provide more information for establishing the relationship between nodes in the graph. To achieve this, we propose an object-difference based graph learner, which learns question-adaptive semantic relations by calculating inter-object difference under the guidance of questions. With the learned relationships, the input image can be represented as an object graph encoded with structural dependencies between objects. In addition, existing graph-based models leverage the pre-extracted object boxes by the object detection model as node features for convenience, but they are suffering from the redundancy problem. To reduce the redundant objects, we introduce a soft-attention mechanism to magnify the questionrelated objects. Moreover, we incorporate our object-difference based graph learner into the soft-attention based Graph Convolutional Networks to capture question-specific objects and their interactions for answer prediction. Our experimental results on the VQA 2.0 dataset demonstrate that our model gives significantly better performance than baseline methods.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\TWS7DTEE\\Zhu 等。 - 2021 - Object-difference drived graph convolutional netwo.pdf}
}

@article{zhuObjectdifferenceDrivedGraph2021a,
  title = {Object-Difference Drived Graph Convolutional Networks for Visual Question Answering},
  author = {Zhu, Xi and Mao, Zhendong and Chen, Zhineng and Li, Yangyang and Wang, Zhaohui and Wang, Bin},
  date = {2021-05},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {80},
  number = {11},
  pages = {16247--16265},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-08790-0},
  url = {https://link.springer.com/10.1007/s11042-020-08790-0},
  urldate = {2022-05-05},
  abstract = {Visual Question Answering(VQA), an important task to evaluate the cross-modal understanding capability of an Artificial Intelligence model, has been a hot research topic in both computer vision and natural language processing communities. Recently, graphbased models have received growing interest in VQA, for its potential of modeling the relationships between objects as well as its formidable interpretability. Nonetheless, those solutions mainly define the similarity between objects as their semantical relationships, while largely ignoring the critical point that the difference between objects can provide more information for establishing the relationship between nodes in the graph. To achieve this, we propose an object-difference based graph learner, which learns question-adaptive semantic relations by calculating inter-object difference under the guidance of questions. With the learned relationships, the input image can be represented as an object graph encoded with structural dependencies between objects. In addition, existing graph-based models leverage the pre-extracted object boxes by the object detection model as node features for convenience, but they are suffering from the redundancy problem. To reduce the redundant objects, we introduce a soft-attention mechanism to magnify the questionrelated objects. Moreover, we incorporate our object-difference based graph learner into the soft-attention based Graph Convolutional Networks to capture question-specific objects and their interactions for answer prediction. Our experimental results on the VQA 2.0 dataset demonstrate that our model gives significantly better performance than baseline methods.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\NYSNQ26F\\Zhu 等。 - 2021 - Object-difference drived graph convolutional netwo.pdf}
}

@inproceedings{zhuOvercomingLanguagePriors2020,
  title = {Overcoming {{Language Priors}} with {{Self-supervised Learning}} for {{Visual Question Answering}}},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhu, Xi and Mao, Zhendong and Liu, Chunxiao and Zhang, Peng and Wang, Bin and Zhang, Yongdong},
  date = {2020-07},
  pages = {1083--1089},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/151},
  url = {https://www.ijcai.org/proceedings/2020/151},
  urldate = {2022-05-05},
  abstract = {Most Visual Question Answering (VQA) models suffer from the language prior problem, which is caused by inherent data biases. Specifically, VQA models tend to answer questions (e.g., what color is the banana?) based on the high-frequency answers (e.g., yellow) ignoring image contents. Existing approaches tackle this problem by creating delicate models or introducing additional visual annotations to reduce question dependency while strengthening image dependency. However, they are still subject to the language prior problem since the data biases have not been even alleviated. In this paper, we introduce a self-supervised learning framework to solve this problem. Concretely, we first automatically generate labeled data to balance the biased data, and propose a self-supervised auxiliary task to utilize the balanced data to assist the base VQA model to overcome language priors. Our method can compensate for the data biases by generating balanced data without introducing external annotations. Experimental results show that our method can significantly outperform the state-of-the-art, improving the overall accuracy from 49.50\% to 57.59\% on the most commonly used benchmark VQA-CP v2. In other words, we can increase the performance of annotation-based methods by 16\% without using external annotations. Our code is available in GitHub1.},
  eventtitle = {Twenty-{{Ninth International Joint Conference}} on {{Artificial Intelligence}} and {{Seventeenth Pacific Rim International Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-PRICAI-20}}\vphantom\{\}},
  isbn = {978-0-9992411-6-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ALRYFZK8\\Zhu 等。 - 2020 - Overcoming Language Priors with Self-supervised Le.pdf}
}

@unpublished{zhuOvercomingLanguagePriors2020a,
  title = {Overcoming {{Language Priors}} with {{Self-supervised Learning}} for {{Visual Question Answering}}},
  author = {Zhu, Xi and Mao, Zhendong and Liu, Chunxiao and Zhang, Peng and Wang, Bin and Zhang, Yongdong},
  date = {2020-12-17},
  eprint = {2012.11528},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.11528},
  urldate = {2022-03-31},
  abstract = {Most Visual Question Answering (VQA) models suffer from the language prior problem, which is caused by inherent data biases. Specifically, VQA models tend to answer questions (e.g., what color is the banana?) based on the high-frequency answers (e.g., yellow) ignoring image contents. Existing approaches tackle this problem by creating delicate models or introducing additional visual annotations to reduce question dependency while strengthening image dependency. However, they are still subject to the language prior problem since the data biases have not been even alleviated. In this paper, we introduce a self-supervised learning framework to solve this problem. Concretely, we first automatically generate labeled data to balance the biased data, and propose a self-supervised auxiliary task to utilize the balanced data to assist the base VQA model to overcome language priors. Our method can compensate for the data biases by generating balanced data without introducing external annotations. Experimental results show that our method can significantly outperform the state-of-the-art, improving the overall accuracy from 49.50\% to 57.59\% on the most commonly used benchmark VQA-CP v2. In other words, we can increase the performance of annotation-based methods by 16\% without using external annotations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {D\:\\Zotero\\storage\\LJJP25TE\\Zhu et al_2020_Overcoming Language Priors with Self-supervised Learning for Visual Question.pdf;D\:\\Zotero\\storage\\SC24X4E2\\2012.html}
}

@unpublished{zhuSimpleNotEasy2020,
  title = {Simple Is Not {{Easy}}: {{A Simple Strong Baseline}} for {{TextVQA}} and {{TextCaps}}},
  shorttitle = {Simple Is Not {{Easy}}},
  author = {Zhu, Qi and Gao, Chenyu and Wang, Peng and Wu, Qi},
  date = {2020-12-09},
  eprint = {2012.05153},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.05153},
  urldate = {2021-12-08},
  abstract = {Texts appearing in daily scenes that can be recognized by OCR (Optical Character Recognition) tools contain significant information, such as street name, product brand and prices. Two tasks -- text-based visual question answering and text-based image captioning, with a text extension from existing vision-language applications, are catching on rapidly. To address these problems, many sophisticated multi-modality encoding frameworks (such as heterogeneous graph structure) are being used. In this paper, we argue that a simple attention mechanism can do the same or even better job without any bells and whistles. Under this mechanism, we simply split OCR token features into separate visual- and linguistic-attention branches, and send them to a popular Transformer decoder to generate answers or captions. Surprisingly, we find this simple baseline model is rather strong -- it consistently outperforms state-of-the-art (SOTA) models on two popular benchmarks, TextVQA and all three tasks of ST-VQA, although these SOTA models use far more complex encoding mechanisms. Transferring it to text-based image captioning, we also surpass the TextCaps Challenge 2020 winner. We wish this work to set the new baseline for this two OCR text related applications and to inspire new thinking of multi-modality encoder design. Code is available at https://github.com/ZephyrZhuQi/ssbaseline},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\storage\\AHS9D26G\\Zhu et al_2020_Simple is not Easy.pdf;D\:\\Zotero\\storage\\6MJTW9XZ\\2012.html}
}

@inproceedings{zhuStructuredAttentionsVisual2017,
  title = {Structured {{Attentions}} for {{Visual Question Answering}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhu, Chen and Zhao, Yanpeng and Huang, Shuaiyi and Tu, Kewei and Ma, Yi},
  date = {2017-10},
  pages = {1300--1309},
  publisher = {{IEEE}},
  location = {{Venice}},
  doi = {10.1109/ICCV.2017.145},
  url = {http://ieeexplore.ieee.org/document/8237407/},
  urldate = {2021-09-10},
  abstract = {Visual attention, which assigns weights to image regions according to their relevance to a question, is considered as an indispensable part by most Visual Question Answering models. Although the questions may involve complex relations among multiple regions, few attention models can effectively encode such cross-region relations. In this paper, we demonstrate the importance of encoding such relations by showing the limited effective receptive field of ResNet on two datasets, and propose to model the visual attention as a multivariate distribution over a grid-structured Conditional Random Field on image regions. We demonstrate how to convert the iterative inference algorithms, Mean Field and Loopy Belief Propagation, as recurrent layers of an end-to-end neural network. We empirically evaluated our model on 3 datasets, in which it surpasses the best baseline model of the newly released CLEVR dataset [13] by 9.5\%, and the best published model on the VQA dataset [3] by 1.25\%. Source code is available at https: //github.com/zhuchen03/vqa-sva.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\UK9IFZ4N\\Zhu 等。 - 2017 - Structured Attentions for Visual Question Answerin.pdf}
}

@misc{zhuVisualizeYouWrite2022,
  title = {Visualize {{Before You Write}}: {{Imagination-Guided Open-Ended Text Generation}}},
  shorttitle = {Visualize {{Before You Write}}},
  author = {Zhu, Wanrong and Yan, An and Lu, Yujie and Xu, Wenda and Wang, Xin Eric and Eckstein, Miguel and Wang, William Yang},
  date = {2022-10-07},
  number = {arXiv:2210.03765},
  eprint = {2210.03765},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.03765},
  urldate = {2022-10-11},
  abstract = {Recent advances in text-to-image synthesis make it possible to visualize machine imaginations for a given context. On the other hand, when generating text, human writers are gifted at creative visualization, which enhances their writings by forming imaginations as blueprints before putting down the stories in words. Inspired by such a cognitive process, we ask the natural question of whether we can endow machines with the same ability to utilize visual information and construct a general picture of the context to guide text generation. In this work, we propose iNLG that uses machine-generated images to guide language models (LM) in open-ended text generation. The experiments and analyses demonstrate the effectiveness of iNLG on open-ended text generation tasks, including text completion, story generation, and concept-to-text generation in few-shot scenarios. Both automatic metrics and human evaluations verify that the text snippets generated by our iNLG are coherent and informative while displaying minor degeneration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\storage\\I5Q5WQ9S\\Zhu et al_2022_Visualize Before You Write.pdf;D\:\\Zotero\\storage\\MYSXHGD9\\2210.html}
}

@online{zotero-960,
  url = {https://www.aminer.cn/download_pdf?link=https://static.aminer.cn/upload/pdf/program/58437725ac44360f1082f74a_0.pdf},
  urldate = {2021-10-11}
}

